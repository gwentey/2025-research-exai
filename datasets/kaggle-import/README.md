# üöÄ Syst√®me d'Import Kaggle EXAI

## Vue d'ensemble

Le syst√®me d'import Kaggle permet d'importer automatiquement des datasets depuis Kaggle vers la plateforme EXAI, r√©solvant le probl√®me des datasets trop volumineux pour GitHub.

## Architecture

```
Kaggle API ‚Üí T√©l√©chargement ‚Üí Conversion Parquet ‚Üí Upload Stockage ‚Üí BDD M√©tadonn√©es
```

**Avantages** :
- ‚úÖ Pas de limite de taille
- ‚úÖ Import automatique en production
- ‚úÖ Cache intelligent (7 jours)
- ‚úÖ Conversion Parquet optimis√©e
- ‚úÖ Validation multi-niveaux

## Configuration Requise

### 1. Credentials Kaggle

Cr√©ez un compte Kaggle et t√©l√©chargez votre `kaggle.json` :

1. Allez sur https://www.kaggle.com/account
2. Cliquez sur "Create New API Token"
3. T√©l√©chargez `kaggle.json`
4. Placez-le dans `~/.kaggle/kaggle.json` (Linux/Mac) ou `%USERPROFILE%\.kaggle\kaggle.json` (Windows)

```bash
# Linux/Mac
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Ou via variables d'environnement
export KAGGLE_USERNAME="votre_username"
export KAGGLE_KEY="votre_api_key"
```

### 2. Variables d'Environnement

```bash
# Base de donn√©es
export DATABASE_URL="postgresql://user:pass@localhost:5432/exaidb"

# Stockage d'objets
export STORAGE_BACKEND="minio"  # ou "azure"
export MINIO_ENDPOINT="localhost:9000"
export MINIO_ACCESS_KEY="minioadmin"
export MINIO_SECRET_KEY="minioadmin"
export MINIO_BUCKET="exai-datasets"
```

## Utilisation Locale

### Installation

```bash
cd datasets/kaggle-import
make install
```

### Test de Configuration

```bash
# Tester l'authentification Kaggle
make test-auth

# V√©rifier la configuration
make check-config

# Tester les services
make test-services
```

### Import de Datasets

```bash
# Lister les datasets disponibles
make list-datasets

# Importer tous les datasets
make import-all

# Importer seulement les petits datasets (recommand√© pour test)
make import-small

# Importer un dataset sp√©cifique
make import-dataset DATASET=student_performance

# Forcer le re-t√©l√©chargement (ignore le cache)
make force-refresh
```

### Monitoring

```bash
# Afficher l'√©tat des imports
make status

# Nettoyer les fichiers temporaires
make clean
```

## Utilisation en Production (Kubernetes)

### 1. Configuration des Secrets

```bash
# Encoder les credentials Kaggle en base64
echo -n "votre_username" | base64
echo -n "votre_api_key" | base64

# √âditer le secret Kaggle
kubectl edit secret kaggle-secrets -n exai
```

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: kaggle-secrets
  namespace: exai
data:
  username: <KAGGLE_USERNAME_BASE64>
  key: <KAGGLE_API_KEY_BASE64>
```

### 2. Lancement du Job

```bash
# Cr√©er le job
kubectl apply -f k8s/base/jobs/kaggle-dataset-import-job.yaml

# Suivre les logs
kubectl logs -f job/kaggle-dataset-import-job -n exai

# V√©rifier le statut
kubectl get jobs -n exai
kubectl describe job kaggle-dataset-import-job -n exai
```

### 3. Job R√©current (Optionnel)

Pour automatiser l'import quotidien :

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kaggle-import-cronjob
spec:
  schedule: "0 2 * * *"  # Tous les jours √† 2h du matin
  jobTemplate:
    spec:
      template:
        # Copier le contenu du job kaggle-dataset-import-job
```

## Configuration des Datasets

### Fichier de Configuration

`kaggle_datasets_config.yaml` contient la liste des datasets √† importer :

```yaml
datasets:
  student_performance:
    kaggle_ref: "spscientist/students-performance-in-exams"
    domain: "education"
    description: "Student performance analysis"
    ml_task: "classification"
    target_column: "math_score"
```

### Ajouter un Nouveau Dataset

1. **Trouver la r√©f√©rence Kaggle** :
   ```
   URL: https://www.kaggle.com/datasets/USERNAME/DATASET-NAME
   R√©f√©rence: "USERNAME/DATASET-NAME"
   ```

2. **Ajouter √† la configuration** :
   ```yaml
   nouveau_dataset:
     kaggle_ref: "username/dataset-name"
     domain: "votre_domaine"
     description: "Description du dataset"
     ml_task: "classification"  # ou "regression"
     target_column: "colonne_cible"
   ```

3. **Importer** :
   ```bash
   make import-dataset DATASET=nouveau_dataset
   ```

## Fonctionnalit√©s Avanc√©es

### Cache Intelligent

- Cache de 7 jours par d√©faut
- √âvite les re-t√©l√©chargements inutiles
- Fichiers cache dans `cache/`

### Gestion des Gros Datasets

Pour les datasets > 1GB :

```yaml
large_dataset: true  # Active la gestion sp√©ciale
chunk_size: 50000   # Traitement par chunks
```

### Multi-fichiers

Pour les datasets avec plusieurs CSV :

```yaml
multi_file: true  # G√®re automatiquement les fichiers multiples
```

### Conversion Parquet

- Compression Snappy par d√©faut
- Optimisation automatique des types
- Gains de performance 10-50x

## D√©pannage

### Erreurs Courantes

**‚ùå Erreur d'authentification Kaggle**
```bash
# V√©rifier les credentials
make test-auth

# V√©rifier le fichier kaggle.json
cat ~/.kaggle/kaggle.json
```

**‚ùå Erreur de connexion stockage**
```bash
# V√©rifier MinIO
docker ps | grep minio

# Tester la connexion
make test-services
```

**‚ùå Dataset introuvable**
```bash
# V√©rifier la r√©f√©rence Kaggle
kaggle datasets list -s "nom_dataset"

# V√©rifier la configuration
make check-config
```

### Logs D√©taill√©s

```bash
# Logs locaux
tail -f kaggle_import.log

# Logs Kubernetes
kubectl logs -f job/kaggle-dataset-import-job -n exai
```

### Nettoyage en Cas de Probl√®me

```bash
# Local
make clean

# Kubernetes
kubectl delete job kaggle-dataset-import-job -n exai
```

## Monitoring et M√©triques

### Statut des Imports

```bash
# Local
make status

# Kubernetes
kubectl get jobs -n exai
kubectl describe job kaggle-dataset-import-job -n exai
```

### M√©triques Importantes

- **Temps d'import** : Visible dans les logs
- **Taille des fichiers** : Avant/apr√®s conversion Parquet
- **Succ√®s/√©checs** : R√©sum√© final dans les logs
- **Utilisation cache** : Datasets ignor√©s car cache valide

## S√©curit√©

### Credentials

- Kaggle API key stock√© en secret Kubernetes
- Pas de credentials en dur dans le code
- Rotation recommand√©e tous les 90 jours

### Donn√©es

- Pas de stockage local permanent
- Upload direct vers stockage s√©curis√©
- Nettoyage automatique des fichiers temporaires

## √âvolutions Futures

### Pr√©vues

- [ ] Support d'autres sources (GitHub, URLs directes)
- [ ] Interface web pour la gestion
- [ ] Notifications Slack/Teams
- [ ] M√©triques Prometheus

### Possibles

- [ ] Import incr√©mental
- [ ] Compression avanc√©e
- [ ] Validation de qualit√© des donn√©es
- [ ] Export vers d'autres formats

## Support

### Documentation

- [Guide Architecture](../../docs/modules/ROOT/pages/dev-guide/batch-dataset-import-system.adoc)
- [Configuration Stockage](../../docs/modules/ROOT/pages/dev-guide/object-storage-implementation.adoc)

### Commandes d'Aide

```bash
# Aide g√©n√©rale
make help

# Lister les datasets
make list-datasets

# V√©rifier la configuration
make check-config
```

---

**üéâ Le syst√®me d'import Kaggle EXAI permet maintenant d'importer facilement des datasets de toute taille sans limitations GitHub !** 