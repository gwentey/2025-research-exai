"""
G√®re les op√©rations de base de donn√©es pour l'importation.

- Cr√©ation des enregistrements de dataset
- Cr√©ation des enregistrements de fichiers et de colonnes
- Mapping des m√©tadonn√©es vers les mod√®les SQLAlchemy
"""
import logging
from typing import Dict, Any, List
from pathlib import Path
import uuid
import json
import numpy as np

from sqlalchemy.orm import Session
from .database import get_db_session

# Import des mod√®les avec gestion automatique de l'environnement (local vs container)
try:
    # Essai import direct (fonctionnement local)
    from models import Dataset, DatasetFile, FileColumn
except ModuleNotFoundError:
    # Import depuis le container service-selection
    import sys
    import os
    
    # Dans le container: /app/kaggle-import/importer_lib/db_manager.py
    # Les mod√®les sont dans: /app/models/
    # Donc on doit remonter de /app/kaggle-import/importer_lib/ vers /app/
    current_file = os.path.abspath(__file__)
    kaggle_import_dir = os.path.dirname(os.path.dirname(current_file))  # /app/kaggle-import/
    app_root = os.path.dirname(kaggle_import_dir)  # /app/
    
    print(f"üîß DEBUG PYTHONPATH: Ajout de {app_root} au PYTHONPATH")
    sys.path.insert(0, app_root)
    
    from models import Dataset, DatasetFile, FileColumn

logger = logging.getLogger(__name__)

class DatabaseManager:
    """G√®re la persistance des m√©tadonn√©es du dataset en base."""

    def _convert_numpy_types(self, obj: Any) -> Any:
        """Convertit r√©cursivement les types numpy en types Python pour s√©rialisation JSON."""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return obj

    def save_dataset_metadata(
        self,
        dataset_name: str,
        dataset_config: Dict[str, Any],
        kaggle_metadata: Dict[str, Any],
        files_metadata: List[Dict[str, Any]],
        storage_path: str,
        dataset_uuid: str,
        file_uuid_mapping: Dict[str, str],
        enriched_metadata: Dict[str, Any] = None
    ) -> None:
        """
        Sauvegarde l'ensemble des m√©tadonn√©es pour un dataset et ses fichiers.
        """
        logger.info(f"=== D√âBUT SAUVEGARDE DATASET '{dataset_name}' (UUID: {dataset_uuid}) ===")
        logger.info(f"Config re√ßue: {dataset_config}")
        logger.info(f"Metadata Kaggle: {kaggle_metadata}")
        logger.info(f"Files metadata: {files_metadata}")
        
        with get_db_session() as db:
            try:
                # 1. Cr√©er l'enregistrement du Dataset
                logger.info("=== √âtape 1: Cr√©ation de l'enregistrement Dataset ===")
                dataset_record = self._create_dataset_record(
                    db,
                    dataset_name,
                    dataset_config,
                    kaggle_metadata,
                    storage_path,
                    dataset_uuid,
                    files_metadata,
                    enriched_metadata
                )
                logger.info(f"Dataset record cr√©√© avec ID: {dataset_record.id}")
                
                # 2. Cr√©er les enregistrements pour chaque fichier et ses colonnes
                logger.info("=== √âtape 2: Cr√©ation des enregistrements de fichiers ===")
                for file_meta in files_metadata:
                    logger.info(f"Cr√©ation enregistrement pour fichier: {file_meta}")
                    self._create_file_and_columns_records(
                        db,
                        dataset_record.id,
                        file_meta,
                        file_uuid_mapping
                    )

                logger.info("=== √âtape 3: Commit de la transaction ===")
                db.commit()
                logger.info(f"‚úÖ SUCCESS: Dataset '{dataset_name}' sauvegard√© avec succ√®s en base de donn√©es.")

            except Exception as e:
                logger.error(f"‚ùå ERREUR lors de la sauvegarde en base de donn√©es pour le dataset '{dataset_name}': {e}")
                logger.error(f"Type d'erreur: {type(e).__name__}")
                logger.error(f"D√©tails de l'erreur: {str(e)}")
                import traceback
                logger.error(f"Traceback complet: {traceback.format_exc()}")
                db.rollback()
                raise

    def _create_dataset_record(
        self,
        db: Session,
        dataset_name: str,
        dataset_config: Dict[str, Any],
        kaggle_metadata: Dict[str, Any],
        storage_path: str,
        dataset_uuid: str,
        files_metadata: List[Dict[str, Any]],
        enriched_metadata: Dict[str, Any] = None
    ) -> Dataset:
        """Cr√©e l'enregistrement principal pour le Dataset."""
        
        total_rows = sum(f.get('row_count', 0) for f in files_metadata)
        # Supposons que le nombre de features est le nombre de colonnes du premier fichier
        total_features = files_metadata[0].get('column_count', 0) if files_metadata else 0

        if enriched_metadata:
            # Utiliser les m√©tadonn√©es enrichies si disponibles
            logger.info("Utilisation des m√©tadonn√©es enrichies du KaggleMetadataMapper")
            dataset_data = enriched_metadata.copy()
            dataset_data.update({
                'id': dataset_uuid,
                'dataset_name': dataset_name,
                'storage_path': storage_path,
                'instances_number': total_rows,
                'features_number': total_features,
            })
        else:
            # Fallback vers l'ancienne m√©thode si pas de m√©tadonn√©es enrichies
            logger.info("Utilisation des m√©tadonn√©es basiques (fallback)")
            dataset_data = {
                'id': dataset_uuid,
                'dataset_name': dataset_name,
                'objective': kaggle_metadata.get('description', dataset_config.get('description', '')),
                'storage_uri': f"https://www.kaggle.com/datasets/{dataset_config.get('kaggle_ref', '')}",
                'domain': dataset_config.get('domain', []),
                'task': dataset_config.get('ml_task', []),  # 'task' est le bon nom de champ
                'storage_path': storage_path,
                'instances_number': total_rows,
                'features_number': total_features,
                **self._extract_ethical_guidelines(dataset_config)
            }
        
        logger.info(f"=== DONN√âES POUR CR√âATION DATASET ===")
        logger.info(f"dataset_data complet: {dataset_data}")
        
        try:
            dataset = Dataset(**dataset_data)
            logger.info(f"Objet Dataset SQLAlchemy cr√©√© avec succ√®s")
            db.add(dataset)
            logger.info(f"Dataset ajout√© √† la session SQLAlchemy")
            db.flush() # Pour s'assurer que l'ID est disponible pour les relations
            logger.info(f"Flush r√©ussi, Dataset ID: {dataset.id}")
            return dataset
        except Exception as e:
            logger.error(f"‚ùå ERREUR lors de la cr√©ation du Dataset: {e}")
            logger.error(f"Type d'erreur: {type(e).__name__}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise

    def _create_file_and_columns_records(
        self,
        db: Session,
        dataset_id: str,
        file_meta: Dict[str, Any],
        file_uuid_mapping: Dict[str, str]
    ) -> None:
        """Cr√©e les enregistrements pour un fichier et toutes ses colonnes."""
        
        original_filename = file_meta['original_filename']
        file_uuid = file_uuid_mapping[original_filename]
        
        file_record = DatasetFile(
            id=file_uuid,
            dataset_id=dataset_id,
            original_filename=original_filename,
            file_name_in_storage=f"{file_uuid}.parquet",
            format="parquet",
            size_bytes=int(file_meta.get('size_mb', 0) * 1024 * 1024),
            row_count=file_meta.get('row_count', 0),
        )
        db.add(file_record)
        db.flush()

        for col_info in file_meta.get('column_details', []):
            # Nettoyer les stats pour la s√©rialisation JSON
            clean_col_info = self._convert_numpy_types(col_info)
            logger.info(f"Cr√©ation colonne '{clean_col_info['column_name']}' avec stats: {clean_col_info.get('stats', {})}")
            
            column_record = FileColumn(
                dataset_file_id=file_record.id,
                **clean_col_info
            )
            db.add(column_record)

    def _extract_ethical_guidelines(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Extrait les directives √©thiques depuis la configuration."""
        # Valeurs par d√©faut prudentes
        return {
            'anonymization_applied': config.get('ethical_guidelines', {}).get('anonymization_applied', False),
            'informed_consent': config.get('ethical_guidelines', {}).get('informed_consent', False),
        }


