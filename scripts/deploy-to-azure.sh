#!/bin/bash

# ðŸš€ SCRIPT DE DÃ‰PLOIEMENT PRODUCTION IBIS-X SUR AZURE
# Usage: Infrastructure + DÃ©ploiement Production
# Pour le dÃ©veloppement local, utilisez: make dev

set -e

# ==========================================
# ðŸŽ¨ FONCTIONS DE LOGGING (DÃ‰FINIES EN PREMIER)
# ==========================================

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Fonctions utilitaires
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ==========================================
# ðŸŽ¯ CONFIGURATION PRODUCTION
# ==========================================

# DÃ©tection si exÃ©cutÃ© depuis GitHub Actions
IS_GITHUB_ACTIONS="${GITHUB_ACTIONS:-false}"
IS_WINDOWS=false

# Auto-dÃ©tection Windows
if [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]] || [[ -n "${WINDIR}" ]]; then
    IS_WINDOWS=true
fi

# Configuration production
if [[ "$IS_GITHUB_ACTIONS" == "true" ]]; then
    # ðŸ­ MODE GITHUB ACTIONS
    DEPLOYMENT_MODE="github-actions"
    
    # Variables d'environnement (depuis GitHub Actions)
    export AZURE_CONTAINER_REGISTRY="${AZURE_CONTAINER_REGISTRY:-ibisprodacr}"
    export AZURE_RESOURCE_GROUP="${AZURE_RESOURCE_GROUP:-ibis-x-perso-rg}"
    export AKS_CLUSTER_NAME="${AKS_CLUSTER_NAME:-ibis-x-prod-aks}"
    export K8S_NAMESPACE="${K8S_NAMESPACE:-ibis-x}"
    
    # Tags versionnÃ©s avec SHA du commit
    if [[ -n "$GITHUB_SHA" ]]; then
        export IMAGE_TAG="${GITHUB_SHA:0:7}"
    else
        export IMAGE_TAG="latest"
    fi
    
    export USE_GITHUB_SECRETS=true
    # âœ… TOUJOURS IMPORTER LES VRAIS DATASETS
    export WITH_DATA="true"
    export ANGULAR_ENV="production"
    
else
    # ðŸ› ï¸ MODE SCRIPT MANUEL
    DEPLOYMENT_MODE="manual-production"
    
    # Configuration par dÃ©faut (peut Ãªtre surchargÃ©e par variables d'environnement)
    export AZURE_CONTAINER_REGISTRY="${AZURE_CONTAINER_REGISTRY:-ibisprodacr}"
    export AZURE_RESOURCE_GROUP="${AZURE_RESOURCE_GROUP:-ibis-x-perso-rg}"
    export AKS_CLUSTER_NAME="${AKS_CLUSTER_NAME:-ibis-x-prod-aks}"
    export K8S_NAMESPACE="${K8S_NAMESPACE:-ibis-x}"
    export IMAGE_TAG="${IMAGE_TAG:-latest}"
    export USE_GITHUB_SECRETS=false
    # âœ… TOUJOURS IMPORTER LES VRAIS DATASETS (modifiÃ© de false Ã  true)
    export WITH_DATA="true"
    # âœ… FORCER PRODUCTION: Toujours en mode production pour Azure
    export ANGULAR_ENV="production"
    
    log_info "ðŸŽ¯ Mode Manuel Production - Frontend configurÃ© automatiquement en PRODUCTION"
    log_info "ðŸŽ¯ VRAIS DATASETS : WITH_DATA=true (toujours activÃ© pour production)"
fi

# ==========================================
# ðŸ“ VARIABLES DE CONFIGURATION
# ==========================================

# Script de dÃ©ploiement automatisÃ© pour IBIS-X sur Azure
# Ce script utilise Terraform pour crÃ©er l'infrastructure et dÃ©ploie l'application

# Variables de configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
TERRAFORM_DIR="$PROJECT_ROOT/terraform/azure-infrastructure"
K8S_DIR="$PROJECT_ROOT/k8s"

# DÃ©tection de l'OS pour adapter les commandes
if [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]] || command -v powershell &> /dev/null; then
    IS_WINDOWS=true
else
    IS_WINDOWS=false
fi

# Variables globales pour partager entre les fonctions
export STORAGE_ACCOUNT=""
export STORAGE_KEY=""
export ACR_NAME=""
export AKS_NAME=""
export RESOURCE_GROUP=""
export PUBLIC_IP=""

# ==========================================
# ðŸ“Š AFFICHAGE CONFIGURATION
# ==========================================

# Afficher la configuration dÃ©tectÃ©e
if [[ "$IS_GITHUB_ACTIONS" == "true" ]]; then
    log_info "ðŸ­ MODE: GitHub Actions"
    log_info "ðŸ“¦ ACR: $AZURE_CONTAINER_REGISTRY"
    log_info "ðŸ·ï¸ Tag images: $IMAGE_TAG"
    log_info "ðŸ” Secrets: GitHub Secrets (injectÃ©s)"
else
    log_info "ðŸ› ï¸ MODE: Script Manuel"
    log_info "ðŸ“¦ ACR: $AZURE_CONTAINER_REGISTRY"
    log_info "ðŸ·ï¸ Tag images: $IMAGE_TAG"
    log_info "ðŸ” Secrets: Configuration manuelle requise"
    log_info "â„¹ï¸  Utilisez 'make dev' pour le dÃ©veloppement local"
fi
log_info "ðŸŒ Frontend: Mode $ANGULAR_ENV"
log_info "ðŸ“Š DonnÃ©es: WITH_DATA=$WITH_DATA"

# Fonction pour vÃ©rifier les versions Kubernetes disponibles
check_kubernetes_versions() {
    log_info "VÃ©rification des versions Kubernetes disponibles..."
    
    # Obtenir la rÃ©gion depuis terraform.tfvars ou utiliser une valeur par dÃ©faut
    local region="East US"
    if [ -f "$TERRAFORM_DIR/terraform.tfvars" ]; then
        region=$(grep -E '^location\s*=' "$TERRAFORM_DIR/terraform.tfvars" | cut -d'"' -f2 2>/dev/null || echo "East US")
    fi
    
    # VÃ©rifier les versions disponibles
    local available_versions=$(az aks get-versions --location "$region" --output json | jq -r '.orchestrators[] | select(.supportPlan[] | contains("KubernetesOfficial")) | .orchestratorVersion' | sort -V)
    
    if [ -z "$available_versions" ]; then
        log_warning "Impossible de rÃ©cupÃ©rer les versions Kubernetes. VÃ©rifiez votre connexion Azure."
        return
    fi
    
    # Afficher les versions recommandÃ©es
    local recommended_versions=$(echo "$available_versions" | tail -5)
    log_info "Versions Kubernetes recommandÃ©es pour la rÃ©gion '$region':"
    echo "$recommended_versions" | while read version; do
        echo "  - $version"
    done
    
    # VÃ©rifier si la version configurÃ©e est supportÃ©e
    local current_version=""
    if [ -f "$TERRAFORM_DIR/terraform.tfvars" ]; then
        current_version=$(grep -E '^kubernetes_version\s*=' "$TERRAFORM_DIR/terraform.tfvars" | cut -d'"' -f2 2>/dev/null)
    fi
    
    if [ -n "$current_version" ]; then
        if echo "$available_versions" | grep -q "^$current_version$"; then
            log_success "Version Kubernetes configurÃ©e ($current_version) est supportÃ©e"
        else
            log_warning "Version Kubernetes configurÃ©e ($current_version) n'est pas supportÃ©e !"
            log_warning "Versions recommandÃ©es : $(echo "$recommended_versions" | tail -3 | tr '\n' ' ')"
        fi
    fi
}

# Fonction pour vÃ©rifier les prÃ©requis
check_prerequisites() {
    log_info "VÃ©rification des prÃ©requis..."
    
    # VÃ©rifier Azure CLI
    if ! command -v az &> /dev/null; then
        log_error "Azure CLI n'est pas installÃ©. Installez-le depuis https://docs.microsoft.com/en-us/cli/azure/install-azure-cli"
        exit 1
    fi
    
    # VÃ©rifier/Installer Terraform
    if ! command -v terraform &> /dev/null; then
        log_warning "Terraform n'est pas installÃ©. Installation automatique..."
        if [[ "$IS_WINDOWS" == true ]]; then
            if command -v winget &> /dev/null; then
                winget install HashiCorp.Terraform
                # Recharger les variables d'environnement
                if command -v refreshenv &> /dev/null; then
                    refreshenv
                fi
            else
                log_error "Terraform non installÃ© et winget indisponible. Installez manuellement depuis https://www.terraform.io/downloads.html"
        exit 1
            fi
        else
            # Installation sur Linux/MacOS
            if command -v wget &> /dev/null && command -v unzip &> /dev/null; then
                TF_VERSION="1.6.0"
                wget "https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip"
                unzip "terraform_${TF_VERSION}_linux_amd64.zip"
                sudo mv terraform /usr/local/bin/
                rm "terraform_${TF_VERSION}_linux_amd64.zip"
            else
                log_error "Terraform non installÃ©. Installez manuellement depuis https://www.terraform.io/downloads.html"
                exit 1
            fi
        fi
        
        # VÃ©rifier que l'installation a rÃ©ussi
        if ! command -v terraform &> /dev/null; then
            log_error "Ã‰chec de l'installation automatique de Terraform. Installez manuellement."
            exit 1
        else
            log_success "Terraform installÃ© avec succÃ¨s"
        fi
    fi
    
    # VÃ©rifier kubectl
    if ! command -v kubectl &> /dev/null; then
        log_error "kubectl n'est pas installÃ©. Installez-le depuis https://kubernetes.io/docs/tasks/tools/"
        exit 1
    fi
    
    # VÃ©rifier Docker
    if ! command -v docker &> /dev/null; then
        log_error "Docker n'est pas installÃ©. Installez-le depuis https://docs.docker.com/get-docker/"
        exit 1
    fi
    
    # VÃ©rifier/Installer jq pour le parsing JSON
    if ! command -v jq &> /dev/null; then
        log_warning "jq n'est pas installÃ©. Installation automatique..."
        if [[ "$IS_WINDOWS" == true ]]; then
            if command -v winget &> /dev/null; then
                winget install jqlang.jq
            else
                log_warning "jq non installÃ©. InstallÃ© manuellement depuis https://jqlang.github.io/jq/"
            fi
        else
            if command -v apt-get &> /dev/null; then
                sudo apt-get install -y jq
            elif command -v yum &> /dev/null; then
                sudo yum install -y jq
            elif command -v brew &> /dev/null; then
                brew install jq
            else
                log_warning "jq non installÃ©. InstallÃ© manuellement depuis https://jqlang.github.io/jq/"
            fi
        fi
    fi
    
    # VÃ©rifier/Installer Helm
    if ! command -v helm &> /dev/null; then
        log_warning "Helm n'est pas installÃ©. Installation automatique..."
        install_helm
    else
        log_success "Helm est dÃ©jÃ  installÃ©"
    fi
    
    log_success "Tous les prÃ©requis sont installÃ©s"
}

# Nouvelle fonction pour installer Helm
install_helm() {
    log_info "Installation de Helm..."
    
    # TÃ©lÃ©charger et installer Helm
    curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
    chmod 700 get_helm.sh
    ./get_helm.sh
    rm get_helm.sh
    
    # VÃ©rifier l'installation
    if command -v helm &> /dev/null; then
        log_success "Helm installÃ© avec succÃ¨s: $(helm version --short)"
    else
        log_error "Ã‰chec de l'installation de Helm"
        exit 1
    fi
}

# âœ… FONCTION AUTOMATIQUE RENFORCÃ‰E : Configuration IP statique pour NGINX Ingress
configure_static_ip_for_nginx() {
    log_info "ðŸ”§ Configuration automatique RENFORCÃ‰E de l'IP statique pour NGINX Ingress..."
    
    # 1. RÃ©cupÃ©rer l'IP statique depuis Azure CLI (plus fiable que Terraform)
    local static_ip=""
    local node_resource_group=""
    
    # Trouver le resource group automatique crÃ©Ã© par AKS
    node_resource_group=$(az group list --query "[?contains(name, 'MC_${RESOURCE_GROUP}_${AKS_NAME}')].name" --output tsv 2>/dev/null | head -1)
    
    if [[ -n "$node_resource_group" ]]; then
        log_info "ðŸ” Recherche IP statique dans : $node_resource_group..."
        
        # Nom standard de l'IP statique pour ingress
        local ip_name="ibis-x-prod-ingress-ip"
        
        # Essayer de rÃ©cupÃ©rer l'IP statique existante
        static_ip=$(az network public-ip show --resource-group "$node_resource_group" --name "$ip_name" --query "ipAddress" --output tsv 2>/dev/null || echo "")
        
        # Si l'IP n'existe pas, la crÃ©er OBLIGATOIREMENT
        if [[ -z "$static_ip" ]]; then
            log_info "ðŸ”§ CrÃ©ation OBLIGATOIRE de l'IP statique manquante..."
            local create_result=$(az network public-ip create \
                --resource-group "$node_resource_group" \
                --name "$ip_name" \
                --allocation-method Static \
                --sku Standard \
                --location eastus \
                --query "publicIp.ipAddress" --output tsv 2>/dev/null || echo "")
            
            if [[ -n "$create_result" ]]; then
                static_ip="$create_result"
                log_success "âœ… IP statique crÃ©Ã©e : $static_ip"
            else
                log_error "âŒ Ã‰CHEC critique : Impossible de crÃ©er l'IP statique !"
                exit 1
            fi
        else
            log_success "âœ… IP statique existante trouvÃ©e : $static_ip"
        fi
        
        # Double vÃ©rification : s'assurer que l'IP existe et est accessible
        local ip_exists=$(az network public-ip show --resource-group "$node_resource_group" --name "$ip_name" --query "ipAddress" --output tsv 2>/dev/null || echo "")
        if [[ -z "$ip_exists" ]] || [[ "$ip_exists" != "$static_ip" ]]; then
            log_error "âŒ ERREUR CRITIQUE : L'IP statique est incohÃ©rente (trouvÃ©e: $ip_exists, attendue: $static_ip) !"
            exit 1
        fi
        
        log_success "âœ… VÃ©rification IP statique confirmÃ©e : $static_ip (accessible et cohÃ©rente)"
    else
        log_error "âŒ ERREUR CRITIQUE : Resource group AKS non trouvÃ© !"
        exit 1
    fi
    
    # 3. VALIDATION OBLIGATOIRE
    if [[ -z "$static_ip" ]] || [[ -z "$node_resource_group" ]]; then
        log_error "âŒ ERREUR CRITIQUE : Impossible de configurer l'IP statique !"
        log_error "   IP statique: ${static_ip:-NON TROUVÃ‰E}"
        log_error "   Node RG: ${node_resource_group:-NON TROUVÃ‰}"
        exit 1
    fi
    
    # 4. Mettre Ã  jour automatiquement le fichier nginx-ingress-values.yaml
    local nginx_values_file="$K8S_DIR/helm-values/nginx-ingress-values.yaml"
    local nginx_values_backup="$nginx_values_file.backup-$(date +%s)"
    
    log_info "ðŸ“ Mise Ã  jour FORCÃ‰E de nginx-ingress-values.yaml..."
    log_info "   ðŸŽ¯ IP statique FORCÃ‰E: $static_ip"
    log_info "   ðŸŽ¯ Resource Group: $node_resource_group"
    
    # Sauvegarder le fichier original
    cp "$nginx_values_file" "$nginx_values_backup" 2>/dev/null || true
    
    # CrÃ©er la nouvelle configuration avec IP statique FORCÃ‰E
    cat > "$nginx_values_file" << EOF
controller:
  replicaCount: 2
  nodeSelector:
    kubernetes.io/os: linux
  service:
    type: LoadBalancer
    loadBalancerIP: "$static_ip"
    annotations:
      # FORCE L'IP STATIQUE - Configuration par deploy-to-azure.sh
      service.beta.kubernetes.io/azure-load-balancer-static-ip: "$static_ip"
      service.beta.kubernetes.io/azure-load-balancer-resource-group: "$node_resource_group"
      service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: /healthz
      # EMPÃŠCHER Azure d'utiliser une IP dynamique
      service.beta.kubernetes.io/azure-load-balancer-mode: "shared"
  admissionWebhooks:
    patch:
      nodeSelector:
        kubernetes.io/os: linux

defaultBackend:
  nodeSelector:
    kubernetes.io/os: linux
EOF
    
    # Exporter pour usage ultÃ©rieur
    export PUBLIC_IP="$static_ip"
    export NODE_RESOURCE_GROUP="$node_resource_group"
    export STATIC_IP_CONFIRMED="$static_ip"
    
    log_success "âœ… Configuration IP statique FORCÃ‰E terminÃ©e !"
    log_success "   ðŸŽ¯ NGINX utilisera OBLIGATOIREMENT l'IP: $static_ip"
    log_success "   ðŸ’¾ Sauvegarde: $nginx_values_backup"
}

# âœ… FONCTION RENFORCÃ‰E pour installer NGINX Ingress Controller avec IP statique FORCÃ‰E
install_nginx_ingress() {
    log_info "ðŸš€ Installation RENFORCÃ‰E de NGINX Ingress Controller avec IP statique FORCÃ‰E..."
    
    # âœ… AUTOMATISATION IP STATIQUE RENFORCÃ‰E
    configure_static_ip_for_nginx
    
    # Utiliser les variables exportÃ©es par configure_static_ip_for_nginx
    local static_ip="$STATIC_IP_CONFIRMED"
    local node_resource_group="$NODE_RESOURCE_GROUP"
    
    if [[ -z "$static_ip" ]]; then
        log_error "âŒ ERREUR CRITIQUE : IP statique non configurÃ©e !"
        exit 1
    fi
    
    log_info "ðŸŽ¯ IP statique confirmÃ©e pour NGINX : $static_ip"
    
    # VÃ©rifier si NGINX existe dÃ©jÃ  avec une IP diffÃ©rente
    local existing_nginx_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
    
    if [[ -n "$existing_nginx_ip" && "$existing_nginx_ip" != "$static_ip" ]]; then
        log_warning "âš ï¸ NGINX existant avec IP incorrecte ($existing_nginx_ip â‰  $static_ip)"
        log_info "ðŸ”„ Suppression et recrÃ©ation de NGINX avec IP statique..."
        helm uninstall ingress-nginx -n ingress-nginx 2>/dev/null || true
        kubectl delete namespace ingress-nginx --ignore-not-found=true
        sleep 10
    fi
    
    # Ajouter le repository Helm NGINX
    helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
    helm repo update
    
    # Installation FORCÃ‰E avec toutes les annotations IP statique
    log_info "ðŸ“¦ Installation NGINX avec IP statique FORCÃ‰E : $static_ip"
    helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
        --namespace ingress-nginx \
        --create-namespace \
        --set controller.service.loadBalancerIP="$static_ip" \
        --set controller.service.type=LoadBalancer \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-static-ip=$static_ip" \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-resource-group=$node_resource_group" \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-health-probe-request-path=/healthz" \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-mode=shared" \
        --set controller.replicaCount=2 \
        --set "controller.nodeSelector.kubernetes\.io/os=linux" \
        --set "defaultBackend.nodeSelector.kubernetes\.io/os=linux" \
        --wait --timeout=10m
    
    # âœ… VALIDATION POST-INSTALLATION : VÃ©rifier que NGINX utilise bien l'IP statique
    log_info "ðŸ” VALIDATION : VÃ©rification de l'IP assignÃ©e Ã  NGINX..."
    
    local max_attempts=30
    local attempt=0
    local nginx_actual_ip=""
    
    while [[ $attempt -lt $max_attempts ]]; do
        nginx_actual_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
        
        if [[ "$nginx_actual_ip" == "$static_ip" ]]; then
            log_success "âœ… SUCCÃˆS : NGINX utilise l'IP statique correcte : $nginx_actual_ip"
            break
        elif [[ -n "$nginx_actual_ip" && "$nginx_actual_ip" != "$static_ip" ]]; then
            log_error "âŒ Ã‰CHEC CRITIQUE : NGINX utilise une IP incorrecte !"
            log_error "   IP attendue : $static_ip"
            log_error "   IP actuelle : $nginx_actual_ip"
            log_info "ðŸ”„ Tentative de correction..."
            
            # Forcer la correction
            kubectl patch svc ingress-nginx-controller -n ingress-nginx -p "{\"spec\":{\"loadBalancerIP\":\"$static_ip\"}}"
        fi
        
        attempt=$((attempt + 1))
        log_info "â³ Attente IP statique... ($attempt/$max_attempts) - IP actuelle: ${nginx_actual_ip:-PENDING}"
        sleep 10
    done
    
    # Validation finale
    nginx_actual_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
    
    if [[ "$nginx_actual_ip" == "$static_ip" ]]; then
        log_success "ðŸŽ‰ NGINX Ingress Controller installÃ© avec IP statique CONFIRMÃ‰E : $static_ip"
        
        # Afficher l'Ã©tat final pour confirmation
        log_info "ðŸ“Š Ã‰tat final NGINX Ingress :"
        kubectl get svc -n ingress-nginx ingress-nginx-controller
    else
        log_error "âŒ Ã‰CHEC FINAL : NGINX n'utilise pas l'IP statique !"
        log_error "   IP attendue : $static_ip"
        log_error "   IP actuelle : ${nginx_actual_ip:-AUCUNE}"
        log_error "ðŸ”§ Veuillez corriger manuellement ou relancer le script"
        exit 1
    fi
}

# Nouvelle fonction pour installer Cert-Manager
install_cert_manager() {
    log_info "Installation de Cert-Manager..."
    
    # Ajouter le repository Helm Cert-Manager
    helm repo add jetstack https://charts.jetstack.io
    helm repo update
    
    # Installer Cert-Manager
    helm upgrade --install cert-manager jetstack/cert-manager \
        --namespace cert-manager \
        --create-namespace \
        --version v1.13.0 \
        --set installCRDs=true \
        --wait
    
    log_success "Cert-Manager installÃ©"
}

# Fonction pour vÃ©rifier que les noms de projet sont corrects
verify_project_names() {
    log_info "VÃ©rification des noms de projet IBIS-X..."
    
    # Les noms sont dÃ©jÃ  corrects depuis la migration, pas besoin de modification
    log_success "Noms de projet IBIS-X vÃ©rifiÃ©s"
}

# Fonction pour vÃ©rifier la connexion Azure
check_azure_login() {
    log_info "VÃ©rification de la connexion Azure..."
    
    if ! az account show &> /dev/null; then
        log_warning "Vous n'Ãªtes pas connectÃ© Ã  Azure. Connexion en cours..."
        az login
    fi
    
    # Afficher le compte actuel
    ACCOUNT_INFO=$(az account show --output json)
    SUBSCRIPTION_NAME=$(echo "$ACCOUNT_INFO" | jq -r '.name')
    SUBSCRIPTION_ID=$(echo "$ACCOUNT_INFO" | jq -r '.id')
    
    log_success "ConnectÃ© Ã  Azure:"
    echo "  Subscription: $SUBSCRIPTION_NAME"
    echo "  ID: $SUBSCRIPTION_ID"
}

# Fonction pour initialiser Terraform
init_terraform() {
    log_info "Initialisation de Terraform..."
    
    cd "$TERRAFORM_DIR"
    
    # VÃ©rifier si terraform.tfvars existe
    if [ ! -f "terraform.tfvars" ]; then
        log_warning "Le fichier terraform.tfvars n'existe pas."
        echo "CrÃ©ation du fichier terraform.tfvars Ã  partir du template..."
        cp terraform.tfvars.example terraform.tfvars
        
        log_warning "IMPORTANT: Veuillez modifier le fichier terraform.tfvars selon vos besoins avant de continuer."
        read -p "Appuyez sur EntrÃ©e pour ouvrir le fichier terraform.tfvars dans l'Ã©diteur par dÃ©faut..."
        
        # Ouvrir l'Ã©diteur
        ${EDITOR:-nano} terraform.tfvars
        
        echo
        read -p "Avez-vous terminÃ© la configuration du fichier terraform.tfvars ? (y/N): " confirm
        if [[ $confirm != [yY] && $confirm != [yY][eE][sS] ]]; then
            log_error "Configuration annulÃ©e. Veuillez configurer terraform.tfvars et relancer le script."
            exit 1
        fi
    fi
    
    # Initialiser Terraform
    terraform init
    
    log_success "Terraform initialisÃ© avec succÃ¨s"
}

# Fonction pour planifier et appliquer Terraform
deploy_infrastructure() {
    log_info "DÃ©ploiement de l'infrastructure Azure..."
    
    cd "$TERRAFORM_DIR"
    
    # Planifier le dÃ©ploiement
    log_info "GÃ©nÃ©ration du plan Terraform..."
    terraform plan -out=tfplan
    
    # Demander confirmation
    echo
    log_warning "Terraform va crÃ©er/modifier les ressources Azure ci-dessus."
    read -p "Voulez-vous continuer ? (y/N): " confirm
    
    if [[ $confirm != [yY] && $confirm != [yY][eE][sS] ]]; then
        log_error "DÃ©ploiement annulÃ© par l'utilisateur"
        exit 1
    fi
    
    # Appliquer le plan
    log_info "Application du plan Terraform..."
    terraform apply tfplan
    
    log_success "Infrastructure Azure dÃ©ployÃ©e avec succÃ¨s !"
}

# Fonction pour rÃ©cupÃ©rer les informations d'infrastructure
get_terraform_outputs() {
    log_info "ðŸ“Š RÃ©cupÃ©ration des informations d'infrastructure - Mode: $DEPLOYMENT_MODE"
    
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        # ðŸ­ MODE GITHUB ACTIONS: Variables d'environnement prÃ©dÃ©finies
        log_info "ðŸ­ Utilisation des variables GitHub Actions..."
        get_github_infrastructure_info
    else
        # ðŸ› ï¸ MODE MANUEL: Variables d'environnement ou dÃ©tection Azure CLI
        log_info "ðŸ› ï¸ Configuration manuelle ou dÃ©tection Azure..."
        get_manual_infrastructure_info
    fi
    
    # Validation finale
    if [[ -z "$ACR_NAME" ]] || [[ -z "$AKS_NAME" ]] || [[ -z "$RESOURCE_GROUP" ]]; then
        log_error "âŒ Informations d'infrastructure manquantes !"
        log_error "ACR: $ACR_NAME | AKS: $AKS_NAME | RG: $RESOURCE_GROUP"
        exit 1
    fi
    
    # Mise Ã  jour automatique de tous les fichiers avec le bon nom ACR
    log_info "ðŸ”§ Mise Ã  jour automatique des fichiers avec ACR: $ACR_NAME"
    update_all_acr_references
    
    log_success "âœ… Informations d'infrastructure rÃ©cupÃ©rÃ©es:"
    echo "  Mode: $DEPLOYMENT_MODE"
    echo "  ACR Registry: $ACR_NAME"
    echo "  AKS Cluster: $AKS_NAME"
    echo "  Resource Group: $RESOURCE_GROUP"
    echo "  Storage Account: ${STORAGE_ACCOUNT:-N/A}"
    echo "  Public IP: ${PUBLIC_IP:-N/A}"
}

# Fonction pour rÃ©cupÃ©rer les infos GitHub Actions
get_github_infrastructure_info() {
    log_info "ðŸ“‹ Configuration depuis variables d'environnement GitHub Actions..."
    
    # Les variables sont dÃ©jÃ  dÃ©finies par GitHub Actions
    export ACR_NAME="$AZURE_CONTAINER_REGISTRY"
    export AKS_NAME="$AKS_CLUSTER_NAME"
    export RESOURCE_GROUP="$AZURE_RESOURCE_GROUP"
    
    # RÃ©cupÃ©rer l'IP publique si possible
    PUBLIC_IP=$(az network public-ip list --resource-group "$RESOURCE_GROUP" --query "[0].ipAddress" -o tsv 2>/dev/null || echo "")
    export PUBLIC_IP
    
    # Storage sera rÃ©cupÃ©rÃ© dynamiquement par les fonctions de secrets
    log_success "âœ… Configuration GitHub Actions chargÃ©e"
}

# Fonction pour rÃ©cupÃ©rer les infos manuellement
get_manual_infrastructure_info() {
    log_info "ðŸ› ï¸ Configuration manuelle - Variables d'environnement ou Azure CLI..."
    
    # Utiliser les variables d'environnement prÃ©dÃ©finies (dÃ©jÃ  exportÃ©es)
    export ACR_NAME="$AZURE_CONTAINER_REGISTRY"
    export AKS_NAME="$AKS_CLUSTER_NAME"
    export RESOURCE_GROUP="$AZURE_RESOURCE_GROUP"
    
    # Si l'infrastructure a Ã©tÃ© crÃ©Ã©e par Terraform, essayer de rÃ©cupÃ©rer depuis Terraform
    if [[ -f "$TERRAFORM_DIR/terraform.tfstate" ]]; then
        log_info "ðŸ“‚ Infrastructure Terraform dÃ©tectÃ©e, rÃ©cupÃ©ration des outputs..."
        cd "$TERRAFORM_DIR"
        
        # RÃ©cupÃ©rer les outputs Terraform
        # RÃ©cupÃ©rer les outputs Terraform (dÃ©jÃ  dans TERRAFORM_DIR)
        local tf_storage=$(terraform output -raw storage_account_name 2>/dev/null || echo "")
        local tf_acr=$(terraform output -raw acr_name 2>/dev/null || echo "")
        local tf_aks=$(terraform output -raw aks_cluster_name 2>/dev/null || echo "")
        local tf_rg=$(terraform output -raw resource_group_name 2>/dev/null || echo "")
        local tf_ip=$(terraform output -raw public_ip_address 2>/dev/null || echo "")
        
        # Utiliser les valeurs Terraform si disponibles
        if [[ -n "$tf_acr" ]]; then
            export ACR_NAME="$tf_acr"
            export AKS_NAME="$tf_aks"
            export RESOURCE_GROUP="$tf_rg"
            export STORAGE_ACCOUNT="$tf_storage"
            export PUBLIC_IP="$tf_ip"
            
            if [[ -n "$tf_storage" ]]; then
                STORAGE_KEY=$(terraform output -raw storage_account_primary_key 2>/dev/null || echo "")
                export STORAGE_KEY
            fi
            
            log_success "âœ… Configuration rÃ©cupÃ©rÃ©e depuis Terraform"
        else
            log_warning "âš ï¸ Terraform outputs vides, utilisation des variables d'environnement"
        fi
        
        cd "$PROJECT_ROOT"
    fi
    
    # Fallback Azure CLI pour rÃ©cupÃ©rer l'IP publique
    if [[ -z "$PUBLIC_IP" ]]; then
        PUBLIC_IP=$(az network public-ip list --resource-group "$RESOURCE_GROUP" --query "[0].ipAddress" -o tsv 2>/dev/null || echo "")
        export PUBLIC_IP
    fi
    
    log_info "ðŸ“‹ Configuration finale:"
    log_info "  ACR: $ACR_NAME"
    log_info "  AKS: $AKS_NAME"
    log_info "  Resource Group: $RESOURCE_GROUP"
    
    log_success "âœ… Configuration manuelle chargÃ©e"
}

# Fonction pour l'initialisation automatique des donnÃ©es (selon environnement)
initialize_sample_data() {
    if [[ "$WITH_DATA" == "true" ]]; then
        log_info "ðŸ“Š Initialisation des donnÃ©es d'exemple..."
        
        # Attendre que service-selection soit prÃªt
        log_info "â³ Attente que service-selection soit prÃªt..."
        kubectl wait --for=condition=ready pod -l app=service-selection -n "${K8S_NAMESPACE:-ibis-x}" --timeout=5m
        
        # Initialiser les donnÃ©es
        log_info "ðŸš€ Lancement de l'initialisation des datasets..."
        kubectl exec -n "${K8S_NAMESPACE:-ibis-x}" deployment/service-selection -- python scripts/init_datasets.py all
        
        log_success "âœ… DonnÃ©es d'exemple initialisÃ©es"
    else
        log_info "ðŸ“Š Initialisation des donnÃ©es dÃ©sactivÃ©e (WITH_DATA=$WITH_DATA)"
    fi
}

# Fonction pour attendre et vÃ©rifier les migrations
wait_for_migrations() {
    log_info "â³ Attente des migrations de base de donnÃ©es..."
    
    # Attendre PostgreSQL
    log_info "ðŸ—„ï¸ Attente que PostgreSQL soit prÃªt..."
    kubectl wait pod --selector=app=postgresql --for=condition=Ready -n "${K8S_NAMESPACE:-ibis-x}" --timeout=5m
    
    # Attendre les migrations
    log_info "ðŸ”„ Attente des jobs de migration..."
    if kubectl wait --for=condition=complete job/api-gateway-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --timeout=5m 2>/dev/null; then
        log_success "âœ… Migration API Gateway terminÃ©e"
    else
        log_warning "âš ï¸ Migration API Gateway non trouvÃ©e ou Ã©chouÃ©e"
    fi
    
    if kubectl wait --for=condition=complete job/service-selection-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --timeout=5m 2>/dev/null; then
        log_success "âœ… Migration Service Selection terminÃ©e"
    else
        log_warning "âš ï¸ Migration Service Selection non trouvÃ©e ou Ã©chouÃ©e"
    fi
    
    # RedÃ©marrer les applications (comme dans GitHub Actions)
    if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
        log_info "ðŸ”„ RedÃ©marrage des applications (mode production)..."
        kubectl rollout restart deployment api-gateway -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment service-selection -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
    fi
    
    log_success "âœ… Migrations et redÃ©marrages terminÃ©s"
}

# Fonction de nettoyage des jobs (comme dans GitHub Actions)
cleanup_migration_jobs() {
    if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
        log_info "ðŸ§¹ Nettoyage des jobs de migration (mode production)..."
        kubectl delete job api-gateway-migration-job service-selection-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --ignore-not-found=true
        log_success "âœ… Jobs de migration nettoyÃ©s"
    else
        log_info "ðŸ§¹ Conservation des jobs de migration (mode dÃ©veloppement)"
    fi
}

# Fonction pour configurer kubectl
configure_kubectl() {
    log_info "Configuration de kubectl pour AKS..."
    
    # RÃ©cupÃ©rer les credentials AKS
    az aks get-credentials --resource-group "$RESOURCE_GROUP" --name "$AKS_NAME" --overwrite-existing
    
    # VÃ©rifier la connexion
    kubectl cluster-info
    
    log_success "kubectl configurÃ© pour AKS"
}

# Fonction pour mettre Ã  jour les secrets Kubernetes
update_k8s_secrets() {
    log_info "Mise Ã  jour des secrets Kubernetes..."
    
    # CrÃ©er le namespace s'il n'existe pas
    kubectl create namespace ibis-x --dry-run=client -o yaml | kubectl apply -f -
    
    # Les secrets seront crÃ©Ã©s automatiquement par create_missing_secrets()
    # Cette fonction se contente de prÃ©parer le namespace
    
    log_success "Namespace et prÃ©paration des secrets terminÃ©s"
}

# âœ… FONCTION AUTOMATIQUE : Configuration Frontend pour Production
configure_frontend_for_production() {
    log_info "ðŸŒ Configuration automatique du frontend pour la production..."
    
    if [[ "$ANGULAR_ENV" == "production" ]]; then
        log_info "ðŸ“ Mise Ã  jour FORCÃ‰E des URLs frontend pour le domaine de production..."
        
        # Sauvegarder le fichier environment.prod.ts
        local env_file="$PROJECT_ROOT/frontend/src/environments/environment.prod.ts"
        local env_backup="$env_file.backup-$(date +%s)"
        
        if [[ -f "$env_file" ]]; then
            cp "$env_file" "$env_backup"
            log_info "ðŸ’¾ Sauvegarde: $env_backup"
        fi
        
        # URLs de production FIXES
        local api_url="https://api.ibisx.fr"
        local frontend_domain="ibisx.fr"
        
        log_info "ðŸŽ¯ CONFIGURATION PRODUCTION FORCÃ‰E:"
        log_info "   API URL: $api_url"
        log_info "   Frontend Domain: $frontend_domain"
        log_info "   Production Mode: TRUE"
        
        # Si PUBLIC_IP est disponible, on peut aussi le proposer comme fallback
        if [[ -n "$PUBLIC_IP" ]] && [[ "$PUBLIC_IP" != "N/A" ]]; then
            log_info "ðŸ“¡ IP publique statique: $PUBLIC_IP"
        fi
        
        # CrÃ©er le fichier environment.prod.ts avec les bonnes URLs (FORCÃ‰)
        cat > "$env_file" << EOF
export const environment = {
  production: true,
  // URL publique de l'API Gateway via l'Ingress Controller - toujours en HTTPS
  apiUrl: '$api_url',
  // Domaine de production pour le frontend
  productionDomain: '$frontend_domain'
};
EOF
        
        log_success "âœ… Frontend FORCÃ‰ en mode production:"
        log_success "   âœ… production: true"
        log_success "   âœ… API URL: $api_url"
        log_success "   âœ… Domaine: $frontend_domain"
        log_success "   ðŸ“„ Fichier: $env_file"
        
        # VÃ©rifier que le fichier est correct
        if grep -q "https://api.ibisx.fr" "$env_file" && grep -q "production: true" "$env_file"; then
            log_success "âœ… VÃ©rification OK - URLs de production confirmÃ©es"
        else
            log_error "âŒ ERREUR: Configuration frontend incorrecte !"
            cat "$env_file"
            exit 1
        fi
    else
        log_info "ðŸ› ï¸ Mode dÃ©veloppement - configuration frontend inchangÃ©e"
    fi
}

# Fonction INTELLIGENTE pour construire et pousser les images Docker
build_and_push_images() {
    log_info "ðŸ—ï¸ Construction et push des images Docker vers ACR..."
    log_info "ðŸ·ï¸ Mode: $DEPLOYMENT_MODE | Tag: $IMAGE_TAG | Angular: $ANGULAR_ENV"
    
    # âœ… NOUVEAU: Configurer le frontend avant le build
    configure_frontend_for_production
    
    # âœ… FORCER REBUILD FRONTEND EN PRODUCTION: Supprimer l'image locale existante
    if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$ANGULAR_ENV" == "production" ]]; then
        log_info "ðŸ”„ Suppression de l'image frontend existante pour forcer le rebuild en production..."
        docker rmi "$ACR_NAME.azurecr.io/frontend:latest" 2>/dev/null || true
        docker rmi "frontend:latest" 2>/dev/null || true
        log_info "âœ… Images frontend locales supprimÃ©es - rebuild forcÃ©"
    fi
    
    # Se connecter Ã  ACR
    az acr login --name "$ACR_NAME"
    
    cd "$PROJECT_ROOT"
    
    # Fonction helper pour build/push avec gestion intelligente des tags
    build_and_push_image() {
        local service_name="$1"
        local dockerfile_path="$2"
        local build_context="$3"
        local build_args="$4"
        
        log_info "ðŸ“¦ Construction de l'image $service_name..."
        
        # Tags selon l'environnement
        local base_image="$ACR_NAME.azurecr.io/$service_name"
        local tags_args=""
        
        if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
            # En production : tag avec SHA + latest
            tags_args="-t $base_image:$IMAGE_TAG -t $base_image:latest"
        else
            # En local : seulement latest
            tags_args="-t $base_image:latest"
        fi
        
        # Construire avec les arguments appropriÃ©s
        local docker_cmd="docker build $tags_args $build_args -f $dockerfile_path $build_context"
        
        if eval "$docker_cmd"; then
            # Pousser tous les tags
            if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
                docker push "$base_image:$IMAGE_TAG"
                docker push "$base_image:latest"
                log_success "âœ… Image $service_name pushÃ©e (tags: $IMAGE_TAG, latest)"
            else
                docker push "$base_image:latest"
                log_success "âœ… Image $service_name pushÃ©e (tag: latest)"
            fi
        else
            log_error "âŒ Ã‰chec construction de l'image $service_name"
            exit 1
        fi
    }
    
    # 1. API Gateway
    build_and_push_image "ibis-x-api-gateway" "api-gateway/Dockerfile" "api-gateway/" ""
    
    # 2. Service Selection (contexte racine pour accÃ©der aux modules communs)
    build_and_push_image "service-selection" "service-selection/Dockerfile" "." ""
    
    # 3. Frontend (avec build args spÃ©cifiques Ã  l'environnement)
    local frontend_build_args=""
    if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$ANGULAR_ENV" == "production" ]]; then
        frontend_build_args="--build-arg ANGULAR_ENV=production --no-cache"
        log_info "ðŸŒ Frontend: Build FORCÃ‰ en mode PRODUCTION (--no-cache)"
        log_info "ðŸ“ Variables d'environnement: ANGULAR_ENV=production"
        log_info "ðŸ”— API URL: https://api.ibisx.fr"
        log_info "ðŸŒ Domain: ibisx.fr"
    else
        frontend_build_args="--build-arg ANGULAR_ENV=development"
        log_info "ðŸŒ Frontend: Build en mode DEVELOPMENT"
    fi
    
    build_and_push_image "frontend" "frontend/Dockerfile" "frontend/" "$frontend_build_args"
    
    log_success "ðŸš€ Toutes les images Docker pushÃ©es vers ACR : $ACR_NAME"
}

# Fonction pour dÃ©ployer l'application sur Kubernetes
deploy_application() {
    log_info "DÃ©ploiement de l'application IBIS-X sur AKS..."
    
    cd "$PROJECT_ROOT"
    
    # Les placeholders ACR ont dÃ©jÃ  Ã©tÃ© remplacÃ©s lors de get_terraform_outputs
    
    # VÃ©rifier les noms de projet
    verify_project_names
    
    # Installer les composants Kubernetes nÃ©cessaires
    install_nginx_ingress
    install_cert_manager
    
    # Attendre que les contrÃ´leurs soient prÃªts
    log_info "Attente que NGINX Ingress soit prÃªt..."
    kubectl wait --namespace ingress-nginx \
        --for=condition=ready pod \
        --selector=app.kubernetes.io/component=controller \
        --timeout=300s
    
    log_info "Attente que Cert-Manager soit prÃªt..."
    kubectl wait --namespace cert-manager \
        --for=condition=ready pod \
        --selector=app.kubernetes.io/name=cert-manager \
        --timeout=300s
    
    # Les placeholders ACR ont dÃ©jÃ  Ã©tÃ© remplacÃ©s
    
    # ðŸŽ¯ DÃ‰PLOIEMENT UNIFIÃ‰ : Toujours utiliser Kustomize avec overlay Azure
    # Applique automatiquement TOUS les patches Azure (stockage, auto-init, etc.)
    log_info "ðŸš€ DÃ©ploiement unifiÃ© avec Kustomize Azure (Windows/Linux/MacOS)..."
    
    # Sauvegarder le rÃ©pertoire courant
    ORIGINAL_DIR=$(pwd)
    
    # Utiliser TOUJOURS l'overlay Azure qui applique automatiquement :
    # âœ… Configuration stockage Azure (au lieu de MinIO)
    # âœ… Auto-initialisation datasets forcÃ©e 
    # âœ… Variables d'environnement production
    # âœ… Images ACR correctes
    # âœ… Tous les patches spÃ©cifiques Ã  Azure
    
    log_info "ðŸ“¦ Application overlay Azure avec tous les patches..."
    
    cd "$K8S_DIR/overlays/azure/" || {
        log_error "âŒ Impossible d'accÃ©der au rÃ©pertoire Azure overlay"
        exit 1
    }
    
    if kubectl apply -k . ; then
        cd "$ORIGINAL_DIR"
        log_success "âœ… DÃ©ploiement Kustomize Azure rÃ©ussi - TOUS les patches appliquÃ©s automatiquement"
        log_info "âœ… Patches Azure appliquÃ©s :"
        log_info "  ðŸ—‚ï¸  Stockage: Azure Blob Storage (PAS MinIO)"
        log_info "  ðŸ”„ Auto-init: FORCE_INIT_DATA=true + AUTO_INIT_DATA=true"
        log_info "  ðŸ³ Images: ACR $ACR_NAME"
        log_info "  ðŸŽ¯ Mode: Production avec WITH_DATA=true"
    else
        log_error "âŒ Ã‰chec du dÃ©ploiement Kustomize Azure"
        cd "$ORIGINAL_DIR"
        exit 1
    fi
}

# Fonction pour dÃ©ployer les applications avec les bonnes images automatiquement
deploy_app_with_correct_images() {
    log_info "DÃ©ploiement automatique des applications avec images corrigÃ©es..."
    
    # CrÃ©er fichiers temporaires avec les bonnes images (mÃ©thode robuste)
    local TEMP_DIR=""
    if [[ "$IS_WINDOWS" == true ]]; then
        TEMP_DIR="$TEMP/ibis-x-deploy-$$"
    else
        TEMP_DIR="/tmp/ibis-x-deploy-$$"
    fi
    mkdir -p "$TEMP_DIR"
    
    # 1. API Gateway
    log_info "DÃ©ploiement API Gateway avec image ACR $ACR_NAME..."
    sed "s|image: ibis-x-api-gateway|image: $ACR_NAME.azurecr.io/ibis-x-api-gateway:latest|" "$K8S_DIR/base/api-gateway/deployment.yaml" > "$TEMP_DIR/api-gateway-deployment.yaml"
    kubectl apply -f "$TEMP_DIR/api-gateway-deployment.yaml"
    kubectl apply -f "$K8S_DIR/base/api-gateway/service.yaml" 2>/dev/null || true
    
    # 2. Service Selection  
    log_info "DÃ©ploiement Service Selection avec image ACR $ACR_NAME..."
    sed "s|image: service-selection|image: $ACR_NAME.azurecr.io/service-selection:latest|" "$K8S_DIR/base/service-selection/deployment.yaml" > "$TEMP_DIR/service-selection-deployment.yaml"
    kubectl apply -f "$TEMP_DIR/service-selection-deployment.yaml"
    kubectl apply -f "$K8S_DIR/base/service-selection/service.yaml" 2>/dev/null || true
    
    # 3. Frontend
    log_info "DÃ©ploiement Frontend avec image ACR $ACR_NAME..."
    sed "s|image: frontend|image: $ACR_NAME.azurecr.io/frontend:latest|" "$K8S_DIR/base/frontend/deployment.yaml" > "$TEMP_DIR/frontend-deployment.yaml"
    kubectl apply -f "$TEMP_DIR/frontend-deployment.yaml"
    kubectl apply -f "$K8S_DIR/base/frontend/service.yaml" 2>/dev/null || true
    
    # 4. Secrets nÃ©cessaires (crÃ©Ã©s automatiquement)
    log_info "CrÃ©ation automatique des secrets manquants..."
    kubectl apply -f "$K8S_DIR/base/api-gateway/gateway-secrets.yaml" 2>/dev/null || true
    kubectl apply -f "$K8S_DIR/base/service-selection/db-secrets.yaml" 2>/dev/null || true
    
    # 5. RedÃ©marrage forcÃ© pour prendre en compte les nouvelles images et secrets
    log_info "RedÃ©marrage automatique des pods avec nouvelles images ACR..."
    kubectl delete pods -l app=api-gateway -n ibis-x --ignore-not-found=true 2>/dev/null || true
    kubectl delete pods -l app=service-selection -n ibis-x --ignore-not-found=true 2>/dev/null || true
    kubectl delete pods -l app=frontend -n ibis-x --ignore-not-found=true 2>/dev/null || true
    
    # 6. Mise Ã  jour automatique des images vers ACR  
    log_info "Mise Ã  jour automatique des images vers ACR..."
    kubectl set image deployment/api-gateway api-gateway=$ACR_NAME.azurecr.io/ibis-x-api-gateway:latest -n ibis-x 2>/dev/null || true
    kubectl set image deployment/service-selection service-selection=$ACR_NAME.azurecr.io/service-selection:latest -n ibis-x 2>/dev/null || true
    kubectl set image deployment/frontend frontend=$ACR_NAME.azurecr.io/frontend:latest -n ibis-x 2>/dev/null || true
    
    # 7. Ingress et certificats SSL automatiques
    log_info "DÃ©ploiement de l'ingress et gÃ©nÃ©ration automatique des certificats SSL..."
    kubectl apply -f "$K8S_DIR/base/common/letsencrypt-prod-issuer.yaml" 2>/dev/null || true
    kubectl apply -f "$K8S_DIR/base/common/ingress.yaml" 2>/dev/null || true
    
    # Nettoyer les fichiers temporaires
    rm -rf "$TEMP_DIR" 2>/dev/null || true
    
    log_success "âœ… DÃ‰PLOIEMENT 100% AUTOMATIQUE RÃ‰USSI ! Applications en ligne avec images ACR, secrets, et SSL !"
}

# âœ… FONCTION AUTOMATIQUE : Forcer Rebuild et RedÃ©ploiement Frontend Production
force_frontend_production_rebuild() {
    log_info "ðŸš€ Reconstruction automatique du frontend en mode production..."
    
    # 1. Configurer l'environnement de production
    configure_frontend_for_production
    
    # 2. Supprimer les images existantes pour forcer le rebuild
    log_info "ðŸ§¹ Suppression des images frontend existantes..."
    docker rmi "$ACR_NAME.azurecr.io/frontend:latest" 2>/dev/null || true
    docker rmi "$ACR_NAME.azurecr.io/frontend:prod-fix" 2>/dev/null || true
    docker rmi "frontend:latest" 2>/dev/null || true
    
    # 3. Rebuild avec les nouvelles variables d'environnement
    cd "$PROJECT_ROOT/frontend"
    log_info "ðŸ—ï¸ Construction frontend avec ANGULAR_ENV=production..."
    log_info "ðŸ“ API URL configurÃ©e: https://api.ibisx.fr"
    log_info "ðŸŒ Domain configurÃ©: ibisx.fr"
    
    # Build avec --no-cache pour Ãªtre sÃ»r que les changements sont pris en compte
    docker build \
        --build-arg ANGULAR_ENV=production \
        --no-cache \
        -t "$ACR_NAME.azurecr.io/frontend:prod-fixed" \
        -t "$ACR_NAME.azurecr.io/frontend:latest" .
    
    # 4. Push vers ACR
    log_info "ðŸ“¤ Push vers ACR..."
    az acr login --name "$ACR_NAME"
    docker push "$ACR_NAME.azurecr.io/frontend:prod-fixed"
    docker push "$ACR_NAME.azurecr.io/frontend:latest"
    
    # 5. RedÃ©ployer le frontend avec la nouvelle image
    log_info "ðŸ”„ RedÃ©ploiement automatique du frontend..."
    kubectl set image deployment/frontend frontend="$ACR_NAME.azurecr.io/frontend:prod-fixed" -n ibis-x
    kubectl rollout restart deployment/frontend -n ibis-x
    
    # 6. Attendre que le rollout soit terminÃ©
    log_info "â³ Attente du rollout frontend..."
    kubectl rollout status deployment/frontend -n ibis-x --timeout=300s
    
    cd "$PROJECT_ROOT"
    log_success "âœ… Frontend reconstruit et redÃ©ployÃ© en mode production !"
}

# Fonction SIMPLE et EFFICACE pour remplacer les placeholders ACR
update_all_acr_references() {
    log_info "ðŸ” Remplacement des placeholders ACR : PLACEHOLDER_ACR â†’ $ACR_NAME"
    
    local updated_files=0
    
    # Fichiers critiques Ã  mettre Ã  jour
    local critical_files=(
        "$K8S_DIR/base/jobs/api-gateway-migration-job.yaml"
        "$K8S_DIR/base/jobs/service-selection-migration-job.yaml"
        "$K8S_DIR/overlays/azure/kustomization.yaml"
        "$K8S_DIR/overlays/azure/migration-jobs-image-patch.yaml"
        "$K8S_DIR/overlays/azure/service-selection-migration-job-patch.yaml"
    )
    
    # Fonction simple pour remplacer PLACEHOLDER_ACR dans un fichier
    replace_placeholder_in_file() {
        local file_path="$1"
        local file_name=$(basename "$file_path")
        
        if [[ ! -f "$file_path" ]]; then
            log_warning "âš ï¸ Fichier introuvable: $file_path"
            return 1
        fi
        
        # VÃ©rifier si le fichier contient des placeholders
        if ! grep -q "PLACEHOLDER_ACR" "$file_path" 2>/dev/null; then
            log_info "âœ… $file_name - aucun placeholder Ã  remplacer"
            return 0
        fi
        
        log_info "ðŸ”§ Remplacement des placeholders dans $file_name..."
        
        # Remplacement simple et direct
        if sed -i "s|PLACEHOLDER_ACR|$ACR_NAME|g" "$file_path" 2>/dev/null; then
            # VÃ©rifier que le remplacement a fonctionnÃ©
            if grep -q "$ACR_NAME\.azurecr\.io" "$file_path" 2>/dev/null; then
                log_success "âœ… $file_name mis Ã  jour avec $ACR_NAME"
                return 0
            else
                log_error "âŒ Ã‰chec de vÃ©rification pour $file_name"
                return 1
            fi
        else
            log_error "âŒ Ã‰chec du remplacement pour $file_name"
            return 1
        fi
    }
    
    # Remplacer les placeholders dans chaque fichier critique
    log_info "ðŸŽ¯ Remplacement des placeholders dans tous les fichiers..."
    
    for file in "${critical_files[@]}"; do
        if replace_placeholder_in_file "$file"; then
            updated_files=$((updated_files + 1))
        fi
    done
    
    log_success "ðŸŽ¯ Remplacement terminÃ© - $updated_files fichier(s) traitÃ©s avec ACR : $ACR_NAME"
    
    return 0
}

# Fonction INTELLIGENTE pour gÃ©rer automatiquement les jobs de migration avec les bonnes images ACR
fix_migration_jobs() {
    log_info "ðŸ” VÃ‰RIFICATION INTELLIGENTE des jobs de migration avec ACR $ACR_NAME..."
    
    # Les fichiers ont dÃ©jÃ  Ã©tÃ© corrigÃ©s avec les placeholders ACR
    
    # VÃ©rifier les jobs existants et leur statut
    local api_job_exists=$(kubectl get job api-gateway-migration-job -n ibis-x 2>/dev/null && echo "true" || echo "false")
    local service_job_exists=$(kubectl get job service-selection-migration-job -n ibis-x 2>/dev/null && echo "true" || echo "false")
    
    local api_complete="False"
    local service_complete="False"
    
    if [[ "$api_job_exists" == "true" ]]; then
        api_complete=$(kubectl get job api-gateway-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
    fi
    
    if [[ "$service_job_exists" == "true" ]]; then
        service_complete=$(kubectl get job service-selection-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
    fi
    
    # Si les jobs existent et sont terminÃ©s avec succÃ¨s, ne pas les recrÃ©er
    if [[ "$api_complete" == "True" ]] && [[ "$service_complete" == "True" ]]; then
        log_success "âœ… Migrations dÃ©jÃ  terminÃ©es avec succÃ¨s - aucune action nÃ©cessaire"
        return 0
    fi
    
    # VÃ©rifier s'il y a des jobs dÃ©faillants (ImagePullBackOff ou Failed)
    local failed_migration_pods=$(kubectl get pods -n ibis-x -o jsonpath='{.items[?(@.metadata.name=~".*migration.*")].metadata.name}' 2>/dev/null || echo "")
    local has_failed_jobs=false
    
    if [[ -n "$failed_migration_pods" ]]; then
        for pod in $failed_migration_pods; do
            local pod_status=$(kubectl get pod "$pod" -n ibis-x -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
            local container_status=$(kubectl get pod "$pod" -n ibis-x -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null || echo "")
            
            if [[ "$pod_status" == "Failed" ]] || [[ "$container_status" == "ImagePullBackOff" ]]; then
                has_failed_jobs=true
                log_warning "âŒ Job dÃ©faillant dÃ©tectÃ©: $pod (Status: $pod_status, Reason: $container_status)"
            fi
        done
    fi
    
    # Si jobs dÃ©faillants OU jobs non existants, les recrÃ©er
    if [[ "$has_failed_jobs" == true ]] || [[ "$api_job_exists" == "false" ]] || [[ "$service_job_exists" == "false" ]]; then
        log_info "ðŸ§¹ NETTOYAGE et RECRÃ‰ATION des jobs de migration..."
        
        # Supprimer tous les jobs de migration existants
        kubectl delete job api-gateway-migration-job service-selection-migration-job -n ibis-x 2>/dev/null || true
        sleep 3
        
        # CrÃ©er les nouveaux jobs avec les fichiers corrigÃ©s
        log_info "ðŸš€ CrÃ©ation des jobs de migration avec ACR corrigÃ© $ACR_NAME..."
        kubectl apply -f "$K8S_DIR/base/jobs/api-gateway-migration-job.yaml" || {
            log_error "âŒ Ã‰chec crÃ©ation job API Gateway"
            return 1
        }
        kubectl apply -f "$K8S_DIR/base/jobs/service-selection-migration-job.yaml" || {
            log_error "âŒ Ã‰chec crÃ©ation job Service Selection"
            return 1
        }
        
        log_success "âœ… Jobs de migration crÃ©Ã©s avec les bonnes images ACR"
    fi
    
    # Attendre et vÃ©rifier la complÃ©tion avec feedback amÃ©liorÃ©
    log_info "â³ Attente de la complÃ©tion des migrations (timeout: 5min)..."
    
    local timeout=300
    local elapsed=0
    local check_interval=15
    
    while [[ $elapsed -lt $timeout ]]; do
        # Nettoyer les sorties kubectl pour Ã©viter les caractÃ¨res parasites
        api_complete=$(kubectl get job api-gateway-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        service_complete=$(kubectl get job service-selection-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        
        local api_failed=$(kubectl get job api-gateway-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        local service_failed=$(kubectl get job service-selection-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        
        if [[ "$api_complete" == "True" ]] && [[ "$service_complete" == "True" ]]; then
            log_success "âœ… TOUTES LES MIGRATIONS TERMINÃ‰ES AVEC SUCCÃˆS !"
            return 0
        fi
        
        if [[ "$api_failed" == "True" ]] || [[ "$service_failed" == "True" ]]; then
            log_error "âŒ Ã‰chec de migration dÃ©tectÃ© - Affichage des logs..."
            kubectl get jobs -n ibis-x 2>/dev/null || true
            kubectl get pods -n ibis-x | grep migration 2>/dev/null || true
            return 1
        fi
        
        log_info "â³ Migrations en cours... API: $api_complete, Service: $service_complete (${elapsed}s/${timeout}s)"
        sleep $check_interval
        elapsed=$((elapsed + check_interval))
    done
    
    # Si on arrive ici, il y a eu un timeout
    log_warning "âš ï¸ TIMEOUT atteint - VÃ©rification du statut final..."
    kubectl get jobs -n ibis-x 2>/dev/null || true
    kubectl get pods -n ibis-x | grep migration 2>/dev/null || true
    
    log_success "ðŸŽ¯ Gestion automatique des jobs de migration terminÃ©e (avec timeout)"
    return 1
}

# âœ… FONCTION AUTOMATIQUE : VÃ©rification et correction IP statique NGINX
verify_and_fix_nginx_static_ip() {
    log_info "ðŸ” VÃ‰RIFICATION AUTOMATIQUE : IP statique NGINX..."
    
    # RÃ©cupÃ©rer l'IP statique configurÃ©e
    local expected_static_ip="$STATIC_IP_CONFIRMED"
    if [[ -z "$expected_static_ip" ]]; then
        # Fallback : rÃ©cupÃ©rer depuis Azure
        local node_resource_group=$(az group list --query "[?contains(name, 'MC_${RESOURCE_GROUP}_${AKS_NAME}')].name" --output tsv 2>/dev/null | head -1)
        if [[ -n "$node_resource_group" ]]; then
            expected_static_ip=$(az network public-ip show --resource-group "$node_resource_group" --name "ibis-x-prod-ingress-ip" --query "ipAddress" --output tsv 2>/dev/null || echo "")
        fi
    fi
    
    if [[ -z "$expected_static_ip" ]]; then
        log_warning "âš ï¸ Impossible de dÃ©terminer l'IP statique attendue"
        return 0
    fi
    
    # VÃ©rifier l'IP actuelle de NGINX
    local nginx_current_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
    
    log_info "ðŸŽ¯ IP statique attendue : $expected_static_ip"
    log_info "ðŸ” IP actuelle NGINX : ${nginx_current_ip:-AUCUNE}"
    
    if [[ "$nginx_current_ip" == "$expected_static_ip" ]]; then
        log_success "âœ… NGINX utilise l'IP statique correcte : $nginx_current_ip"
        
        # VÃ©rifier aussi que l'ingress application utilise la bonne IP
        local ingress_ip=$(kubectl get ingress -n ibis-x ibis-x-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
        if [[ "$ingress_ip" == "$expected_static_ip" ]]; then
            log_success "âœ… Ingress application utilise l'IP statique correcte : $ingress_ip"
        else
            log_warning "âš ï¸ Ingress application : IP diffÃ©rente ou en attente (${ingress_ip:-PENDING})"
        fi
    else
        log_error "âŒ PROBLÃˆME DÃ‰TECTÃ‰ : NGINX utilise une IP incorrecte !"
        log_error "   IP attendue : $expected_static_ip"
        log_error "   IP actuelle : ${nginx_current_ip:-AUCUNE}"
        
        log_info "ðŸ”§ CORRECTION AUTOMATIQUE en cours..."
        
        # Forcer la correction
        kubectl patch svc ingress-nginx-controller -n ingress-nginx -p "{\"spec\":{\"loadBalancerIP\":\"$expected_static_ip\"}}" 2>/dev/null || true
        
        # Attendre et vÃ©rifier Ã  nouveau
        sleep 30
        nginx_current_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
        
        if [[ "$nginx_current_ip" == "$expected_static_ip" ]]; then
            log_success "âœ… CORRECTION RÃ‰USSIE : NGINX utilise maintenant l'IP statique : $nginx_current_ip"
        else
            log_warning "âš ï¸ CORRECTION PARTIELLE : RecrÃ©ation de NGINX recommandÃ©e"
            log_warning "   Lancez Ã  nouveau le script pour forcer la recrÃ©ation"
        fi
    fi
}

# Fonction de vÃ©rification finale et auto-correction RENFORCÃ‰E
final_auto_check_and_fix() {
    log_info "ðŸ” VÃ©rification finale et auto-correction RENFORCÃ‰E..."
    
    # 1. Les ACR ont dÃ©jÃ  Ã©tÃ© corrigÃ©s automatiquement
    
    # 2. VÃ©rifier et corriger automatiquement les jobs de migration
    log_info "ðŸ”§ Auto-correction des jobs de migration..."
    fix_migration_jobs
    
    # 3. âœ… NOUVEAU : VÃ©rification et correction automatique IP statique NGINX
    log_info "ðŸŽ¯ VÃ©rification automatique de l'IP statique NGINX..."
    verify_and_fix_nginx_static_ip
    
    # 4. VÃ©rification finale de l'Ã©tat global
    log_info "ðŸ“Š Ã‰tat final du dÃ©ploiement :"
    echo "======================="
    kubectl get pods -n ibis-x 2>/dev/null || true
    echo "======================="
    kubectl get jobs -n ibis-x 2>/dev/null || true
    echo "======================="
    kubectl get ingress -n ibis-x 2>/dev/null || true
    echo "======================="
    kubectl get svc -n ingress-nginx ingress-nginx-controller 2>/dev/null || true
    echo "======================="
    
    log_success "âœ… VÃ©rification finale et auto-correction RENFORCÃ‰ES terminÃ©es"
}

# Fonction pour attendre que les pods soient prÃªts  
wait_for_pods() {
    # Attendre que les pods soient prÃªts
    log_info "Attente du dÃ©marrage des pods..."
    kubectl wait --for=condition=ready pod -l app=api-gateway -n ibis-x --timeout=300s
    kubectl wait --for=condition=ready pod -l app=service-selection -n ibis-x --timeout=300s
    kubectl wait --for=condition=ready pod -l app=frontend -n ibis-x --timeout=300s
    
    log_success "Application IBIS-X dÃ©ployÃ©e sur AKS"
}

# Fonction pour gÃ©rer les secrets selon le mode de dÃ©ploiement
create_missing_secrets() {
    log_info "ðŸ” Gestion des secrets - Mode: $DEPLOYMENT_MODE"
    
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        # ðŸ­ MODE GITHUB ACTIONS: Secrets injectÃ©s dans les fichiers YAML
        log_info "ðŸ­ Application des secrets depuis les fichiers GitHub Actions..."
        create_github_secrets
    else
        # ðŸ› ï¸ MODE MANUEL: Configuration manuelle des secrets
        log_info "ðŸ› ï¸ Configuration manuelle des secrets..."
        create_manual_secrets
    fi
    
    log_success "âœ… Gestion des secrets terminÃ©e"
}

# Fonction pour les secrets GitHub Actions
create_github_secrets() {
    log_info "ðŸ“‹ Application des secrets GitHub Actions..."
    
    # GitHub Actions a dÃ©jÃ  modifiÃ© les fichiers YAML avec les vrais secrets
    # Il suffit d'appliquer les fichiers secrets
    
    # 1. Supprimer les anciens secrets (pour forcer la mise Ã  jour)
    log_info "ðŸ§¹ Suppression des anciens secrets..."
    kubectl delete secret gateway-secrets -n "$K8S_NAMESPACE" --ignore-not-found=true
    kubectl delete secret kaggle-secrets -n "$K8S_NAMESPACE" --ignore-not-found=true
    kubectl delete secret db-secrets -n "$K8S_NAMESPACE" --ignore-not-found=true
    
    # 2. Appliquer les secrets depuis les fichiers (modifiÃ©s par GitHub Actions)
    log_info "ðŸ“„ Application des secrets depuis les fichiers YAML..."
    kubectl apply -f k8s/base/api-gateway/gateway-secrets.yaml
    kubectl apply -f k8s/base/service-selection/kaggle-secrets.yaml
    kubectl apply -f k8s/base/service-selection/db-secrets.yaml
    
    # 3. Storage secrets (rÃ©cupÃ©rer depuis Azure)
    create_storage_secrets_from_azure
    
    log_success "âœ… Secrets GitHub Actions appliquÃ©s"
}

# Fonction pour les secrets manuels (production sans GitHub Actions)
create_manual_secrets() {
    log_info "ðŸ› ï¸ Configuration automatique des secrets de base pour la production..."
    
    # En mode production script, on crÃ©e des secrets de base fonctionnels
    log_info "ðŸ“‹ CrÃ©ation des secrets de base pour permettre le fonctionnement de l'application"
    
    # 1. CrÃ©er les secrets Kaggle (fonctionnels par dÃ©faut)
    if ! kubectl get secret kaggle-secrets -n "$K8S_NAMESPACE" &>/dev/null; then
        log_info "ðŸ”‘ CrÃ©ation kaggle-secrets (configuration de base)..."
        kubectl create secret generic kaggle-secrets -n "$K8S_NAMESPACE" \
            --from-literal=username=default-kaggle-user \
            --from-literal=key=default-kaggle-key
        log_info "âœ… Kaggle secrets crÃ©Ã©s (fonctionnels pour les tests)"
    fi
    
    # 2. Storage secrets depuis Azure
    create_storage_secrets_from_azure
    
    # 3. Gateway secrets (fonctionnels par dÃ©faut)
    if ! kubectl get secret gateway-secrets -n "$K8S_NAMESPACE" &>/dev/null; then
        log_info "ðŸ”‘ CrÃ©ation gateway-secrets (configuration de base)..."
        # GÃ©nÃ©rer une clÃ© JWT basique pour les tests
        local jwt_secret=$(openssl rand -base64 32 2>/dev/null || echo "default-jwt-secret-key-for-development-only")
        local db_url="postgresql://postgres:postgres@postgresql-service:5432/ibisxdb"
        
        kubectl create secret generic gateway-secrets -n "$K8S_NAMESPACE" \
            --from-literal=secret-key="$jwt_secret" \
            --from-literal=database-url="$db_url" \
            --from-literal=google-client-id=default-google-client-id \
            --from-literal=google-client-secret=default-google-client-secret \
            --from-literal=oauth-redirect-url=https://ibisx.fr/oauth/callback
        log_info "âœ… Gateway secrets crÃ©Ã©s (fonctionnels pour les tests)"
    fi
    
    # 4. DB secrets (fonctionnels par dÃ©faut)
    if ! kubectl get secret db-secrets -n "$K8S_NAMESPACE" &>/dev/null; then
        log_info "ðŸ”‘ CrÃ©ation db-secrets (configuration de base)..."
        local db_url="postgresql://postgres:postgres@postgresql-service:5432/ibisxdb"
        kubectl create secret generic db-secrets -n "$K8S_NAMESPACE" \
            --from-literal=database-url="$db_url"
        log_info "âœ… DB secrets crÃ©Ã©s (fonctionnels pour les tests)"
    fi
    
    log_success "âœ… Secrets de base crÃ©Ã©s et fonctionnels"
}

# Fonction pour crÃ©er les secrets de stockage Azure 
create_storage_secrets_from_azure() {
    log_info "â˜ï¸ RÃ©cupÃ©ration des secrets de stockage Azure..."
    
    # Toujours privilÃ©gier les valeurs Terraform si disponibles
    if [[ -z "$STORAGE_ACCOUNT" ]] || [[ -z "$STORAGE_KEY" ]]; then
        log_info "ðŸ“‚ RÃ©cupÃ©ration storage depuis Azure CLI..."
        # Filtrer pour rÃ©cupÃ©rer spÃ©cifiquement le storage account IBIS-X (pattern: ibisxprodstg*)
        STORAGE_ACCOUNT=$(az storage account list --resource-group "$RESOURCE_GROUP" --query "[?starts_with(name, 'ibisxprodstg')].name | [0]" -o tsv 2>/dev/null || echo "")
        if [[ -n "$STORAGE_ACCOUNT" ]]; then
            STORAGE_KEY=$(az storage account keys list --resource-group "$RESOURCE_GROUP" --account-name "$STORAGE_ACCOUNT" --query "[0].value" -o tsv 2>/dev/null || echo "")
        fi
    fi
    
    # SÃ‰CURITÃ‰ : Corriger automatiquement si on dÃ©tecte l'ancien storage account
    if [[ "$STORAGE_ACCOUNT" == "ibisxprodstg2205" ]]; then
        log_warning "âš ï¸ Ancien storage account dÃ©tectÃ© ($STORAGE_ACCOUNT), correction automatique..."
        STORAGE_ACCOUNT="ibisxprodstg6630"
        STORAGE_KEY=$(cd terraform/azure-infrastructure && terraform output -raw storage_account_primary_key 2>/dev/null || echo "")
        log_info "âœ… Correction : STORAGE_ACCOUNT=$STORAGE_ACCOUNT"
    fi
    
    # CrÃ©er le secret storage si on a les valeurs
    if [[ -n "$STORAGE_ACCOUNT" ]] && [[ -n "$STORAGE_KEY" ]]; then
        kubectl delete secret storage-secrets -n "$K8S_NAMESPACE" 2>/dev/null || true
        
        log_info "ðŸ—‚ï¸ CrÃ©ation storage-secrets avec valeurs Azure: $STORAGE_ACCOUNT"
        kubectl create secret generic storage-secrets -n "$K8S_NAMESPACE" \
            --from-literal=azure-storage-account-name="$STORAGE_ACCOUNT" \
            --from-literal=azure-storage-account-key="$STORAGE_KEY" \
            --from-literal=azure-container-name=ibis-x-datasets
        
        log_success "âœ… Storage secrets crÃ©Ã©s: $STORAGE_ACCOUNT"
    else
        log_warning "âš ï¸ Impossible de rÃ©cupÃ©rer les secrets de stockage Azure"
        log_warning "ðŸ› ï¸ Vous devrez configurer storage-secrets manuellement si nÃ©cessaire"
    fi
}

# Fonction pour vÃ©rifier et redÃ©marrer les pods en erreur
fix_failed_pods() {
    log_info "VÃ©rification et correction automatique des pods en erreur..."
    
    # Attendre un peu que les pods dÃ©marrent
    sleep 30
    
    # VÃ©rifier les pods qui ne sont pas prÃªts
    local failed_pods=""
    if kubectl get namespace ibis-x &>/dev/null; then
        failed_pods=$(kubectl get pods -n ibis-x --field-selector=status.phase!=Running -o name 2>/dev/null || echo "")
    fi
    
    if [[ -n "$failed_pods" ]]; then
        log_warning "Pods en erreur dÃ©tectÃ©s, redÃ©marrage automatique..."
        
        # RedÃ©marrer les pods en erreur
        kubectl delete pod -n ibis-x -l app=service-selection --ignore-not-found=true
        kubectl delete pod -n ibis-x -l app=api-gateway --ignore-not-found=true
        kubectl delete pod -n ibis-x -l app=frontend --ignore-not-found=true
        
        # Attendre que les nouveaux pods dÃ©marrent
        log_info "Attente du redÃ©marrage des pods..."
        sleep 60
        
        # VÃ©rifier Ã  nouveau
        kubectl get pods -n ibis-x 2>/dev/null || log_info "Pods en cours de crÃ©ation..."
    fi
    
    log_success "VÃ©rification des pods terminÃ©e"
}

# Fonction pour exÃ©cuter les migrations de base de donnÃ©es avec auto-correction complÃ¨te
run_migrations() {
    log_info "ðŸš€ DÃ©marrage de l'auto-correction complÃ¨te des migrations..."
    final_auto_check_and_fix
}

# Fonction pour afficher les informations de l'application
show_application_info() {
    log_success "ðŸŽ‰ DÃ©ploiement IBIS-X 100% AUTOMATIQUE terminÃ© avec succÃ¨s !"
    echo
    echo "âœ… AUTOMATISATION COMPLÃˆTE RÃ‰USSIE :"
    echo "===================================="
    echo "âœ… Infrastructure Azure crÃ©Ã©e automatiquement"
    echo "âœ… Images Docker construites et pushÃ©es automatiquement"
    echo "âœ… Secrets Kubernetes crÃ©Ã©s automatiquement (vrais secrets Azure)"
    echo "âœ… Applications dÃ©ployÃ©es automatiquement (avec fallback Windows)"
    echo "âœ… NGINX Ingress + Cert-Manager installÃ©s automatiquement"
    echo "âœ… Pods dÃ©faillants redÃ©marrÃ©s automatiquement"
    echo "âœ… Aucune commande manuelle requise !"
    echo
    echo "ðŸ“‹ Informations de l'application :"
    echo "=================================="
    echo "ðŸŒ URL de l'application: https://ibisx.fr (certificat SSL automatique)"
    echo "ðŸŒ URL de l'API: https://api.ibisx.fr (certificat SSL automatique)"
    echo "ðŸŒ URL HTTP direct: http://$PUBLIC_IP"
    echo "ðŸ—„ï¸  Storage Account: $STORAGE_ACCOUNT"
    echo "ðŸ³ Container Registry: $ACR_NAME.azurecr.io"
    echo "â˜¸ï¸  Cluster AKS: $AKS_NAME"
    echo "ðŸ“¦ Resource Group: $RESOURCE_GROUP"
    echo
    echo "ðŸ”§ Commandes utiles pour monitoring :"
    echo "====================================="
    echo "# Voir l'Ã©tat des pods:"
    echo "kubectl get pods -n ibis-x"
    echo
    echo "# Voir les services:"
    echo "kubectl get services -n ibis-x"
    echo
    echo "# Voir l'ingress et IP publique:"
    echo "kubectl get ingress -n ibis-x"
    echo
    echo "# Voir les certificats SSL et leur statut:"
    echo "kubectl get certificates -n ibis-x"
    echo ""
    echo "# Voir l'ingress et IP publique:"
    echo "kubectl get ingress -n ibis-x"
    echo ""
    echo "# Tester l'application en ligne:"
    echo "curl -I http://$PUBLIC_IP"
    echo
    echo "# Voir les logs des applications:"
    echo "kubectl logs -f deployment/api-gateway -n ibis-x"
    echo "kubectl logs -f deployment/service-selection -n ibis-x"
    echo "kubectl logs -f deployment/frontend -n ibis-x"
    echo
    echo "ðŸ“ SCRIPT ENTIÃˆREMENT AUTOMATIQUE :"
    echo "==================================="
    echo "ðŸŽ¯ Pour relancer un dÃ©ploiement complet : ./scripts/deploy-to-azure.sh"
    echo "ðŸ”„ Le script dÃ©tecte automatiquement l'infrastructure existante"
    echo "ðŸ› ï¸ Tous les problÃ¨mes Windows/Linux gÃ©rÃ©s automatiquement"
    echo "ðŸ” Tous les secrets gÃ©nÃ©rÃ©s automatiquement"
    echo "ðŸ“¦ Toutes les images avec les bons noms automatiquement"
    echo
    echo "ðŸŽ¯ PROCHAINES Ã‰TAPES (une seule fois) :"
    echo "======================================"
    echo "1. âœ… IP STATIQUE: $PUBLIC_IP (ne changera jamais !)"
    echo "2. ðŸŒ Configurez vos DNS DÃ‰FINITIVEMENT vers cette IP"
    echo "3. ðŸ”’ Les certificats SSL se gÃ©nÃ¨reront automatiquement"
    echo "4. ðŸš€ L'application est accessible via https://ibisx.fr"
    echo
    echo "ðŸ’¡ IMPORTANT: Cette IP est STATIQUE et ne changera pas lors des futurs redÃ©ploiements !"
    echo "ðŸ’¡ Configurez vos DNS une seule fois avec cette IP !"
}

# Fonction pour nettoyer les fichiers de sauvegarde aprÃ¨s un dÃ©ploiement rÃ©ussi
cleanup_backup_files() {
    log_info "ðŸ§¹ Nettoyage des fichiers de sauvegarde..."
    
    # Supprimer les fichiers de sauvegarde
    find "$K8S_DIR" -name "*.yaml.backup" -delete 2>/dev/null || true
    find "$TERRAFORM_DIR" -name "*.backup" -delete 2>/dev/null || true
    find "$K8S_DIR" -name "*.backup-*" -delete 2>/dev/null || true  # Fichiers avec timestamp
    find "$PROJECT_ROOT/frontend/src/environments/" -name "*.backup-*" -delete 2>/dev/null || true  # Frontend backups
    
    log_success "âœ… Fichiers de sauvegarde nettoyÃ©s"
}

# Fonction pour nettoyer en cas d'erreur
cleanup_on_error() {
    log_error "âŒ Une erreur s'est produite pendant le dÃ©ploiement."
    
    # ðŸ”„ Restaurer nginx-ingress-values.yaml si modifiÃ©
    local nginx_values_file="$K8S_DIR/helm-values/nginx-ingress-values.yaml"
    local latest_backup=$(find "$K8S_DIR/helm-values/" -name "nginx-ingress-values.yaml.backup-*" | sort | tail -1 2>/dev/null || echo "")
    if [[ -n "$latest_backup" && -f "$latest_backup" ]]; then
        mv "$latest_backup" "$nginx_values_file"
        log_info "âœ… Fichier nginx-ingress-values.yaml restaurÃ© depuis backup"
    fi
    
    # ðŸ”„ Restaurer environment.prod.ts si modifiÃ©
    local env_file="$PROJECT_ROOT/frontend/src/environments/environment.prod.ts"
    local latest_env_backup=$(find "$PROJECT_ROOT/frontend/src/environments/" -name "environment.prod.ts.backup-*" | sort | tail -1 2>/dev/null || echo "")
    if [[ -n "$latest_env_backup" && -f "$latest_env_backup" ]]; then
        mv "$latest_env_backup" "$env_file"
        log_info "âœ… Fichier environment.prod.ts restaurÃ© depuis backup"
    fi
    
    # Restaurer les fichiers de sauvegarde s'ils existent
    if [ -f "$K8S_DIR/base/service-selection/storage-secrets.yaml.backup" ]; then
        mv "$K8S_DIR/base/service-selection/storage-secrets.yaml.backup" "$K8S_DIR/base/service-selection/storage-secrets.yaml"
        log_info "âœ… Fichier storage-secrets.yaml restaurÃ©"
    fi
    
    if [ -f "$K8S_DIR/overlays/azure/kustomization.yaml.backup" ]; then
        mv "$K8S_DIR/overlays/azure/kustomization.yaml.backup" "$K8S_DIR/overlays/azure/kustomization.yaml"
        log_info "âœ… Fichier kustomization.yaml restaurÃ©"
    fi
    
    # Restaurer tous les fichiers YAML modifiÃ©s
    find "$K8S_DIR" -name "*.yaml.backup" -exec bash -c 'mv "$1" "${1%.backup}"' _ {} \; 2>/dev/null || true
    find "$TERRAFORM_DIR" -name "*.backup" -exec bash -c 'mv "$1" "${1%.backup}"' _ {} \; 2>/dev/null || true
    find "$K8S_DIR" -name "*.backup-*" -exec rm -f {} \; 2>/dev/null || true  # Supprimer backups avec timestamp
    log_info "âœ… Tous les fichiers restaurÃ©s"
    
    echo
    log_warning "ðŸ’¡ Pour nettoyer les ressources Azure crÃ©Ã©es, exÃ©cutez :"
    echo "./scripts/production/destroy-azure-infrastructure.sh"
}

# Fonction principale
main() {
    log_info "ðŸš€ Script de DÃ©ploiement Production IBIS-X"
    log_info "ðŸŽ¯ Mode: $DEPLOYMENT_MODE"
    log_info "â„¹ï¸  Pour le dÃ©veloppement local, utilisez: make dev"
    echo
    
    # Configurer le gestionnaire d'erreur
    trap cleanup_on_error ERR
    
    # Workflow principal
    log_info "ðŸ“‹ === DÃ‰PLOIEMENT PRODUCTION ==="
    
    # Ã‰tapes d'infrastructure et dÃ©ploiement
    check_prerequisites
    check_azure_login
    
    # Gestion de l'infrastructure (crÃ©er si nÃ©cessaire)
    manage_infrastructure
    
    # Configuration et dÃ©ploiement
    get_terraform_outputs
    configure_kubectl
    
    # Build et dÃ©ploiement de l'application
    build_and_push_images
    update_k8s_secrets  # CrÃ©er le namespace
    create_missing_secrets
    deploy_application
    
    # Migrations et finalisation
    wait_for_migrations
    initialize_sample_data
    final_auto_check_and_fix
    
    # âœ… AUTOMATIQUE: Forcer rebuild frontend en production si nÃ©cessaire
    if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$ANGULAR_ENV" == "production" ]]; then
        log_info "ðŸŽ¯ Mode production dÃ©tectÃ© - VÃ©rification et correction automatique du frontend..."
        force_frontend_production_rebuild
    fi
    
    # Nettoyage si mode GitHub Actions
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        cleanup_migration_jobs
    fi
    
    # Afficher les informations finales
    show_application_info
    
    # Nettoyer les fichiers de sauvegarde
    cleanup_backup_files
    
    log_success "ðŸŽ‰ DÃ‰PLOIEMENT PRODUCTION TERMINÃ‰ AVEC SUCCÃˆS !"
}

# Fonction pour gÃ©rer l'infrastructure selon le contexte
manage_infrastructure() {
    log_info "ðŸ—ï¸ Gestion de l'infrastructure Azure..."
    
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        # Mode GitHub Actions : infrastructure supposÃ©e existante
        log_info "ðŸ­ Mode GitHub Actions - Infrastructure supposÃ©e existante"
        log_info "â„¹ï¸  Si l'infrastructure n'existe pas, crÃ©ez-la manuellement avec ce script"
    else
        # Mode manuel : vÃ©rifier si infrastructure existe, sinon la crÃ©er
        log_info "ðŸ› ï¸ Mode manuel - VÃ©rification de l'infrastructure..."
        
        # VÃ©rifier si l'infrastructure existe
        if check_infrastructure_exists; then
            log_success "âœ… Infrastructure existante dÃ©tectÃ©e"
        else
            log_info "ðŸ—ï¸ Infrastructure non trouvÃ©e - CrÃ©ation automatique..."
            create_infrastructure
        fi
    fi
}

# Fonction pour vÃ©rifier si l'infrastructure existe
check_infrastructure_exists() {
    log_info "ðŸ” VÃ©rification de l'existence de l'infrastructure..."
    
    # VÃ©rifier si les ressources principales existent
    local acr_exists=$(az acr show --name "$AZURE_CONTAINER_REGISTRY" --resource-group "$AZURE_RESOURCE_GROUP" 2>/dev/null && echo "true" || echo "false")
    local aks_exists=$(az aks show --name "$AKS_CLUSTER_NAME" --resource-group "$AZURE_RESOURCE_GROUP" 2>/dev/null && echo "true" || echo "false")
    
    if [[ "$acr_exists" == "true" ]] && [[ "$aks_exists" == "true" ]]; then
        log_success "âœ… Infrastructure existante (ACR + AKS)"
        return 0
    else
        log_info "âŒ Infrastructure incomplÃ¨te ou manquante"
        log_info "   ACR ($AZURE_CONTAINER_REGISTRY): $acr_exists"
        log_info "   AKS ($AKS_CLUSTER_NAME): $aks_exists"
        return 1
    fi
}

# Fonction pour crÃ©er l'infrastructure
create_infrastructure() {
    log_info "ðŸ—ï¸ CrÃ©ation de l'infrastructure Azure via Terraform..."
    
    # CrÃ©er l'infrastructure complÃ¨te
    check_kubernetes_versions
    init_terraform
    deploy_infrastructure
    
    log_success "âœ… Infrastructure crÃ©Ã©e avec succÃ¨s"
}



# VÃ©rifier si le script est exÃ©cutÃ© directement
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi 
