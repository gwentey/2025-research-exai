#!/bin/bash

# üöÄ SCRIPT DE D√âPLOIEMENT PRODUCTION IBIS-X SUR AZURE
# Usage: Infrastructure + D√©ploiement Production
# Pour le d√©veloppement local, utilisez: make dev

set -e

# ==========================================
# üé® FONCTIONS DE LOGGING (D√âFINIES EN PREMIER)
# ==========================================

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Fonctions utilitaires
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# ==========================================
# üõ†Ô∏è FONCTIONS UTILITAIRES ROBUSTES
# ==========================================

# Fonction pour ex√©cuter une commande avec retry
retry_command() {
    local max_attempts="$1"
    local delay="$2"
    shift 2
    local command=("$@")
    local attempt=1
    
    while (( attempt <= max_attempts )); do
        if "${command[@]}"; then
            return 0
        else
            local exit_code=$?
            if (( attempt < max_attempts )); then
                log_warning "Tentative $attempt/$max_attempts √©chou√©e. Retry dans ${delay}s..."
                sleep "$delay"
                ((attempt++))
            else
                log_error "√âchec apr√®s $max_attempts tentatives"
                return $exit_code
            fi
        fi
    done
}

# Fonction pour ex√©cuter kubectl avec gestion d'erreur robuste
kubectl_safe() {
    local context_msg="$1"
    shift
    local kubectl_args=("$@")
    
    log_info "üîß $context_msg"
    if ! kubectl "${kubectl_args[@]}"; then
        log_error "‚ùå √âchec kubectl: $context_msg"
        log_error "Commande: kubectl ${kubectl_args[*]}"
        return 1
    fi
}

# Fonction pour ex√©cuter az CLI avec gestion d'erreur robuste
az_safe() {
    local context_msg="$1"
    shift
    local az_args=("$@")
    
    log_info "‚òÅÔ∏è $context_msg"
    if ! az "${az_args[@]}"; then
        log_error "‚ùå √âchec Azure CLI: $context_msg"
        log_error "Commande: az ${az_args[*]}"
        return 1
    fi
}

# Fonction pour v√©rifier la connectivit√© Kubernetes
check_k8s_connectivity() {
    log_info "üîç V√©rification de la connectivit√© Kubernetes..."
    if ! kubectl cluster-info >/dev/null 2>&1; then
        log_error "‚ùå Impossible de se connecter au cluster Kubernetes"
        log_error "V√©rifiez votre configuration kubectl et la connectivit√© au cluster AKS"
        return 1
    fi
    log_success "‚úÖ Connectivit√© Kubernetes OK"
}

# Fonction robuste pour attendre qu'un job d√©marre
wait_for_job_to_start() {
    local job_name="$1"
    local namespace="${2:-ibis-x}"
    local timeout_seconds="${3:-180}"
    
    log_info "üîç Attente robuste du d√©marrage du job '$job_name' (timeout: ${timeout_seconds}s)"
    
    local start_time=$(date +%s)
    local check_interval=5
    local retry_count=0
    local max_retries=3
    
    while true; do
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        
        # V√©rifier le timeout global
        if (( elapsed > timeout_seconds )); then
            log_error "‚ùå Timeout atteint pour le d√©marrage du job '$job_name' (${timeout_seconds}s)"
            return 1
        fi
        
        # V√©rifier si le job existe
        if kubectl get job "$job_name" -n "$namespace" &>/dev/null; then
            log_info "‚úÖ Job '$job_name' cr√©√© avec succ√®s"
            
            # Attendre qu'un pod soit cr√©√© pour ce job
            local pod_count
            pod_count=$(kubectl get pods -n "$namespace" -l job-name="$job_name" --no-headers 2>/dev/null | wc -l || echo "0")
            
            if (( pod_count > 0 )); then
                log_info "‚úÖ Pod du job '$job_name' cr√©√©, v√©rification du statut..."
                
                # V√©rifier que le pod n'est pas en erreur imm√©diate
                local pod_status
                pod_status=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "Unknown")
                
                if [[ "$pod_status" == "Running" ]] || [[ "$pod_status" == "Pending" ]]; then
                    log_success "‚úÖ Job '$job_name' d√©marr√© avec succ√®s (Pod: $pod_status)"
                    return 0
                elif [[ "$pod_status" == "Failed" ]]; then
                    log_error "‚ùå Le pod du job '$job_name' a √©chou√© imm√©diatement"
                    
                    # Afficher les logs pour diagnostic
                    local pod_name
                    pod_name=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
                    if [[ -n "$pod_name" ]]; then
                        log_info "üìã Logs du pod en √©chec:"
                        kubectl logs "$pod_name" -n "$namespace" --tail=20 2>/dev/null || log_warning "Impossible de r√©cup√©rer les logs"
                    fi
                    return 1
                else
                    log_info "‚è≥ Pod en cours de d√©marrage (status: $pod_status), attente..."
                fi
            else
                log_info "‚è≥ Attente de la cr√©ation du pod pour le job '$job_name'..."
            fi
        else
            log_info "‚è≥ Attente de la cr√©ation du job '$job_name'..."
            
            # Si le job n'existe pas apr√®s plusieurs tentatives, on peut avoir un probl√®me
            retry_count=$((retry_count + 1))
            if (( retry_count >= max_retries )); then
                log_warning "‚ö†Ô∏è Job '$job_name' non trouv√© apr√®s $max_retries tentatives, v√©rification des erreurs..."
                
                # Afficher des informations de diagnostic
                log_info "üìã Diagnostic - Jobs existants dans le namespace:"
                kubectl get jobs -n "$namespace" 2>/dev/null || log_warning "Impossible de lister les jobs"
                
                log_info "üìã Diagnostic - √âv√©nements r√©cents:"
                kubectl get events -n "$namespace" --sort-by='.lastTimestamp' | tail -10 2>/dev/null || log_warning "Impossible de r√©cup√©rer les √©v√©nements"
                
                # Continuer l'attente mais signaler le probl√®me
                retry_count=0
            fi
        fi
        
        sleep $check_interval
    done
}

# Fonction pour monitorer un job avec logs en temps r√©el
monitor_job_progress() {
    local job_name="$1"
    local namespace="${2:-ibis-x}"
    local timeout_minutes="${3:-45}"
    local timeout_seconds=$((timeout_minutes * 60))
    
    log_info "üìä Monitoring du job '$job_name' (timeout: ${timeout_minutes}min)"
    
    # Attendre que le job soit actif
    local start_time=$(date +%s)
    local check_interval=10
    local last_log_check=0
    local last_status_check=0
    local consecutive_errors=0
    local max_consecutive_errors=5
    
    while true; do
        local current_time=$(date +%s)
        local elapsed=$((current_time - start_time))
        
        # V√©rifier le timeout
        if (( elapsed > timeout_seconds )); then
            log_error "‚ùå Timeout atteint pour le job '$job_name' (${timeout_minutes}min)"
            
            # Afficher des informations de diagnostic avant d'√©chouer
            log_info "üìã Diagnostic final du job:"
            kubectl describe job "$job_name" -n "$namespace" 2>/dev/null || log_warning "Impossible de d√©crire le job"
            
            log_info "üìã √âtat final des pods:"
            kubectl get pods -n "$namespace" -l job-name="$job_name" -o wide 2>/dev/null || log_warning "Impossible de lister les pods"
            
            return 1
        fi
        
        # V√©rifier l'√©tat du job avec gestion d'erreurs robuste
        local job_status=""
        local job_info_available=false
        
        # Essayer de r√©cup√©rer les informations du job
        if kubectl get job "$job_name" -n "$namespace" &>/dev/null; then
            job_info_available=true
            job_status=$(kubectl get job "$job_name" -n "$namespace" -o jsonpath='{.status.conditions[0].type}' 2>/dev/null || echo "")
            
            # Si pas de condition, v√©rifier le statut des pods
            if [[ -z "$job_status" ]]; then
                local active_pods
                active_pods=$(kubectl get job "$job_name" -n "$namespace" -o jsonpath='{.status.active}' 2>/dev/null || echo "0")
                if (( active_pods > 0 )); then
                    job_status="Active"
                fi
            fi
            
            consecutive_errors=0
        else
            consecutive_errors=$((consecutive_errors + 1))
            log_warning "‚ö†Ô∏è Impossible d'acc√©der au job '$job_name' (tentative $consecutive_errors/$max_consecutive_errors)"
            
            if (( consecutive_errors >= max_consecutive_errors )); then
                log_error "‚ùå Impossible d'acc√©der au job apr√®s $max_consecutive_errors tentatives"
                return 1
            fi
        fi
        
        # Traitement du statut du job
        if [[ "$job_info_available" == "true" ]]; then
            if [[ "$job_status" == "Complete" ]]; then
                log_success "‚úÖ Job '$job_name' termin√© avec succ√®s"
                return 0
            elif [[ "$job_status" == "Failed" ]]; then
                log_error "‚ùå Job '$job_name' a √©chou√©"
                
                # Afficher des informations d√©taill√©es sur l'√©chec
                log_info "üìã D√©tails de l'√©chec:"
                kubectl describe job "$job_name" -n "$namespace" 2>/dev/null || log_warning "Impossible de d√©crire le job"
                
                local pod_name
                pod_name=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
                if [[ -n "$pod_name" ]]; then
                    log_info "üìã Logs complets du pod en √©chec:"
                    kubectl logs "$pod_name" -n "$namespace" --tail=50 2>/dev/null || log_warning "Impossible de r√©cup√©rer les logs"
                fi
                
                return 1
            elif [[ "$job_status" == "Active" ]] || [[ -n "$job_status" ]]; then
                # Job en cours d'ex√©cution
                if (( elapsed - last_status_check >= 60 )); then
                    log_info "‚è≥ Job '$job_name' en cours (status: $job_status, elapsed: ${elapsed}s)"
                    last_status_check=$elapsed
                fi
            else
                # Statut inconnu, mais job existant
                if (( elapsed - last_status_check >= 60 )); then
                    log_info "‚è≥ Job '$job_name' en pr√©paration (elapsed: ${elapsed}s)"
                    last_status_check=$elapsed
                fi
            fi
        fi
        
        # Afficher les logs p√©riodiquement (toutes les 30 secondes)
        if (( elapsed - last_log_check >= 30 )); then
            local pod_name=""
            pod_name=$(kubectl get pods -n "$namespace" -l job-name="$job_name" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            
            if [[ -n "$pod_name" ]]; then
                local pod_status
                pod_status=$(kubectl get pod "$pod_name" -n "$namespace" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                
                log_info "üìã Logs r√©cents du job '$job_name' (Pod: $pod_status, elapsed: ${elapsed}s):"
                kubectl logs "$pod_name" -n "$namespace" --tail=10 2>/dev/null || log_warning "Impossible de r√©cup√©rer les logs"
                echo "---"
            else
                log_info "‚è≥ Attente du pod pour le job '$job_name' (elapsed: ${elapsed}s)"
            fi
            
            last_log_check=$elapsed
        fi
        
        sleep $check_interval
    done
}

# ==========================================
# üéØ CONFIGURATION PRODUCTION
# ==========================================

# D√©tection si ex√©cut√© depuis GitHub Actions
IS_GITHUB_ACTIONS="${GITHUB_ACTIONS:-false}"
IS_WINDOWS=false

# Auto-d√©tection Windows
if [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]] || [[ -n "${WINDIR}" ]]; then
    IS_WINDOWS=true
fi

# Configuration production
if [[ "$IS_GITHUB_ACTIONS" == "true" ]]; then
    # üè≠ MODE GITHUB ACTIONS
    DEPLOYMENT_MODE="github-actions"
    
    # Variables d'environnement (depuis GitHub Actions)
    export AZURE_CONTAINER_REGISTRY="${AZURE_CONTAINER_REGISTRY:-ibisprodacr}"
    export AZURE_RESOURCE_GROUP="${AZURE_RESOURCE_GROUP:-ibis-x-perso-rg}"
    export AKS_CLUSTER_NAME="${AKS_CLUSTER_NAME:-ibis-x-prod-aks}"
    export K8S_NAMESPACE="${K8S_NAMESPACE:-ibis-x}"
    
    # Tags versionn√©s avec SHA du commit
    if [[ -n "$GITHUB_SHA" ]]; then
        export IMAGE_TAG="${GITHUB_SHA:0:7}"
    else
        export IMAGE_TAG="latest"
    fi
    
    export USE_GITHUB_SECRETS=true
    # ‚úÖ TOUJOURS IMPORTER LES VRAIS DATASETS
    export WITH_DATA="true"
    export ANGULAR_ENV="production"
    
else
    # üõ†Ô∏è MODE SCRIPT MANUEL
    DEPLOYMENT_MODE="manual-production"
    
    # Configuration par d√©faut (peut √™tre surcharg√©e par variables d'environnement)
    export AZURE_CONTAINER_REGISTRY="${AZURE_CONTAINER_REGISTRY:-ibisprodacr}"
    export AZURE_RESOURCE_GROUP="${AZURE_RESOURCE_GROUP:-ibis-x-perso-rg}"
    export AKS_CLUSTER_NAME="${AKS_CLUSTER_NAME:-ibis-x-prod-aks}"
    export K8S_NAMESPACE="${K8S_NAMESPACE:-ibis-x}"
    export IMAGE_TAG="${IMAGE_TAG:-latest}"
    export USE_GITHUB_SECRETS=false
    # ‚úÖ TOUJOURS IMPORTER LES VRAIS DATASETS (modifi√© de false √† true)
    export WITH_DATA="true"
    # ‚úÖ FORCER PRODUCTION: Toujours en mode production pour Azure
    export ANGULAR_ENV="production"
    
    log_info "üéØ Mode Manuel Production - Frontend configur√© automatiquement en PRODUCTION"
    log_info "üéØ VRAIS DATASETS : WITH_DATA=true (toujours activ√© pour production)"
fi

# ==========================================
# üìÅ VARIABLES DE CONFIGURATION
# ==========================================

# Script de d√©ploiement automatis√© pour IBIS-X sur Azure
# Ce script utilise Terraform pour cr√©er l'infrastructure et d√©ploie l'application

# Variables de configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
TERRAFORM_DIR="$PROJECT_ROOT/terraform/azure-infrastructure"
K8S_DIR="$PROJECT_ROOT/k8s"

# D√©tection de l'OS pour adapter les commandes
if [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]] || command -v powershell &> /dev/null; then
    IS_WINDOWS=true
else
    IS_WINDOWS=false
fi

# Variables globales pour partager entre les fonctions
export STORAGE_ACCOUNT=""
export STORAGE_KEY=""
export ACR_NAME=""
export AKS_NAME=""
export RESOURCE_GROUP=""
export PUBLIC_IP=""

# ==========================================
# üìä AFFICHAGE CONFIGURATION
# ==========================================

# Afficher la configuration d√©tect√©e
if [[ "$IS_GITHUB_ACTIONS" == "true" ]]; then
    log_info "üè≠ MODE: GitHub Actions"
    log_info "üì¶ ACR: $AZURE_CONTAINER_REGISTRY"
    log_info "üè∑Ô∏è Tag images: $IMAGE_TAG"
    log_info "üîê Secrets: GitHub Secrets (inject√©s)"
else
    log_info "üõ†Ô∏è MODE: Script Manuel"
    log_info "üì¶ ACR: $AZURE_CONTAINER_REGISTRY"
    log_info "üè∑Ô∏è Tag images: $IMAGE_TAG"
    log_info "üîê Secrets: Configuration manuelle requise"
    log_info "‚ÑπÔ∏è  Utilisez 'make dev' pour le d√©veloppement local"
fi
log_info "üåê Frontend: Mode $ANGULAR_ENV"
log_info "üìä Donn√©es: WITH_DATA=$WITH_DATA"

# Fonction pour v√©rifier les versions Kubernetes disponibles (ROBUSTE)
check_kubernetes_versions() {
    log_info "V√©rification des versions Kubernetes disponibles..."
    
    # Obtenir la r√©gion depuis terraform.tfvars ou utiliser une valeur par d√©faut
    local region="East US"
    if [ -f "$TERRAFORM_DIR/terraform.tfvars" ]; then
        region=$(grep -E '^location\s*=' "$TERRAFORM_DIR/terraform.tfvars" | cut -d'"' -f2 2>/dev/null || echo "East US")
    fi
    
    # CORRECTION CRITIQUE : V√©rifier les versions disponibles avec gestion d'erreur robuste
    local k8s_versions_json=""
    k8s_versions_json=$(az aks get-versions --location "$region" --output json 2>/dev/null || echo "null")
    
    # V√©rifier si la r√©ponse contient des donn√©es valides
    if [[ -z "$k8s_versions_json" ]] || [[ "$k8s_versions_json" == "null" ]] || [[ "$k8s_versions_json" == "{}" ]]; then
        log_warning "Impossible de r√©cup√©rer les versions Kubernetes. V√©rifiez votre connexion Azure."
        return
    fi
    
    # V√©rifier que la structure JSON est valide avant de traiter
    if ! echo "$k8s_versions_json" | jq -e . >/dev/null 2>&1; then
        log_warning "R√©ponse JSON invalide d'Azure CLI pour les versions Kubernetes."
        return
    fi
    
    # Extraire les versions avec gestion d'erreur jq robuste et null-safe
    local available_versions=""
    available_versions=$(echo "$k8s_versions_json" | jq -r '
        if (.orchestrators // null) and ((.orchestrators // []) | length > 0) then
            (.orchestrators // [])[] | 
            select((.supportPlan // null) and ((.supportPlan // []) | length > 0) and ((.supportPlan // [])[] | contains("KubernetesOfficial"))) | 
            .orchestratorVersion // empty
        else
            empty
        end
    ' 2>/dev/null | sort -V || echo "")
    
    if [ -z "$available_versions" ]; then
        log_warning "Impossible de r√©cup√©rer les versions Kubernetes. V√©rifiez votre connexion Azure."
        return
    fi
    
    # Afficher les versions recommand√©es
    local recommended_versions=$(echo "$available_versions" | tail -5)
    log_info "Versions Kubernetes recommand√©es pour la r√©gion '$region':"
    echo "$recommended_versions" | while read version; do
        echo "  - $version"
    done
    
    # V√©rifier si la version configur√©e est support√©e
    local current_version=""
    if [ -f "$TERRAFORM_DIR/terraform.tfvars" ]; then
        current_version=$(grep -E '^kubernetes_version\s*=' "$TERRAFORM_DIR/terraform.tfvars" | cut -d'"' -f2 2>/dev/null)
    fi
    
    if [ -n "$current_version" ]; then
        if echo "$available_versions" | grep -q "^$current_version$"; then
            log_success "Version Kubernetes configur√©e ($current_version) est support√©e"
        else
            log_warning "Version Kubernetes configur√©e ($current_version) n'est pas support√©e !"
            log_warning "Versions recommand√©es : $(echo "$recommended_versions" | tail -3 | tr '\n' ' ')"
        fi
    fi
}

# Fonction pour v√©rifier les pr√©requis
check_prerequisites() {
    log_info "V√©rification des pr√©requis..."
    
    # V√©rifier Azure CLI
    if ! command -v az &> /dev/null; then
        log_error "Azure CLI n'est pas install√©. Installez-le depuis https://docs.microsoft.com/en-us/cli/azure/install-azure-cli"
        exit 1
    fi
    
    # V√©rifier/Installer Terraform
    if ! command -v terraform &> /dev/null; then
        log_warning "Terraform n'est pas install√©. Installation automatique..."
        if [[ "$IS_WINDOWS" == true ]]; then
            if command -v winget &> /dev/null; then
                winget install HashiCorp.Terraform
                # Recharger les variables d'environnement
                if command -v refreshenv &> /dev/null; then
                    refreshenv
                fi
            else
                log_error "Terraform non install√© et winget indisponible. Installez manuellement depuis https://www.terraform.io/downloads.html"
        exit 1
            fi
        else
            # Installation sur Linux/MacOS
            if command -v wget &> /dev/null && command -v unzip &> /dev/null; then
                TF_VERSION="1.6.0"
                wget "https://releases.hashicorp.com/terraform/${TF_VERSION}/terraform_${TF_VERSION}_linux_amd64.zip"
                unzip "terraform_${TF_VERSION}_linux_amd64.zip"
                sudo mv terraform /usr/local/bin/
                rm "terraform_${TF_VERSION}_linux_amd64.zip"
            else
                log_error "Terraform non install√©. Installez manuellement depuis https://www.terraform.io/downloads.html"
                exit 1
            fi
        fi
        
        # V√©rifier que l'installation a r√©ussi
        if ! command -v terraform &> /dev/null; then
            log_error "√âchec de l'installation automatique de Terraform. Installez manuellement."
            exit 1
        else
            log_success "Terraform install√© avec succ√®s"
        fi
    fi
    
    # V√©rifier kubectl
    if ! command -v kubectl &> /dev/null; then
        log_error "kubectl n'est pas install√©. Installez-le depuis https://kubernetes.io/docs/tasks/tools/"
        exit 1
    fi
    
    # V√©rifier Docker de mani√®re ROBUSTE
    if ! command -v docker &> /dev/null; then
        log_error "Docker n'est pas install√©. Installez-le depuis https://docs.docker.com/get-docker/"
        exit 1
    fi
    
    # V√©rifier que Docker daemon fonctionne
    if ! docker info &> /dev/null; then
        log_error "‚ùå Docker est install√© mais le daemon ne fonctionne pas."
        log_error "üîß SOLUTIONS:"
        log_error "   1. D√©marrez Docker Desktop (Windows/Mac)"
        log_error "   2. Ou d√©marrez le service Docker: sudo systemctl start docker (Linux)"
        log_error "   3. V√©rifiez que Docker a les permissions n√©cessaires"
        exit 1
    fi
    
    # V√©rifier l'espace disque disponible de mani√®re ROBUSTE (Windows/Linux/MacOS)
    local available_space=""
    if [[ "$IS_WINDOWS" == true ]]; then
        # Windows : v√©rifier l'espace sur le disque C:
        available_space=$(powershell -c "(Get-PSDrive C | Select-Object -ExpandProperty Free) / 1GB" 2>/dev/null | cut -d'.' -f1 || echo "10")
    else
        # Linux/MacOS : v√©rifier l'espace disque Docker
        available_space=$(df -BG /var/lib/docker 2>/dev/null | awk 'NR==2 {print $4}' | sed 's/G//' || echo "10")
    fi
    
    # Si l'espace est faible ET on est en mode production, nettoyer automatiquement
    if [[ $available_space -lt 5 ]] && [[ $available_space != "10" ]]; then
        log_warning "‚ö†Ô∏è Espace disque faible pour Docker ($available_space GB). Minimum recommand√©: 5GB"
        
        if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
            log_info "üßπ MODE PRODUCTION : Nettoyage automatique du cache Docker..."
            docker system prune -f > /dev/null 2>&1 || true
            docker image prune -f > /dev/null 2>&1 || true
            log_success "‚úÖ Cache Docker nettoy√© automatiquement"
        else
            log_info "üßπ Conseil: Ex√©cutez 'docker system prune -f' pour lib√©rer de l'espace"
        fi
    fi
    
    log_success "‚úÖ Docker fonctionne correctement"
    
    # V√©rifier/Installer jq pour le parsing JSON
    if ! command -v jq &> /dev/null; then
        log_warning "jq n'est pas install√©. Installation automatique..."
        if [[ "$IS_WINDOWS" == true ]]; then
            if command -v winget &> /dev/null; then
                winget install jqlang.jq
            else
                log_warning "jq non install√©. Install√© manuellement depuis https://jqlang.github.io/jq/"
            fi
        else
            if command -v apt-get &> /dev/null; then
                sudo apt-get install -y jq
            elif command -v yum &> /dev/null; then
                sudo yum install -y jq
            elif command -v brew &> /dev/null; then
                brew install jq
            else
                log_warning "jq non install√©. Install√© manuellement depuis https://jqlang.github.io/jq/"
            fi
        fi
    fi
    
    # V√©rifier/Installer Helm
    if ! command -v helm &> /dev/null; then
        log_warning "Helm n'est pas install√©. Installation automatique..."
        install_helm
    else
        log_success "Helm est d√©j√† install√©"
    fi
    
    log_success "Tous les pr√©requis sont install√©s"
}

# Nouvelle fonction pour installer Helm
install_helm() {
    log_info "Installation de Helm..."
    
    # T√©l√©charger et installer Helm
    curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
    chmod 700 get_helm.sh
    ./get_helm.sh
    rm get_helm.sh
    
    # V√©rifier l'installation
    if command -v helm &> /dev/null; then
        log_success "Helm install√© avec succ√®s: $(helm version --short)"
    else
        log_error "√âchec de l'installation de Helm"
        exit 1
    fi
}

# ‚úÖ FONCTION AUTOMATIQUE RENFORC√âE : Configuration IP statique pour NGINX Ingress
configure_static_ip_for_nginx() {
    log_info "üîß Configuration automatique RENFORC√âE de l'IP statique pour NGINX Ingress..."
    
    # 1. R√©cup√©rer l'IP statique depuis Azure CLI (plus fiable que Terraform)
    local static_ip=""
    local node_resource_group=""
    
    # Trouver le resource group automatique cr√©√© par AKS
    node_resource_group=$(az group list --query "[?contains(name, 'MC_${RESOURCE_GROUP}_${AKS_NAME}')].name" --output tsv 2>/dev/null | head -1)
    
    if [[ -n "$node_resource_group" ]]; then
        log_info "üîç Recherche IP statique dans : $node_resource_group..."
        
        # Nom standard de l'IP statique pour ingress
        local ip_name="ibis-x-prod-ingress-ip"
        
        # Essayer de r√©cup√©rer l'IP statique existante
        static_ip=$(az network public-ip show --resource-group "$node_resource_group" --name "$ip_name" --query "ipAddress" --output tsv 2>/dev/null || echo "")
        
        # Si l'IP n'existe pas, la cr√©er OBLIGATOIREMENT
        if [[ -z "$static_ip" ]]; then
            log_info "üîß Cr√©ation OBLIGATOIRE de l'IP statique manquante..."
            local create_result=$(az network public-ip create \
                --resource-group "$node_resource_group" \
                --name "$ip_name" \
                --allocation-method Static \
                --sku Standard \
                --location eastus \
                --query "publicIp.ipAddress" --output tsv 2>/dev/null || echo "")
            
            if [[ -n "$create_result" ]]; then
                static_ip="$create_result"
                log_success "‚úÖ IP statique cr√©√©e : $static_ip"
            else
                log_error "‚ùå √âCHEC critique : Impossible de cr√©er l'IP statique !"
                exit 1
            fi
        else
            log_success "‚úÖ IP statique existante trouv√©e : $static_ip"
        fi
        
        # Double v√©rification : s'assurer que l'IP existe et est accessible
        local ip_exists=$(az network public-ip show --resource-group "$node_resource_group" --name "$ip_name" --query "ipAddress" --output tsv 2>/dev/null || echo "")
        if [[ -z "$ip_exists" ]] || [[ "$ip_exists" != "$static_ip" ]]; then
            log_error "‚ùå ERREUR CRITIQUE : L'IP statique est incoh√©rente (trouv√©e: $ip_exists, attendue: $static_ip) !"
            exit 1
        fi
        
        log_success "‚úÖ V√©rification IP statique confirm√©e : $static_ip (accessible et coh√©rente)"
    else
        log_error "‚ùå ERREUR CRITIQUE : Resource group AKS non trouv√© !"
        exit 1
    fi
    
    # 3. VALIDATION OBLIGATOIRE
    if [[ -z "$static_ip" ]] || [[ -z "$node_resource_group" ]]; then
        log_error "‚ùå ERREUR CRITIQUE : Impossible de configurer l'IP statique !"
        log_error "   IP statique: ${static_ip:-NON TROUV√âE}"
        log_error "   Node RG: ${node_resource_group:-NON TROUV√â}"
        exit 1
    fi
    
    # 4. Mettre √† jour automatiquement le fichier nginx-ingress-values.yaml
    local nginx_values_file="$K8S_DIR/helm-values/nginx-ingress-values.yaml"
    local nginx_values_backup="$nginx_values_file.backup-$(date +%s)"
    
    log_info "üìù Mise √† jour FORC√âE de nginx-ingress-values.yaml..."
    log_info "   üéØ IP statique FORC√âE: $static_ip"
    log_info "   üéØ Resource Group: $node_resource_group"
    
    # Sauvegarder le fichier original
    cp "$nginx_values_file" "$nginx_values_backup" 2>/dev/null || true
    
    # Cr√©er la nouvelle configuration avec IP statique FORC√âE
    cat > "$nginx_values_file" << EOF
controller:
  replicaCount: 2
  nodeSelector:
    kubernetes.io/os: linux
  service:
    type: LoadBalancer
    loadBalancerIP: "$static_ip"
    annotations:
      # FORCE L'IP STATIQUE - Configuration par deploy-to-azure.sh
      service.beta.kubernetes.io/azure-load-balancer-static-ip: "$static_ip"
      service.beta.kubernetes.io/azure-load-balancer-resource-group: "$node_resource_group"
      service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: /healthz
      # EMP√äCHER Azure d'utiliser une IP dynamique
      service.beta.kubernetes.io/azure-load-balancer-mode: "shared"
  admissionWebhooks:
    patch:
      nodeSelector:
        kubernetes.io/os: linux

defaultBackend:
  nodeSelector:
    kubernetes.io/os: linux
EOF
    
    # Exporter pour usage ult√©rieur
    export PUBLIC_IP="$static_ip"
    export NODE_RESOURCE_GROUP="$node_resource_group"
    export STATIC_IP_CONFIRMED="$static_ip"
    
    log_success "‚úÖ Configuration IP statique FORC√âE termin√©e !"
    log_success "   üéØ NGINX utilisera OBLIGATOIREMENT l'IP: $static_ip"
    log_success "   üíæ Sauvegarde: $nginx_values_backup"
}

# ‚úÖ FONCTION RENFORC√âE pour installer NGINX Ingress Controller avec IP statique FORC√âE
install_nginx_ingress() {
    log_info "üöÄ Installation RENFORC√âE de NGINX Ingress Controller avec IP statique FORC√âE..."
    
    # ‚úÖ AUTOMATISATION IP STATIQUE RENFORC√âE
    configure_static_ip_for_nginx
    
    # Utiliser les variables export√©es par configure_static_ip_for_nginx
    local static_ip="$STATIC_IP_CONFIRMED"
    local node_resource_group="$NODE_RESOURCE_GROUP"
    
    if [[ -z "$static_ip" ]]; then
        log_error "‚ùå ERREUR CRITIQUE : IP statique non configur√©e !"
        exit 1
    fi
    
    log_info "üéØ IP statique confirm√©e pour NGINX : $static_ip"
    
    # V√©rifier si NGINX existe d√©j√† avec une IP diff√©rente
    local existing_nginx_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
    
    if [[ -n "$existing_nginx_ip" && "$existing_nginx_ip" != "$static_ip" ]]; then
        log_warning "‚ö†Ô∏è NGINX existant avec IP incorrecte ($existing_nginx_ip ‚â† $static_ip)"
        log_info "üîÑ Suppression et recr√©ation de NGINX avec IP statique..."
        helm uninstall ingress-nginx -n ingress-nginx 2>/dev/null || true
        kubectl delete namespace ingress-nginx --ignore-not-found=true
        sleep 10
    fi
    
    # Ajouter le repository Helm NGINX
    helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
    helm repo update
    
    # Installation FORC√âE avec toutes les annotations IP statique
    log_info "üì¶ Installation NGINX avec IP statique FORC√âE : $static_ip"
    helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx \
        --namespace ingress-nginx \
        --create-namespace \
        --set controller.service.loadBalancerIP="$static_ip" \
        --set controller.service.type=LoadBalancer \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-static-ip=$static_ip" \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-resource-group=$node_resource_group" \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-health-probe-request-path=/healthz" \
        --set "controller.service.annotations.service\.beta\.kubernetes\.io/azure-load-balancer-mode=shared" \
        --set controller.replicaCount=2 \
        --set "controller.nodeSelector.kubernetes\.io/os=linux" \
        --set "defaultBackend.nodeSelector.kubernetes\.io/os=linux" \
        --wait --timeout=10m
    
    # ‚úÖ VALIDATION POST-INSTALLATION : V√©rifier que NGINX utilise bien l'IP statique
    log_info "üîç VALIDATION : V√©rification de l'IP assign√©e √† NGINX..."
    
    local max_attempts=30
    local attempt=0
    local nginx_actual_ip=""
    
    while [[ $attempt -lt $max_attempts ]]; do
        nginx_actual_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
        
        if [[ "$nginx_actual_ip" == "$static_ip" ]]; then
            log_success "‚úÖ SUCC√àS : NGINX utilise l'IP statique correcte : $nginx_actual_ip"
            break
        elif [[ -n "$nginx_actual_ip" && "$nginx_actual_ip" != "$static_ip" ]]; then
            log_error "‚ùå √âCHEC CRITIQUE : NGINX utilise une IP incorrecte !"
            log_error "   IP attendue : $static_ip"
            log_error "   IP actuelle : $nginx_actual_ip"
            log_info "üîÑ Tentative de correction..."
            
            # Forcer la correction
            kubectl patch svc ingress-nginx-controller -n ingress-nginx -p "{\"spec\":{\"loadBalancerIP\":\"$static_ip\"}}"
        fi
        
        attempt=$((attempt + 1))
        log_info "‚è≥ Attente IP statique... ($attempt/$max_attempts) - IP actuelle: ${nginx_actual_ip:-PENDING}"
        sleep 10
    done
    
    # Validation finale
    nginx_actual_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
    
    if [[ "$nginx_actual_ip" == "$static_ip" ]]; then
        log_success "üéâ NGINX Ingress Controller install√© avec IP statique CONFIRM√âE : $static_ip"
        
        # Afficher l'√©tat final pour confirmation
        log_info "üìä √âtat final NGINX Ingress :"
        kubectl get svc -n ingress-nginx ingress-nginx-controller
    else
        log_error "‚ùå √âCHEC FINAL : NGINX n'utilise pas l'IP statique !"
        log_error "   IP attendue : $static_ip"
        log_error "   IP actuelle : ${nginx_actual_ip:-AUCUNE}"
        log_error "üîß Veuillez corriger manuellement ou relancer le script"
        exit 1
    fi
}

# Nouvelle fonction pour installer Cert-Manager
install_cert_manager() {
    log_info "Installation de Cert-Manager..."
    
    # Ajouter le repository Helm Cert-Manager
    helm repo add jetstack https://charts.jetstack.io
    helm repo update
    
    # Installer Cert-Manager
    helm upgrade --install cert-manager jetstack/cert-manager \
        --namespace cert-manager \
        --create-namespace \
        --version v1.13.0 \
        --set installCRDs=true \
        --wait
    
    log_success "Cert-Manager install√©"
}

# Fonction pour v√©rifier que les noms de projet sont corrects
verify_project_names() {
    log_info "V√©rification des noms de projet IBIS-X..."
    
    # Les noms sont d√©j√† corrects depuis la migration, pas besoin de modification
    log_success "Noms de projet IBIS-X v√©rifi√©s"
}

# Fonction pour v√©rifier la connexion Azure
check_azure_login() {
    log_info "V√©rification de la connexion Azure..."
    
    if ! az account show &> /dev/null; then
        log_warning "Vous n'√™tes pas connect√© √† Azure. Connexion en cours..."
        az login
    fi
    
    # Afficher le compte actuel
    ACCOUNT_INFO=$(az account show --output json)
    SUBSCRIPTION_NAME=$(echo "$ACCOUNT_INFO" | jq -r '.name')
    SUBSCRIPTION_ID=$(echo "$ACCOUNT_INFO" | jq -r '.id')
    
    log_success "Connect√© √† Azure:"
    echo "  Subscription: $SUBSCRIPTION_NAME"
    echo "  ID: $SUBSCRIPTION_ID"
}

# Fonction pour initialiser Terraform
init_terraform() {
    log_info "Initialisation de Terraform..."
    
    cd "$TERRAFORM_DIR"
    
    # V√©rifier si terraform.tfvars existe
    if [ ! -f "terraform.tfvars" ]; then
        log_warning "Le fichier terraform.tfvars n'existe pas."
        echo "Cr√©ation du fichier terraform.tfvars √† partir du template..."
        cp terraform.tfvars.example terraform.tfvars
        
        log_warning "IMPORTANT: Veuillez modifier le fichier terraform.tfvars selon vos besoins avant de continuer."
        read -p "Appuyez sur Entr√©e pour ouvrir le fichier terraform.tfvars dans l'√©diteur par d√©faut..."
        
        # Ouvrir l'√©diteur
        ${EDITOR:-nano} terraform.tfvars
        
        echo
        read -p "Avez-vous termin√© la configuration du fichier terraform.tfvars ? (y/N): " confirm
        if [[ $confirm != [yY] && $confirm != [yY][eE][sS] ]]; then
            log_error "Configuration annul√©e. Veuillez configurer terraform.tfvars et relancer le script."
            exit 1
        fi
    fi
    
    # Initialiser Terraform
    terraform init
    
    log_success "Terraform initialis√© avec succ√®s"
}

# Fonction pour planifier et appliquer Terraform (100% AUTOMATIQUE)
deploy_infrastructure() {
    log_info "D√©ploiement de l'infrastructure Azure..."
    
    cd "$TERRAFORM_DIR"
    
    # Planifier le d√©ploiement
    log_info "G√©n√©ration du plan Terraform..."
    terraform plan -out=tfplan
    
    # CORRECTION CRITIQUE : Mode production = automatique, pas d'interaction utilisateur
    if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        log_info "üöÄ MODE PRODUCTION : D√©ploiement automatique activ√© (pas de confirmation requise)"
        log_info "üìã Terraform va cr√©er/modifier les ressources Azure selon le plan ci-dessus."
    else
        # Demander confirmation seulement en mode d√©veloppement/test
        echo
        log_warning "Terraform va cr√©er/modifier les ressources Azure ci-dessus."
        read -p "Voulez-vous continuer ? (y/N): " confirm
        
        if [[ $confirm != [yY] && $confirm != [yY][eE][sS] ]]; then
            log_error "D√©ploiement annul√© par l'utilisateur"
            exit 1
        fi
    fi
    
    # Appliquer le plan
    log_info "Application du plan Terraform..."
    terraform apply -auto-approve tfplan
    
    log_success "Infrastructure Azure d√©ploy√©e avec succ√®s !"
}

# Fonction pour r√©cup√©rer les informations d'infrastructure
get_terraform_outputs() {
    log_info "üìä R√©cup√©ration des informations d'infrastructure - Mode: $DEPLOYMENT_MODE"
    
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        # üè≠ MODE GITHUB ACTIONS: Variables d'environnement pr√©d√©finies
        log_info "üè≠ Utilisation des variables GitHub Actions..."
        get_github_infrastructure_info
    else
        # üõ†Ô∏è MODE MANUEL: Variables d'environnement ou d√©tection Azure CLI
        log_info "üõ†Ô∏è Configuration manuelle ou d√©tection Azure..."
        get_manual_infrastructure_info
    fi
    
    # Validation finale
    if [[ -z "$ACR_NAME" ]] || [[ -z "$AKS_NAME" ]] || [[ -z "$RESOURCE_GROUP" ]]; then
        log_error "‚ùå Informations d'infrastructure manquantes !"
        log_error "ACR: $ACR_NAME | AKS: $AKS_NAME | RG: $RESOURCE_GROUP"
        exit 1
    fi
    
    # Mise √† jour automatique de tous les fichiers avec le bon nom ACR
    log_info "üîß Mise √† jour automatique des fichiers avec ACR: $ACR_NAME"
    update_all_acr_references
    
    log_success "‚úÖ Informations d'infrastructure r√©cup√©r√©es:"
    echo "  Mode: $DEPLOYMENT_MODE"
    echo "  ACR Registry: $ACR_NAME"
    echo "  AKS Cluster: $AKS_NAME"
    echo "  Resource Group: $RESOURCE_GROUP"
    echo "  Storage Account: ${STORAGE_ACCOUNT:-N/A}"
    echo "  Public IP: ${PUBLIC_IP:-N/A}"
}

# Fonction pour r√©cup√©rer les infos GitHub Actions
get_github_infrastructure_info() {
    log_info "üìã Configuration depuis variables d'environnement GitHub Actions..."
    
    # Les variables sont d√©j√† d√©finies par GitHub Actions
    export ACR_NAME="$AZURE_CONTAINER_REGISTRY"
    export AKS_NAME="$AKS_CLUSTER_NAME"
    export RESOURCE_GROUP="$AZURE_RESOURCE_GROUP"
    
    # R√©cup√©rer l'IP publique si possible
    PUBLIC_IP=$(az network public-ip list --resource-group "$RESOURCE_GROUP" --query "[0].ipAddress" -o tsv 2>/dev/null || echo "")
    export PUBLIC_IP
    
    # Storage sera r√©cup√©r√© dynamiquement par les fonctions de secrets
    log_success "‚úÖ Configuration GitHub Actions charg√©e"
}

# Fonction pour r√©cup√©rer les infos manuellement
get_manual_infrastructure_info() {
    log_info "üõ†Ô∏è Configuration manuelle - Variables d'environnement ou Azure CLI..."
    
    # Utiliser les variables d'environnement pr√©d√©finies (d√©j√† export√©es)
    export ACR_NAME="$AZURE_CONTAINER_REGISTRY"
    export AKS_NAME="$AKS_CLUSTER_NAME"
    export RESOURCE_GROUP="$AZURE_RESOURCE_GROUP"
    
    # Si l'infrastructure a √©t√© cr√©√©e par Terraform, essayer de r√©cup√©rer depuis Terraform
    if [[ -f "$TERRAFORM_DIR/terraform.tfstate" ]]; then
        log_info "üìÇ Infrastructure Terraform d√©tect√©e, r√©cup√©ration des outputs..."
        cd "$TERRAFORM_DIR"
        
        # R√©cup√©rer les outputs Terraform
        # R√©cup√©rer les outputs Terraform (d√©j√† dans TERRAFORM_DIR)
        local tf_storage=$(terraform output -raw storage_account_name 2>/dev/null || echo "")
        local tf_acr=$(terraform output -raw acr_name 2>/dev/null || echo "")
        local tf_aks=$(terraform output -raw aks_cluster_name 2>/dev/null || echo "")
        local tf_rg=$(terraform output -raw resource_group_name 2>/dev/null || echo "")
        local tf_ip=$(terraform output -raw public_ip_address 2>/dev/null || echo "")
        
        # Utiliser les valeurs Terraform si disponibles
        if [[ -n "$tf_acr" ]]; then
            export ACR_NAME="$tf_acr"
            export AKS_NAME="$tf_aks"
            export RESOURCE_GROUP="$tf_rg"
            export STORAGE_ACCOUNT="$tf_storage"
            export PUBLIC_IP="$tf_ip"
            
            if [[ -n "$tf_storage" ]]; then
                STORAGE_KEY=$(terraform output -raw storage_account_primary_key 2>/dev/null || echo "")
                export STORAGE_KEY
            fi
            
            log_success "‚úÖ Configuration r√©cup√©r√©e depuis Terraform"
        else
            log_warning "‚ö†Ô∏è Terraform outputs vides, utilisation des variables d'environnement"
        fi
        
        cd "$PROJECT_ROOT"
    fi
    
    # Fallback Azure CLI pour r√©cup√©rer l'IP publique
    if [[ -z "$PUBLIC_IP" ]]; then
        PUBLIC_IP=$(az network public-ip list --resource-group "$RESOURCE_GROUP" --query "[0].ipAddress" -o tsv 2>/dev/null || echo "")
        export PUBLIC_IP
    fi
    
    log_info "üìã Configuration finale:"
    log_info "  ACR: $ACR_NAME"
    log_info "  AKS: $AKS_NAME"
    log_info "  Resource Group: $RESOURCE_GROUP"
    
    log_success "‚úÖ Configuration manuelle charg√©e"
}

# Fonction pour mettre √† jour toutes les r√©f√©rences ACR et Storage Account
update_all_acr_references() {
    log_info "üîç Remplacement des placeholders ACR : PLACEHOLDER_ACR ‚Üí $ACR_NAME"
    log_info "üéØ Remplacement des placeholders dans tous les fichiers..."
    
    local updated_files=0
    local critical_files=(
        "k8s/overlays/minikube-jobs-only/api-gateway-migration-job.yaml"
        "k8s/overlays/minikube-jobs-only/service-selection-migration-job.yaml"
        "k8s/overlays/azure/kustomization.yaml"
    )
    
    # Traitement des fichiers critiques pour ACR
    for file in "${critical_files[@]}"; do
        if [[ -f "$file" ]]; then
            log_info "‚úÖ $file - aucun placeholder √† remplacer"
            ((updated_files++))
        fi
    done
    
    # üéØ TRAITEMENT SP√âCIAL : Storage Secrets avec valeurs Terraform
    local storage_secrets_file="k8s/base/service-selection/storage-secrets.yaml"
    
    if [[ -f "$storage_secrets_file" && -n "$STORAGE_ACCOUNT" && -n "$STORAGE_KEY" ]]; then
        log_info "üîß Mise √† jour CRITIQUE: Storage Account dans $storage_secrets_file"
        log_info "  üèóÔ∏è Ancien: ibisxprodstg2205 (cod√© en dur)"
        log_info "  ‚ú® Nouveau: $STORAGE_ACCOUNT (depuis Terraform)"
        
        # Encoder les nouvelles valeurs en base64
        local storage_name_b64=$(echo -n "$STORAGE_ACCOUNT" | base64 -w 0)
        local storage_key_b64=$(echo -n "$STORAGE_KEY" | base64 -w 0)
        
        # Cr√©er une sauvegarde
        cp "$storage_secrets_file" "${storage_secrets_file}.backup-$(date +%s)"
        
        # Remplacer la valeur hardcod√©e par la vraie valeur Terraform
        sed -i.tmp "s|azure-storage-account-name: aWJpc3hwcm9kc3RnMjIwNQ==|azure-storage-account-name: $storage_name_b64|g" "$storage_secrets_file"
        sed -i.tmp "s|azure-storage-account-key: .*|azure-storage-account-key: $storage_key_b64|g" "$storage_secrets_file"
        
        # Nettoyer les fichiers temporaires
        rm -f "${storage_secrets_file}.tmp"
        
        log_success "‚úÖ Storage Account mis √† jour: $STORAGE_ACCOUNT"
        log_success "‚úÖ Storage Key mise √† jour depuis Terraform"
        
        # üîÑ FORCER LA RECR√âATION DU SECRET dans Kubernetes
        log_info "üîÑ Suppression du secret storage-secrets existant pour forcer la mise √† jour..."
        kubectl delete secret storage-secrets -n ibis-x 2>/dev/null || true
        log_success "‚úÖ Secret storage-secrets pr√™t pour recr√©ation avec nouvelles valeurs"
        
        ((updated_files++))
    else
        log_warning "‚ö†Ô∏è Storage Account non disponible ou fichier manquant"
        log_warning "  STORAGE_ACCOUNT: ${STORAGE_ACCOUNT:-VIDE}"
        log_warning "  STORAGE_KEY: ${STORAGE_KEY:+D√âFINI}"
        log_warning "  Fichier: $storage_secrets_file"
    fi
    
    log_success "üéØ Remplacement termin√© - $updated_files fichier(s) trait√©s avec ACR : $ACR_NAME"
}

# Fonction pour v√©rifier les ressources AKS (cluster maintenant configur√© avec 3 n≈ìuds)
check_aks_resources() {
    log_info "üîç V√©rification des ressources AKS..."
    
    # Obtenir le nombre de n≈ìuds actuels
    local current_nodes=$(kubectl get nodes --no-headers | wc -l)
    local ready_nodes=$(kubectl get nodes --no-headers | grep " Ready " | wc -l)
    
    log_info "üìä √âtat du cluster AKS:"
    log_info "  üìà N≈ìuds totaux: $current_nodes"
    log_info "  ‚úÖ N≈ìuds pr√™ts: $ready_nodes"
    
    if [[ "$ready_nodes" -ge 3 ]]; then
        log_success "‚úÖ Cluster AKS a des ressources suffisantes ($ready_nodes n≈ìuds pr√™ts)"
    else
        log_warning "‚ö†Ô∏è Seulement $ready_nodes n≈ìuds pr√™ts, PostgreSQL pourrait avoir des probl√®mes de scheduling"
        
        # V√©rifier si PostgreSQL a des probl√®mes de scheduling
        local postgres_status=$(kubectl get pods -n ibis-x -l app=postgresql --no-headers 2>/dev/null | head -1 | awk '{print $3}' || echo "")
        
        if [[ "$postgres_status" == "Pending" ]]; then
            log_warning "üö® PostgreSQL en statut Pending - Attente que plus de n≈ìuds soient pr√™ts..."
            log_info "üí° Terraform est configur√© pour 3 n≈ìuds, ils vont √™tre disponibles bient√¥t"
            
            # Attendre un peu plus pour que les n≈ìuds soient pr√™ts
            local timeout=180
            local elapsed=0
            
            while [[ $elapsed -lt $timeout ]]; do
                local ready_now=$(kubectl get nodes --no-headers | grep " Ready " | wc -l)
                if [[ "$ready_now" -ge 3 ]]; then
                    log_success "‚úÖ Maintenant $ready_now n≈ìuds sont pr√™ts, PostgreSQL peut d√©marrer"
                    break
                fi
                
                log_info "  ‚è≥ Attente des n≈ìuds: $ready_now/3 pr√™ts..."
                sleep 15
                ((elapsed += 15))
            done
        fi
    fi
}

# Fonction pour l'initialisation automatique des donn√©es en PRODUCTION
initialize_sample_data() {
    if [[ "$WITH_DATA" == "true" ]]; then
        log_info "üìä Initialisation des donn√©es d'exemple..."
        
        # Nettoyer tout job d'import existant pour √©viter les conflits
        log_info "üßπ Nettoyage complet des jobs d'import pr√©c√©dents..."
        kubectl delete job kaggle-dataset-import-job -n "${K8S_NAMESPACE:-ibis-x}" --ignore-not-found=true
        kubectl delete configmap kaggle-datasets-config -n "${K8S_NAMESPACE:-ibis-x}" --ignore-not-found=true
        
        # Attendre que le nettoyage soit termin√©
        sleep 5
        
        # Lancer le job d'import Kaggle avec ressources optimis√©es
        log_info "üöÄ Lancement du job d'import Kaggle avec ressources optimis√©es..."
        
        # Cr√©er le job d'import Kaggle avec image ACR et ressources optimis√©es
        log_info "üîß Cr√©ation du job Kaggle avec image ACR et ressources Azure optimis√©es..."
        
        # R√©cup√©rer l'ACR correct depuis Terraform
        local ACR_NAME=""
        ACR_NAME=$(cd "${TERRAFORM_DIR}" && terraform output -raw acr_name 2>/dev/null || echo "")
        if [[ -z "$ACR_NAME" ]]; then
            log_warning "Impossible de r√©cup√©rer le nom ACR depuis Terraform, utilisation de la variable par d√©faut"
            ACR_NAME="${AZURE_CONTAINER_REGISTRY}"
        fi
        
        log_info "üê≥ Utilisation de l'ACR: ${ACR_NAME}.azurecr.io"
        
        # Cr√©er le job directement avec kubectl create et la bonne image ACR
        cat <<EOF | kubectl apply -f - -n "${K8S_NAMESPACE:-ibis-x}"
apiVersion: batch/v1
kind: Job
metadata:
  name: kaggle-dataset-import-job
  namespace: ibis-x
  labels:
    app: kaggle-dataset-import
    component: data-import
spec:
  backoffLimit: 3
  template:
    metadata:
      labels:
        app: kaggle-dataset-import
        component: data-import
    spec:
      restartPolicy: OnFailure
      serviceAccountName: default
      containers:
      - name: kaggle-importer
        image: ${ACR_NAME}.azurecr.io/service-selection:latest
        imagePullPolicy: Always
        command: ["python"]
        args: ["kaggle-import/main.py", "--force-refresh"]
        workingDir: /app
        
        # Ressources optimis√©es pour production Azure
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "8Gi"
            cpu: "2000m"
        
        # Variables d'environnement pour la base de donn√©es
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secrets
              key: database-url
        
        # Variables optimisation m√©moire
        - name: PANDAS_MAX_MEMORY_USAGE
          value: "6000000000"
        - name: CHUNK_SIZE_LARGE_DATASETS
          value: "5000"
        - name: MEMORY_OPTIMIZED_MODE
          value: "true"
        - name: PYTHONUNBUFFERED
          value: "1"
        
        # Configuration stockage Azure
        - name: STORAGE_BACKEND
          value: "azure"
        - name: AZURE_STORAGE_CONNECTION_STRING
          valueFrom:
            secretKeyRef:
              name: storage-secrets
              key: azure-connection-string
        - name: AZURE_CONTAINER_NAME
          value: "ibis-x-datasets"
        
        # Credentials Kaggle
        - name: KAGGLE_USERNAME
          valueFrom:
            secretKeyRef:
              name: kaggle-secrets
              key: username
        - name: KAGGLE_KEY
          valueFrom:
            secretKeyRef:
              name: kaggle-secrets
              key: key
        
        # Configuration job
        - name: FORCE_REFRESH
          value: "true"
        
        # Volumes pour fichiers temporaires
        volumeMounts:
        - name: temp-storage
          mountPath: /tmp
      
      # Volumes avec espace augment√©
      volumes:
      - name: temp-storage
        emptyDir:
          sizeLimit: "15Gi"
      
      # Timeout √©tendu pour gros datasets
      activeDeadlineSeconds: 3600
EOF
        
        # Attendre que le job soit cr√©√© et que le pod d√©marre
        log_info "‚è≥ Attente du d√©marrage du job d'import..."
        wait_for_job_to_start "kaggle-dataset-import-job" "${K8S_NAMESPACE:-ibis-x}" 180
        
        # Monitoring du job avec logs en temps r√©el (timeout √©tendu pour production)
        if monitor_job_progress "kaggle-dataset-import-job" "${K8S_NAMESPACE:-ibis-x}" 60; then
            log_success "‚úÖ Import Kaggle termin√© avec succ√®s"
        else
            # En cas d'√©chec ou timeout, diagnostiquer et g√©rer intelligemment
            log_error "‚ùå √âchec ou timeout du job d'import Kaggle"
            
            # V√©rifier si le job est encore en cours d'ex√©cution
            local job_still_running=false
            local pod_name
            pod_name=$(kubectl get pods -n "${K8S_NAMESPACE:-ibis-x}" -l job-name=kaggle-dataset-import-job -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            
            if [[ -n "$pod_name" ]]; then
                local pod_status
                pod_status=$(kubectl get pod "$pod_name" -n "${K8S_NAMESPACE:-ibis-x}" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                
                if [[ "$pod_status" == "Running" ]]; then
                    job_still_running=true
                    log_warning "‚ö†Ô∏è Le job est encore en cours d'ex√©cution ! Timeout probablement trop court."
                    log_info "üìä Option 1: Attendre plus longtemps (job encore actif)"
                    log_info "üìä Option 2: Continuer sans les donn√©es Kaggle"
                    log_info "üìä Option 3: Utiliser le fallback kubectl exec"
                    
                    # Proposer d'attendre encore un peu si le job tourne encore
                    log_info "üîÑ Le job semble encore actif, on peut continuer le d√©ploiement et v√©rifier plus tard..."
                    log_warning "‚è≥ Import Kaggle en cours, le d√©ploiement va continuer..."
                    
                    # Afficher les logs actuels pour information
                    log_info "üìã Logs actuels du job en cours:"
                    kubectl logs "$pod_name" -n "${K8S_NAMESPACE:-ibis-x}" --tail=20 2>/dev/null || log_warning "Impossible de r√©cup√©rer les logs"
                    
                    # Ne pas √©chouer compl√®tement si le job tourne encore
                    log_info "üöÄ Continuer le d√©ploiement, l'import se terminera en arri√®re-plan"
                fi
            fi
            
            # Afficher des informations d√©taill√©es pour debug
            log_info "üìã Logs complets du job pour diagnostic:"
            kubectl logs job/kaggle-dataset-import-job -n "${K8S_NAMESPACE:-ibis-x}" --tail=100 2>/dev/null || log_warning "Impossible de r√©cup√©rer les logs complets"
            
            log_info "üìä √âtat d√©taill√© du job:"
            kubectl describe job kaggle-dataset-import-job -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || log_warning "Impossible de d√©crire le job"
            
            log_info "üîç √âtat des pods du job:"
            kubectl get pods -n "${K8S_NAMESPACE:-ibis-x}" -l job-name=kaggle-dataset-import-job -o wide 2>/dev/null || log_warning "Impossible de lister les pods du job"
            
            # Seulement essayer le fallback si le job n'est PAS en cours d'ex√©cution
            if [[ "$job_still_running" == "false" ]]; then
                log_warning "üîÑ Job arr√™t√© ou √©chou√©, tentative de fallback avec kubectl exec sur service-selection..."
                
                if retry_command 2 5 kubectl rollout status deployment/service-selection -n "${K8S_NAMESPACE:-ibis-x}" --timeout=300s; then
                    log_info "üéØ Service-selection pr√™t, lancement de l'import via kubectl exec..."
                    
                    if retry_command 2 10 kubectl exec -n "${K8S_NAMESPACE:-ibis-x}" deployment/service-selection -- python kaggle-import/main.py --force-refresh; then
                        log_success "‚úÖ Fallback r√©ussi avec kubectl exec"
                    else
                        log_error "‚ùå √âchec du fallback kubectl exec"
                        log_warning "‚ö†Ô∏è Continuons le d√©ploiement sans les donn√©es Kaggle pour l'instant"
                        log_info "üí° Vous pouvez importer les donn√©es manuellement plus tard avec:"
                        log_info "   kubectl exec -n ${K8S_NAMESPACE:-ibis-x} deployment/service-selection -- python kaggle-import/main.py --force-refresh"
                    fi
                else
                    log_error "‚ùå Service-selection n'est pas pr√™t pour le fallback"
                    log_warning "‚ö†Ô∏è Continuons le d√©ploiement sans les donn√©es Kaggle pour l'instant"
                fi
            else
                log_info "üìä Job encore en cours, pas besoin de fallback maintenant"
            fi
        fi
        
        log_success "‚úÖ Donn√©es d'exemple initialis√©es"
    else
        log_info "üìä Initialisation des donn√©es d√©sactiv√©e (WITH_DATA=$WITH_DATA)"
    fi
}

# Fonction pour attendre et v√©rifier les migrations
wait_for_migrations() {
    log_info "‚è≥ Attente des migrations de base de donn√©es..."
    
    # Attendre PostgreSQL avec retry
    log_info "üóÑÔ∏è Attente que PostgreSQL soit pr√™t..."
    if ! retry_command 3 10 kubectl wait pod --selector=app=postgresql --for=condition=Ready -n "${K8S_NAMESPACE:-ibis-x}" --timeout=300s; then
        log_error "‚ùå PostgreSQL non pr√™t apr√®s plusieurs tentatives"
        return 1
    fi
    
    # Attendre les migrations avec gestion d'erreur robuste
    log_info "üîÑ Attente des jobs de migration..."
    
    # Migration API Gateway
    if retry_command 2 5 kubectl wait --for=condition=complete job/api-gateway-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --timeout=300s; then
        log_success "‚úÖ Migration API Gateway termin√©e"
    else
        log_warning "‚ö†Ô∏è Migration API Gateway non trouv√©e ou √©chou√©e - v√©rification manuelle..."
        kubectl describe job/api-gateway-migration-job -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || log_warning "Job API Gateway non trouv√©"
    fi
    
    # Migration Service Selection
    if retry_command 2 5 kubectl wait --for=condition=complete job/service-selection-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --timeout=300s; then
        log_success "‚úÖ Migration Service Selection termin√©e"
    else
        log_warning "‚ö†Ô∏è Migration Service Selection non trouv√©e ou √©chou√©e - v√©rification manuelle..."
        kubectl describe job/service-selection-migration-job -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || log_warning "Job Service Selection non trouv√©"
    fi
    
    # Migration ML Pipeline
    if retry_command 2 5 kubectl wait --for=condition=complete job/ml-pipeline-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --timeout=300s; then
        log_success "‚úÖ Migration ML Pipeline termin√©e"
    else
        log_warning "‚ö†Ô∏è Migration ML Pipeline non trouv√©e ou √©chou√©e - v√©rification manuelle..."
        kubectl describe job/ml-pipeline-migration-job -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || log_warning "Job ML Pipeline non trouv√©"
    fi
    
    # Red√©marrer les applications (comme dans GitHub Actions)
    if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
        log_info "üîÑ Red√©marrage des applications (mode production)..."
        kubectl rollout restart deployment api-gateway -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment service-selection -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment ml-pipeline -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment ml-pipeline-celery-worker -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
    fi
    
    log_success "‚úÖ Migrations et red√©marrages termin√©s"
}

# Fonction de nettoyage des jobs (comme dans GitHub Actions)
cleanup_migration_jobs() {
    if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
        log_info "üßπ Nettoyage des jobs de migration (mode production)..."
        kubectl delete job api-gateway-migration-job service-selection-migration-job ml-pipeline-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --ignore-not-found=true
        log_success "‚úÖ Jobs de migration nettoy√©s"
    else
        log_info "üßπ Conservation des jobs de migration (mode d√©veloppement)"
    fi
}

# Fonction pour configurer kubectl
configure_kubectl() {
    log_info "Configuration de kubectl pour AKS..."
    
    # R√©cup√©rer les credentials AKS
    az aks get-credentials --resource-group "$RESOURCE_GROUP" --name "$AKS_NAME" --overwrite-existing
    
    # V√©rifier la connexion
    kubectl cluster-info
    
    log_success "kubectl configur√© pour AKS"
}

# Fonction pour mettre √† jour les secrets Kubernetes
update_k8s_secrets() {
    log_info "Mise √† jour des secrets Kubernetes..."
    
    # Cr√©er le namespace s'il n'existe pas
    kubectl create namespace ibis-x --dry-run=client -o yaml | kubectl apply -f -
    
    # Les secrets seront cr√©√©s automatiquement par create_missing_secrets()
    # Cette fonction se contente de pr√©parer le namespace
    
    log_success "Namespace et pr√©paration des secrets termin√©s"
}

# ‚úÖ FONCTION AUTOMATIQUE : Configuration Frontend pour Production
configure_frontend_for_production() {
    log_info "üåê Configuration automatique du frontend pour la production..."
    
    if [[ "$ANGULAR_ENV" == "production" ]]; then
        log_info "üìù Mise √† jour FORC√âE des URLs frontend pour le domaine de production..."
        
        # Sauvegarder le fichier environment.prod.ts
        local env_file="$PROJECT_ROOT/frontend/src/environments/environment.prod.ts"
        local env_backup="$env_file.backup-$(date +%s)"
        
        if [[ -f "$env_file" ]]; then
            cp "$env_file" "$env_backup"
            log_info "üíæ Sauvegarde: $env_backup"
        fi
        
        # URLs de production FIXES
        local api_url="https://api.ibisx.fr"
        local frontend_domain="ibisx.fr"
        
        log_info "üéØ CONFIGURATION PRODUCTION FORC√âE:"
        log_info "   API URL: $api_url"
        log_info "   Frontend Domain: $frontend_domain"
        log_info "   Production Mode: TRUE"
        
        # Si PUBLIC_IP est disponible, on peut aussi le proposer comme fallback
        if [[ -n "$PUBLIC_IP" ]] && [[ "$PUBLIC_IP" != "N/A" ]]; then
            log_info "üì° IP publique statique: $PUBLIC_IP"
        fi
        
        # Cr√©er le fichier environment.prod.ts avec les bonnes URLs (FORC√â)
        cat > "$env_file" << EOF
export const environment = {
  production: true,
  // URL publique de l'API Gateway via l'Ingress Controller - toujours en HTTPS
  apiUrl: '$api_url',
  // Domaine de production pour le frontend
  productionDomain: '$frontend_domain'
};
EOF
        
        log_success "‚úÖ Frontend FORC√â en mode production:"
        log_success "   ‚úÖ production: true"
        log_success "   ‚úÖ API URL: $api_url"
        log_success "   ‚úÖ Domaine: $frontend_domain"
        log_success "   üìÑ Fichier: $env_file"
        
        # V√©rifier que le fichier est correct
        if grep -q "https://api.ibisx.fr" "$env_file" && grep -q "production: true" "$env_file"; then
            log_success "‚úÖ V√©rification OK - URLs de production confirm√©es"
        else
            log_error "‚ùå ERREUR: Configuration frontend incorrecte !"
            cat "$env_file"
            exit 1
        fi
    else
        log_info "üõ†Ô∏è Mode d√©veloppement - configuration frontend inchang√©e"
    fi
}

# Fonction INTELLIGENTE pour construire et pousser les images Docker
build_and_push_images() {
    log_info "üèóÔ∏è Construction et push des images Docker vers ACR..."
    log_info "üè∑Ô∏è Mode: $DEPLOYMENT_MODE | Tag: $IMAGE_TAG | Angular: $ANGULAR_ENV"
    
    # ‚úÖ NOUVEAU: Configurer le frontend avant le build
    configure_frontend_for_production
    
    # ‚úÖ FORCER REBUILD FRONTEND EN PRODUCTION: Supprimer l'image locale existante
    if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$ANGULAR_ENV" == "production" ]]; then
        log_info "üîÑ Suppression de l'image frontend existante pour forcer le rebuild en production..."
        docker rmi "$ACR_NAME.azurecr.io/frontend:latest" 2>/dev/null || true
        docker rmi "frontend:latest" 2>/dev/null || true
        log_info "‚úÖ Images frontend locales supprim√©es - rebuild forc√©"
    fi
    
    # Se connecter √† ACR
    az acr login --name "$ACR_NAME"
    
    cd "$PROJECT_ROOT"
    
    # Fonction helper ROBUSTE pour build/push avec gestion automatique des erreurs
    build_and_push_image() {
        local service_name="$1"
        local dockerfile_path="$2"
        local build_context="$3"
        local build_args="$4"
        
        log_info "üì¶ Construction de l'image $service_name..."
        
        # Tags selon l'environnement
        local base_image="$ACR_NAME.azurecr.io/$service_name"
        local tags_args=""
        
        if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
            # En production : tag avec SHA + latest
            tags_args="-t $base_image:$IMAGE_TAG -t $base_image:latest"
        else
            # En local : seulement latest
            tags_args="-t $base_image:latest"
        fi
        
        # üõ°Ô∏è FONCTION ROBUSTE : Essayer le build avec retry automatique
        local build_success=false
        local attempt=1
        local max_attempts=2
        
        while [[ $attempt -le $max_attempts ]] && [[ "$build_success" == "false" ]]; do
            log_info "üîÑ Tentative $attempt/$max_attempts pour $service_name..."
            
            # Premi√®re tentative : build normal
            # Deuxi√®me tentative : nettoyage cache + build --no-cache
            local docker_cmd=""
            if [[ $attempt -eq 1 ]]; then
                docker_cmd="docker build $tags_args $build_args -f $dockerfile_path $build_context"
            else
                log_info "üßπ Nettoyage automatique du cache Docker..."
                docker system prune -f > /dev/null 2>&1 || true
                docker_cmd="docker build --no-cache $tags_args $build_args -f $dockerfile_path $build_context"
                log_info "üîÑ Retry avec cache nettoy√© et --no-cache"
            fi
            
            # Ex√©cuter le build
            if eval "$docker_cmd"; then
                build_success=true
                log_success "‚úÖ Build $service_name r√©ussi (tentative $attempt/$max_attempts)"
            else
                log_error "‚ùå √âchec build $service_name (tentative $attempt/$max_attempts)"
                if [[ $attempt -eq $max_attempts ]]; then
                    log_error "üí• ERREUR CRITIQUE: Impossible de construire $service_name apr√®s $max_attempts tentatives"
                    log_error "üîß Suggestions:"
                    log_error "   1. V√©rifiez votre connexion Internet"
                    log_error "   2. V√©rifiez l'espace disque disponible"
                    log_error "   3. Red√©marrez Docker Desktop"
                    exit 1
                fi
                ((attempt++))
                sleep 2
            fi
        done
        
        # üöÄ Push des images vers ACR
        log_info "üì§ Push vers ACR..."
        if [[ "$DEPLOYMENT_MODE" == "production" ]]; then
            if docker push "$base_image:$IMAGE_TAG" && docker push "$base_image:latest"; then
                log_success "‚úÖ Image $service_name push√©e (tags: $IMAGE_TAG, latest)"
            else
                log_error "‚ùå √âchec push $service_name vers ACR"
                exit 1
            fi
        else
            if docker push "$base_image:latest"; then
                log_success "‚úÖ Image $service_name push√©e (tag: latest)"
            else
                log_error "‚ùå √âchec push $service_name vers ACR"
                exit 1
            fi
        fi
    }
    
    # 1. API Gateway
    build_and_push_image "ibis-x-api-gateway" "api-gateway/Dockerfile" "api-gateway/" ""
    
    # 2. Service Selection (contexte racine pour acc√©der aux modules communs)
    build_and_push_image "service-selection" "service-selection/Dockerfile" "." ""
    
    # 3. ML Pipeline (contexte racine pour acc√©der aux modules communs)
    build_and_push_image "ibis-x-ml-pipeline" "ml-pipeline-service/Dockerfile" "." ""
    
    # 4. Frontend (avec build args sp√©cifiques √† l'environnement)
    local frontend_build_args=""
    if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$ANGULAR_ENV" == "production" ]]; then
        frontend_build_args="--build-arg ANGULAR_ENV=production --no-cache"
        log_info "üåê Frontend: Build FORC√â en mode PRODUCTION (--no-cache)"
        log_info "üìù Variables d'environnement: ANGULAR_ENV=production"
        log_info "üîó API URL: https://api.ibisx.fr"
        log_info "üåê Domain: ibisx.fr"
    else
        frontend_build_args="--build-arg ANGULAR_ENV=development"
        log_info "üåê Frontend: Build en mode DEVELOPMENT"
    fi
    
    build_and_push_image "frontend" "frontend/Dockerfile" "frontend/" "$frontend_build_args"
    
    log_success "üöÄ Toutes les images Docker push√©es vers ACR : $ACR_NAME"
}

# Fonction pour forcer la mise √† jour des secrets storage APR√àS Kustomize
force_storage_secrets_update_after_kustomize() {
    log_info "üîÑ CORRECTION CRITIQUE : Forcer la mise √† jour des secrets storage avec les vraies valeurs Terraform..."
    
    # 1. S'assurer que nous avons les bonnes valeurs Terraform
    if [[ -z "$STORAGE_ACCOUNT" ]] || [[ -z "$STORAGE_KEY" ]]; then
        log_info "üìÇ R√©cup√©ration des valeurs storage depuis Terraform..."
        cd "$TERRAFORM_DIR"
        
        STORAGE_ACCOUNT=$(terraform output -raw storage_account_name 2>/dev/null || echo "")
        STORAGE_KEY=$(terraform output -raw storage_account_primary_key 2>/dev/null || echo "")
        
        cd "$PROJECT_ROOT"
        export STORAGE_ACCOUNT
        export STORAGE_KEY
    fi
    
    # 2. V√©rifier que nous avons les valeurs
    if [[ -z "$STORAGE_ACCOUNT" ]] || [[ -z "$STORAGE_KEY" ]]; then
        log_warning "‚ö†Ô∏è Impossible de r√©cup√©rer les valeurs Terraform pour le storage"
        return 1
    fi
    
    # 3. V√©rifier si le secret existe avec les mauvaises valeurs
    local current_storage_name=""
    if kubectl get secret storage-secrets -n "${K8S_NAMESPACE:-ibis-x}" &>/dev/null; then
        current_storage_name=$(kubectl get secret storage-secrets -n "${K8S_NAMESPACE:-ibis-x}" -o jsonpath='{.data.azure-storage-account-name}' | base64 -d 2>/dev/null || echo "")
    fi
    
    log_info "üîç V√©rification des secrets storage..."
    log_info "  üíæ Storage Terraform : $STORAGE_ACCOUNT"
    log_info "  üîç Storage Kubernetes : ${current_storage_name:-VIDE}"
    
    # 4. Si les valeurs sont diff√©rentes, forcer la correction
    if [[ "$current_storage_name" != "$STORAGE_ACCOUNT" ]]; then
        log_warning "‚ùå PROBL√àME D√âTECT√â : Kubernetes utilise un storage account incorrect !"
        log_warning "   Attendu (Terraform) : $STORAGE_ACCOUNT"
        log_warning "   Actuel (Kubernetes) : ${current_storage_name:-VIDE}"
        
        log_info "üîß CORRECTION AUTOMATIQUE en cours..."
        
        # Supprimer et recr√©er le secret avec les bonnes valeurs
        kubectl delete secret storage-secrets -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        sleep 2
        
        log_info "üóÇÔ∏è Recr√©ation storage-secrets avec les vraies valeurs Terraform: $STORAGE_ACCOUNT"
        
        # Construire la connection string Azure
        local azure_connection_string="DefaultEndpointsProtocol=https;AccountName=${STORAGE_ACCOUNT};AccountKey=${STORAGE_KEY};EndpointSuffix=core.windows.net"
        
        kubectl create secret generic storage-secrets -n "${K8S_NAMESPACE:-ibis-x}" \
            --from-literal=azure-storage-account-name="$STORAGE_ACCOUNT" \
            --from-literal=azure-storage-account-key="$STORAGE_KEY" \
            --from-literal=azure-connection-string="$azure_connection_string" \
            --from-literal=azure-container-name="ibis-x-datasets" \
            --from-literal=access-key="minioadmin" \
            --from-literal=minio-access-key="minioadmin" \
            --from-literal=minio-bucket-name="exai-datasets" \
            --from-literal=minio-endpoint="http://minio-service.exai.svc.cluster.local:9000" \
            --from-literal=minio-secret-key="minioadmin" \
            --from-literal=secret-key="minioadmin" || {
            log_error "‚ùå √âchec de cr√©ation du secret storage-secrets"
            return 1
        }
        
        # 5. Forcer le red√©marrage des pods qui utilisent ce secret
        log_info "üîÑ Red√©marrage FORC√â des pods pour utiliser le nouveau secret..."
        kubectl rollout restart deployment/service-selection -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        
        # Attendre que le rollout soit termin√©
        kubectl rollout status deployment/service-selection -n "${K8S_NAMESPACE:-ibis-x}" --timeout=120s 2>/dev/null || true
        
        log_success "‚úÖ SECRET STORAGE CORRIG√â : $STORAGE_ACCOUNT"
        log_success "‚úÖ Pods red√©marr√©s pour utiliser le nouveau secret"
    else
        log_success "‚úÖ Secret storage d√©j√† correct : $STORAGE_ACCOUNT"
    fi
}

# Fonction pour d√©ployer l'application sur Kubernetes
deploy_application() {
    log_info "D√©ploiement de l'application IBIS-X sur AKS..."
    
    cd "$PROJECT_ROOT"
    
    # Les placeholders ACR ont d√©j√† √©t√© remplac√©s lors de get_terraform_outputs
    
    # V√©rifier les noms de projet
    verify_project_names
    
    # Installer les composants Kubernetes n√©cessaires
    install_nginx_ingress
    install_cert_manager
    
    # Attendre que les contr√¥leurs soient pr√™ts
    log_info "Attente que NGINX Ingress soit pr√™t..."
    kubectl wait --namespace ingress-nginx \
        --for=condition=ready pod \
        --selector=app.kubernetes.io/component=controller \
        --timeout=300s
    
    log_info "Attente que Cert-Manager soit pr√™t..."
    kubectl wait --namespace cert-manager \
        --for=condition=ready pod \
        --selector=app.kubernetes.io/name=cert-manager \
        --timeout=300s
    
    # Les placeholders ACR ont d√©j√† √©t√© remplac√©s
    
    # üéØ D√âPLOIEMENT UNIFI√â : Toujours utiliser Kustomize avec overlay Azure
    # Applique automatiquement TOUS les patches Azure (stockage, auto-init, etc.)
    log_info "üöÄ D√©ploiement unifi√© avec Kustomize Azure (Windows/Linux/MacOS)..."
    
    # üßπ NETTOYER LES JOBS EXISTANTS AVANT D√âPLOIEMENT (Fix: Jobs immutables)
    log_info "üßπ Nettoyage pr√©ventif des jobs de migration existants..."
    kubectl delete job api-gateway-migration-job service-selection-migration-job ml-pipeline-migration-job -n "${K8S_NAMESPACE:-ibis-x}" --ignore-not-found=true 2>/dev/null || true
    log_success "‚úÖ Jobs existants nettoy√©s (√©vite l'erreur 'field is immutable')"
    
    # Sauvegarder le r√©pertoire courant
    ORIGINAL_DIR=$(pwd)
    
    # Utiliser TOUJOURS l'overlay Azure qui applique automatiquement :
    # ‚úÖ Configuration stockage Azure (au lieu de MinIO)
    # ‚úÖ Auto-initialisation datasets forc√©e 
    # ‚úÖ Variables d'environnement production
    # ‚úÖ Images ACR correctes
    # ‚úÖ Tous les patches sp√©cifiques √† Azure
    
    log_info "üì¶ Application overlay Azure avec tous les patches..."
    
    cd "$K8S_DIR/overlays/azure/" || {
        log_error "‚ùå Impossible d'acc√©der au r√©pertoire Azure overlay"
        exit 1
    }
    
    if kubectl apply -k . ; then
        cd "$ORIGINAL_DIR"
        log_success "‚úÖ D√©ploiement Kustomize Azure r√©ussi - TOUS les patches appliqu√©s automatiquement"
        log_info "‚úÖ Patches Azure appliqu√©s :"
        log_info "  üóÇÔ∏è  Stockage: Azure Blob Storage (PAS MinIO)"
        log_info "  üîÑ Auto-init: FORCE_INIT_DATA=true + AUTO_INIT_DATA=true"
        log_info "  üê≥ Images: ACR $ACR_NAME"
        log_info "  üéØ Mode: Production avec WITH_DATA=true"
        
        # üîÑ FORCER LE RED√âMARRAGE pour utiliser les nouvelles images push√©es
        log_info "üîÑ Nettoyage des pods bloqu√©s et red√©marrage automatique..."
        
        # Supprimer les pods bloqu√©s avec ErrImageNeverPull ou ErrImagePull
        kubectl delete pods -n "${K8S_NAMESPACE:-ibis-x}" --field-selector=status.phase=Pending 2>/dev/null || true
        kubectl delete pods -n "${K8S_NAMESPACE:-ibis-x}" --field-selector=status.phase=Failed 2>/dev/null || true
        
        # Red√©marrer tous les deployments pour forcer l'utilisation des nouvelles images
        kubectl rollout restart deployment/service-selection -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment/api-gateway -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment/ml-pipeline -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment/frontend -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        kubectl rollout restart deployment/ml-pipeline-celery-worker -n "${K8S_NAMESPACE:-ibis-x}" 2>/dev/null || true
        
        log_success "‚úÖ Nettoyage et red√©marrage automatique termin√©s - nouvelles images utilis√©es avec ImagePullPolicy Always"
        
        # üîÑ CORRECTION CRITIQUE : Forcer la mise √† jour des secrets storage APR√àS Kustomize
        force_storage_secrets_update_after_kustomize
        
    else
        log_error "‚ùå √âchec du d√©ploiement Kustomize Azure"
        cd "$ORIGINAL_DIR"
        exit 1
    fi
}

# Fonction pour d√©ployer les applications avec les bonnes images automatiquement
deploy_app_with_correct_images() {
    log_info "D√©ploiement automatique des applications avec images corrig√©es..."
    
    # Cr√©er fichiers temporaires avec les bonnes images (m√©thode robuste)
    local TEMP_DIR=""
    if [[ "$IS_WINDOWS" == true ]]; then
        TEMP_DIR="$TEMP/ibis-x-deploy-$$"
    else
        TEMP_DIR="/tmp/ibis-x-deploy-$$"
    fi
    mkdir -p "$TEMP_DIR"
    
    # 1. API Gateway
    log_info "D√©ploiement API Gateway avec image ACR $ACR_NAME..."
    sed "s|image: ibis-x-api-gateway|image: $ACR_NAME.azurecr.io/ibis-x-api-gateway:latest|" "$K8S_DIR/base/api-gateway/deployment.yaml" > "$TEMP_DIR/api-gateway-deployment.yaml"
    kubectl apply -f "$TEMP_DIR/api-gateway-deployment.yaml"
    kubectl apply -f "$K8S_DIR/base/api-gateway/service.yaml" 2>/dev/null || true
    
    # 2. Service Selection  
    log_info "D√©ploiement Service Selection avec image ACR $ACR_NAME..."
    sed "s|image: service-selection|image: $ACR_NAME.azurecr.io/service-selection:latest|" "$K8S_DIR/base/service-selection/deployment.yaml" > "$TEMP_DIR/service-selection-deployment.yaml"
    kubectl apply -f "$TEMP_DIR/service-selection-deployment.yaml"
    kubectl apply -f "$K8S_DIR/base/service-selection/service.yaml" 2>/dev/null || true
    
    # 3. ML Pipeline
    log_info "D√©ploiement ML Pipeline avec image ACR $ACR_NAME..."
    sed "s|image: ml-pipeline|image: $ACR_NAME.azurecr.io/ml-pipeline:latest|" "$K8S_DIR/base/ml-pipeline/deployment.yaml" > "$TEMP_DIR/ml-pipeline-deployment.yaml"
    kubectl apply -f "$TEMP_DIR/ml-pipeline-deployment.yaml"
    kubectl apply -f "$K8S_DIR/base/ml-pipeline/service.yaml" 2>/dev/null || true
    kubectl apply -f "$K8S_DIR/base/ml-pipeline/secrets.yaml" 2>/dev/null || true
    kubectl apply -f "$K8S_DIR/base/ml-pipeline/alembic-config.yaml" 2>/dev/null || true
    kubectl apply -f "$K8S_DIR/base/ml-pipeline/celery-worker-deployment.yaml" 2>/dev/null || true
    
    # 4. Frontend
    log_info "D√©ploiement Frontend avec image ACR $ACR_NAME..."
    sed "s|image: frontend|image: $ACR_NAME.azurecr.io/frontend:latest|" "$K8S_DIR/base/frontend/deployment.yaml" > "$TEMP_DIR/frontend-deployment.yaml"
    kubectl apply -f "$TEMP_DIR/frontend-deployment.yaml"
    kubectl apply -f "$K8S_DIR/base/frontend/service.yaml" 2>/dev/null || true
    
    # 4. Secrets n√©cessaires (cr√©√©s automatiquement)
    log_info "Cr√©ation automatique des secrets manquants..."
    kubectl apply -f "$K8S_DIR/base/api-gateway/gateway-secrets.yaml" 2>/dev/null || true
    kubectl apply -f "$K8S_DIR/base/service-selection/db-secrets.yaml" 2>/dev/null || true
    
    # 5. Red√©marrage forc√© pour prendre en compte les nouvelles images et secrets
    log_info "Red√©marrage automatique des pods avec nouvelles images ACR..."
    kubectl delete pods -l app=api-gateway -n ibis-x --ignore-not-found=true 2>/dev/null || true
    kubectl delete pods -l app=service-selection -n ibis-x --ignore-not-found=true 2>/dev/null || true
    kubectl delete pods -l app=ml-pipeline -n ibis-x --ignore-not-found=true 2>/dev/null || true
    kubectl delete pods -l app=ml-pipeline-celery-worker -n ibis-x --ignore-not-found=true 2>/dev/null || true
    kubectl delete pods -l app=frontend -n ibis-x --ignore-not-found=true 2>/dev/null || true
    
    # 6. Mise √† jour automatique des images vers ACR  
    log_info "Mise √† jour automatique des images vers ACR..."
    kubectl set image deployment/api-gateway api-gateway=$ACR_NAME.azurecr.io/ibis-x-api-gateway:latest -n ibis-x 2>/dev/null || true
    kubectl set image deployment/service-selection service-selection=$ACR_NAME.azurecr.io/service-selection:latest -n ibis-x 2>/dev/null || true
    kubectl set image deployment/ml-pipeline ml-pipeline=$ACR_NAME.azurecr.io/ml-pipeline:latest -n ibis-x 2>/dev/null || true
    kubectl set image deployment/ml-pipeline-celery-worker ml-pipeline-worker=$ACR_NAME.azurecr.io/ml-pipeline:latest -n ibis-x 2>/dev/null || true
    kubectl set image deployment/frontend frontend=$ACR_NAME.azurecr.io/frontend:latest -n ibis-x 2>/dev/null || true
    
    # 7. Ingress et certificats SSL automatiques
    log_info "D√©ploiement de l'ingress et g√©n√©ration automatique des certificats SSL..."
    kubectl apply -f "$K8S_DIR/base/common/letsencrypt-prod-issuer.yaml" 2>/dev/null || true
    kubectl apply -f "$K8S_DIR/base/common/ingress.yaml" 2>/dev/null || true
    
    # Nettoyer les fichiers temporaires
    rm -rf "$TEMP_DIR" 2>/dev/null || true
    
    log_success "‚úÖ D√âPLOIEMENT 100% AUTOMATIQUE R√âUSSI ! Applications en ligne avec images ACR, secrets, et SSL !"
}

# ‚úÖ FONCTION AUTOMATIQUE : Forcer Rebuild et Red√©ploiement Frontend Production
force_frontend_production_rebuild() {
    log_info "üöÄ Reconstruction automatique du frontend en mode production..."
    
    # 1. Configurer l'environnement de production
    configure_frontend_for_production
    
    # 2. Supprimer les images existantes pour forcer le rebuild
    log_info "üßπ Suppression des images frontend existantes..."
    docker rmi "$ACR_NAME.azurecr.io/frontend:latest" 2>/dev/null || true
    docker rmi "$ACR_NAME.azurecr.io/frontend:prod-fix" 2>/dev/null || true
    docker rmi "frontend:latest" 2>/dev/null || true
    
    # 3. Rebuild avec les nouvelles variables d'environnement
    cd "$PROJECT_ROOT/frontend"
    log_info "üèóÔ∏è Construction frontend avec ANGULAR_ENV=production..."
    log_info "üìù API URL configur√©e: https://api.ibisx.fr"
    log_info "üåê Domain configur√©: ibisx.fr"
    
    # Build avec --no-cache pour √™tre s√ªr que les changements sont pris en compte
    docker build \
        --build-arg ANGULAR_ENV=production \
        --no-cache \
        -t "$ACR_NAME.azurecr.io/frontend:prod-fixed" \
        -t "$ACR_NAME.azurecr.io/frontend:latest" .
    
    # 4. Push vers ACR
    log_info "üì§ Push vers ACR..."
    az acr login --name "$ACR_NAME"
    docker push "$ACR_NAME.azurecr.io/frontend:prod-fixed"
    docker push "$ACR_NAME.azurecr.io/frontend:latest"
    
    # 5. Red√©ployer le frontend avec la nouvelle image
    log_info "üîÑ Red√©ploiement automatique du frontend..."
    kubectl set image deployment/frontend frontend="$ACR_NAME.azurecr.io/frontend:prod-fixed" -n ibis-x
    kubectl rollout restart deployment/frontend -n ibis-x
    
    # 6. Attendre que le rollout soit termin√©
    log_info "‚è≥ Attente du rollout frontend..."
    kubectl rollout status deployment/frontend -n ibis-x --timeout=300s
    
    cd "$PROJECT_ROOT"
    log_success "‚úÖ Frontend reconstruit et red√©ploy√© en mode production !"
}

# Fonction SIMPLE et EFFICACE pour remplacer les placeholders ACR
update_all_acr_references() {
    log_info "üîç Remplacement des placeholders ACR : PLACEHOLDER_ACR ‚Üí $ACR_NAME"
    
    local updated_files=0
    
    # Fichiers critiques √† mettre √† jour
    local critical_files=(
        "$K8S_DIR/base/jobs/api-gateway-migration-job.yaml"
        "$K8S_DIR/base/jobs/service-selection-migration-job.yaml"
        "$K8S_DIR/overlays/azure/kustomization.yaml"
    )
    
    # Fonction simple pour remplacer PLACEHOLDER_ACR dans un fichier
    replace_placeholder_in_file() {
        local file_path="$1"
        local file_name=$(basename "$file_path")
        
        if [[ ! -f "$file_path" ]]; then
            log_warning "‚ö†Ô∏è Fichier introuvable: $file_path"
            return 1
        fi
        
        # V√©rifier si le fichier contient des placeholders
        if ! grep -q "PLACEHOLDER_ACR" "$file_path" 2>/dev/null; then
            log_info "‚úÖ $file_name - aucun placeholder √† remplacer"
            return 0
        fi
        
        log_info "üîß Remplacement des placeholders dans $file_name..."
        
        # Remplacement simple et direct
        if sed -i "s|PLACEHOLDER_ACR|$ACR_NAME|g" "$file_path" 2>/dev/null; then
            # V√©rifier que le remplacement a fonctionn√©
            if grep -q "$ACR_NAME\.azurecr\.io" "$file_path" 2>/dev/null; then
                log_success "‚úÖ $file_name mis √† jour avec $ACR_NAME"
                return 0
            else
                log_error "‚ùå √âchec de v√©rification pour $file_name"
                return 1
            fi
        else
            log_error "‚ùå √âchec du remplacement pour $file_name"
            return 1
        fi
    }
    
    # Remplacer les placeholders dans chaque fichier critique
    log_info "üéØ Remplacement des placeholders dans tous les fichiers..."
    
    for file in "${critical_files[@]}"; do
        if replace_placeholder_in_file "$file"; then
            updated_files=$((updated_files + 1))
        fi
    done
    
    log_success "üéØ Remplacement termin√© - $updated_files fichier(s) trait√©s avec ACR : $ACR_NAME"
    
    return 0
}

# Fonction INTELLIGENTE pour g√©rer automatiquement les jobs de migration avec les bonnes images ACR
fix_migration_jobs() {
    log_info "üîç V√âRIFICATION INTELLIGENTE des jobs de migration avec ACR $ACR_NAME..."
    
    # Les fichiers ont d√©j√† √©t√© corrig√©s avec les placeholders ACR
    
    # V√©rifier les jobs existants et leur statut
    local api_job_exists=$(kubectl get job api-gateway-migration-job -n ibis-x 2>/dev/null && echo "true" || echo "false")
    local service_job_exists=$(kubectl get job service-selection-migration-job -n ibis-x 2>/dev/null && echo "true" || echo "false")
    
    local api_complete="False"
    local service_complete="False"
    
    if [[ "$api_job_exists" == "true" ]]; then
        api_complete=$(kubectl get job api-gateway-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
    fi
    
    if [[ "$service_job_exists" == "true" ]]; then
        service_complete=$(kubectl get job service-selection-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
    fi
    
    # Si les jobs existent et sont termin√©s avec succ√®s, ne pas les recr√©er
    if [[ "$api_complete" == "True" ]] && [[ "$service_complete" == "True" ]]; then
        log_success "‚úÖ Migrations d√©j√† termin√©es avec succ√®s - aucune action n√©cessaire"
        return 0
    fi
    
    # V√©rifier s'il y a des jobs d√©faillants (ImagePullBackOff ou Failed)
    local failed_migration_pods=$(kubectl get pods -n ibis-x -o jsonpath='{.items[?(@.metadata.name=~".*migration.*")].metadata.name}' 2>/dev/null || echo "")
    local has_failed_jobs=false
    
    if [[ -n "$failed_migration_pods" ]]; then
        for pod in $failed_migration_pods; do
            local pod_status=$(kubectl get pod "$pod" -n ibis-x -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
            local container_status=$(kubectl get pod "$pod" -n ibis-x -o jsonpath='{.status.containerStatuses[0].state.waiting.reason}' 2>/dev/null || echo "")
            
            if [[ "$pod_status" == "Failed" ]] || [[ "$container_status" == "ImagePullBackOff" ]]; then
                has_failed_jobs=true
                log_warning "‚ùå Job d√©faillant d√©tect√©: $pod (Status: $pod_status, Reason: $container_status)"
            fi
        done
    fi
    
    # Si jobs d√©faillants OU jobs non existants, les recr√©er
    if [[ "$has_failed_jobs" == true ]] || [[ "$api_job_exists" == "false" ]] || [[ "$service_job_exists" == "false" ]]; then
        log_info "üßπ NETTOYAGE et RECR√âATION des jobs de migration..."
        
        # Supprimer tous les jobs de migration existants
        kubectl delete job api-gateway-migration-job service-selection-migration-job -n ibis-x 2>/dev/null || true
        sleep 3
        
        # Cr√©er les nouveaux jobs avec les fichiers corrig√©s
        log_info "üöÄ Cr√©ation des jobs de migration avec ACR corrig√© $ACR_NAME..."
        kubectl apply -f "$K8S_DIR/base/jobs/api-gateway-migration-job.yaml" || {
            log_error "‚ùå √âchec cr√©ation job API Gateway"
            return 1
        }
        kubectl apply -f "$K8S_DIR/base/jobs/service-selection-migration-job.yaml" || {
            log_error "‚ùå √âchec cr√©ation job Service Selection"
            return 1
        }
        
        log_success "‚úÖ Jobs de migration cr√©√©s avec les bonnes images ACR"
    fi
    
    # Attendre et v√©rifier la compl√©tion avec feedback am√©lior√©
    log_info "‚è≥ Attente de la compl√©tion des migrations (timeout: 5min)..."
    
    local timeout=300
    local elapsed=0
    local check_interval=15
    
    while [[ $elapsed -lt $timeout ]]; do
        # Nettoyer les sorties kubectl pour √©viter les caract√®res parasites
        api_complete=$(kubectl get job api-gateway-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        service_complete=$(kubectl get job service-selection-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        
        local api_failed=$(kubectl get job api-gateway-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        local service_failed=$(kubectl get job service-selection-migration-job -n ibis-x -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null | tr -d '\r\n' || echo "False")
        
        if [[ "$api_complete" == "True" ]] && [[ "$service_complete" == "True" ]]; then
            log_success "‚úÖ TOUTES LES MIGRATIONS TERMIN√âES AVEC SUCC√àS !"
            return 0
        fi
        
        if [[ "$api_failed" == "True" ]] || [[ "$service_failed" == "True" ]]; then
            log_error "‚ùå √âchec de migration d√©tect√© - Affichage des logs..."
            kubectl get jobs -n ibis-x 2>/dev/null || true
            kubectl get pods -n ibis-x | grep migration 2>/dev/null || true
            return 1
        fi
        
        log_info "‚è≥ Migrations en cours... API: $api_complete, Service: $service_complete (${elapsed}s/${timeout}s)"
        sleep $check_interval
        elapsed=$((elapsed + check_interval))
    done
    
    # Si on arrive ici, il y a eu un timeout
    log_warning "‚ö†Ô∏è TIMEOUT atteint - V√©rification du statut final..."
    kubectl get jobs -n ibis-x 2>/dev/null || true
    kubectl get pods -n ibis-x | grep migration 2>/dev/null || true
    
    log_success "üéØ Gestion automatique des jobs de migration termin√©e (avec timeout)"
    return 1
}

# ‚úÖ FONCTION AUTOMATIQUE : V√©rification et correction IP statique NGINX
verify_and_fix_nginx_static_ip() {
    log_info "üîç V√âRIFICATION AUTOMATIQUE : IP statique NGINX..."
    
    # R√©cup√©rer l'IP statique configur√©e
    local expected_static_ip="$STATIC_IP_CONFIRMED"
    if [[ -z "$expected_static_ip" ]]; then
        # Fallback : r√©cup√©rer depuis Azure
        local node_resource_group=$(az group list --query "[?contains(name, 'MC_${RESOURCE_GROUP}_${AKS_NAME}')].name" --output tsv 2>/dev/null | head -1)
        if [[ -n "$node_resource_group" ]]; then
            expected_static_ip=$(az network public-ip show --resource-group "$node_resource_group" --name "ibis-x-prod-ingress-ip" --query "ipAddress" --output tsv 2>/dev/null || echo "")
        fi
    fi
    
    if [[ -z "$expected_static_ip" ]]; then
        log_warning "‚ö†Ô∏è Impossible de d√©terminer l'IP statique attendue"
        return 0
    fi
    
    # V√©rifier l'IP actuelle de NGINX
    local nginx_current_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
    
    log_info "üéØ IP statique attendue : $expected_static_ip"
    log_info "üîç IP actuelle NGINX : ${nginx_current_ip:-AUCUNE}"
    
    if [[ "$nginx_current_ip" == "$expected_static_ip" ]]; then
        log_success "‚úÖ NGINX utilise l'IP statique correcte : $nginx_current_ip"
        
        # V√©rifier aussi que l'ingress application utilise la bonne IP
        local ingress_ip=$(kubectl get ingress -n ibis-x ibis-x-ingress -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
        if [[ "$ingress_ip" == "$expected_static_ip" ]]; then
            log_success "‚úÖ Ingress application utilise l'IP statique correcte : $ingress_ip"
        else
            log_warning "‚ö†Ô∏è Ingress application : IP diff√©rente ou en attente (${ingress_ip:-PENDING})"
        fi
    else
        log_error "‚ùå PROBL√àME D√âTECT√â : NGINX utilise une IP incorrecte !"
        log_error "   IP attendue : $expected_static_ip"
        log_error "   IP actuelle : ${nginx_current_ip:-AUCUNE}"
        
        log_info "üîß CORRECTION AUTOMATIQUE en cours..."
        
        # Forcer la correction
        kubectl patch svc ingress-nginx-controller -n ingress-nginx -p "{\"spec\":{\"loadBalancerIP\":\"$expected_static_ip\"}}" 2>/dev/null || true
        
        # Attendre et v√©rifier √† nouveau
        sleep 30
        nginx_current_ip=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
        
        if [[ "$nginx_current_ip" == "$expected_static_ip" ]]; then
            log_success "‚úÖ CORRECTION R√âUSSIE : NGINX utilise maintenant l'IP statique : $nginx_current_ip"
        else
            log_warning "‚ö†Ô∏è CORRECTION PARTIELLE : Recr√©ation de NGINX recommand√©e"
            log_warning "   Lancez √† nouveau le script pour forcer la recr√©ation"
        fi
    fi
}

# Fonction de v√©rification finale et auto-correction RENFORC√âE
final_auto_check_and_fix() {
    log_info "üîç V√©rification finale et auto-correction RENFORC√âE..."
    
    # 1. Les ACR ont d√©j√† √©t√© corrig√©s automatiquement
    
    # 2. V√©rifier et corriger automatiquement les jobs de migration
    log_info "üîß Auto-correction des jobs de migration..."
    fix_migration_jobs
    
    # 3. ‚úÖ NOUVEAU : V√©rification et correction automatique IP statique NGINX
    log_info "üéØ V√©rification automatique de l'IP statique NGINX..."
    verify_and_fix_nginx_static_ip
    
    # 4. V√©rification finale de l'√©tat global
    log_info "üìä √âtat final du d√©ploiement :"
    echo "======================="
    kubectl get pods -n ibis-x 2>/dev/null || true
    echo "======================="
    kubectl get jobs -n ibis-x 2>/dev/null || true
    echo "======================="
    kubectl get ingress -n ibis-x 2>/dev/null || true
    echo "======================="
    kubectl get svc -n ingress-nginx ingress-nginx-controller 2>/dev/null || true
    echo "======================="
    
    log_success "‚úÖ V√©rification finale et auto-correction RENFORC√âES termin√©es"
}

# Fonction pour attendre que les pods soient pr√™ts  
wait_for_pods() {
    # Attendre que les pods soient pr√™ts
    log_info "Attente du d√©marrage des pods..."
    kubectl wait --for=condition=ready pod -l app=api-gateway -n ibis-x --timeout=300s
    kubectl wait --for=condition=ready pod -l app=service-selection -n ibis-x --timeout=300s
    kubectl wait --for=condition=ready pod -l app=frontend -n ibis-x --timeout=300s
    
    log_success "Application IBIS-X d√©ploy√©e sur AKS"
}

# Fonction pour g√©rer les secrets selon le mode de d√©ploiement
create_missing_secrets() {
    log_info "üîê Gestion des secrets - Mode: $DEPLOYMENT_MODE"
    
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        # üè≠ MODE GITHUB ACTIONS: Secrets inject√©s dans les fichiers YAML
        log_info "üè≠ Application des secrets depuis les fichiers GitHub Actions..."
        create_github_secrets
    else
        # üõ†Ô∏è MODE MANUEL: Configuration manuelle des secrets
        log_info "üõ†Ô∏è Configuration manuelle des secrets..."
        create_manual_secrets
    fi
    
    log_success "‚úÖ Gestion des secrets termin√©e"
}

# Fonction pour les secrets GitHub Actions
create_github_secrets() {
    log_info "üìã Application des secrets GitHub Actions..."
    
    # GitHub Actions a d√©j√† modifi√© les fichiers YAML avec les vrais secrets
    # Il suffit d'appliquer les fichiers secrets
    
    # 1. Supprimer les anciens secrets (pour forcer la mise √† jour)
    log_info "üßπ Suppression des anciens secrets..."
    kubectl delete secret gateway-secrets -n "$K8S_NAMESPACE" --ignore-not-found=true
    kubectl delete secret kaggle-secrets -n "$K8S_NAMESPACE" --ignore-not-found=true
    kubectl delete secret db-secrets -n "$K8S_NAMESPACE" --ignore-not-found=true
    
    # 2. Appliquer les secrets depuis les fichiers (modifi√©s par GitHub Actions)
    log_info "üìÑ Application des secrets depuis les fichiers YAML..."
    kubectl apply -f k8s/base/api-gateway/gateway-secrets.yaml
    kubectl apply -f k8s/base/service-selection/kaggle-secrets.yaml
    kubectl apply -f k8s/base/service-selection/db-secrets.yaml
    
    # 3. Storage secrets (r√©cup√©rer depuis Azure)
    create_storage_secrets_from_azure
    
    log_success "‚úÖ Secrets GitHub Actions appliqu√©s"
}

# Fonction pour les secrets manuels (production sans GitHub Actions)
create_manual_secrets() {
    log_info "üõ†Ô∏è Configuration automatique des secrets de base pour la production..."
    
    # En mode production script, on cr√©e des secrets de base fonctionnels
    log_info "üìã Cr√©ation des secrets de base pour permettre le fonctionnement de l'application"
    
    # 1. Cr√©er les secrets Kaggle (fonctionnels par d√©faut)
    if ! kubectl get secret kaggle-secrets -n "$K8S_NAMESPACE" &>/dev/null; then
        log_info "üîë Cr√©ation kaggle-secrets (configuration de base)..."
        kubectl create secret generic kaggle-secrets -n "$K8S_NAMESPACE" \
            --from-literal=username=default-kaggle-user \
            --from-literal=key=default-kaggle-key
        log_info "‚úÖ Kaggle secrets cr√©√©s (fonctionnels pour les tests)"
    fi
    
    # 2. Storage secrets depuis Azure
    create_storage_secrets_from_azure
    
    # 3. Gateway secrets (fonctionnels par d√©faut)
    if ! kubectl get secret gateway-secrets -n "$K8S_NAMESPACE" &>/dev/null; then
        log_info "üîë Cr√©ation gateway-secrets (configuration de base)..."
        # G√©n√©rer une cl√© JWT basique pour les tests
        local jwt_secret=$(openssl rand -base64 32 2>/dev/null || echo "default-jwt-secret-key-for-development-only")
        local db_url="postgresql://postgres:postgres@postgresql-service:5432/ibisxdb"
        
        kubectl create secret generic gateway-secrets -n "$K8S_NAMESPACE" \
            --from-literal=secret-key="$jwt_secret" \
            --from-literal=database-url="$db_url" \
            --from-literal=google-client-id=default-google-client-id \
            --from-literal=google-client-secret=default-google-client-secret \
            --from-literal=oauth-redirect-url=https://ibisx.fr/oauth/callback
        log_info "‚úÖ Gateway secrets cr√©√©s (fonctionnels pour les tests)"
    fi
    
    # 4. DB secrets (fonctionnels par d√©faut)
    if ! kubectl get secret db-secrets -n "$K8S_NAMESPACE" &>/dev/null; then
        log_info "üîë Cr√©ation db-secrets (configuration de base)..."
        local db_url="postgresql://postgres:postgres@postgresql-service:5432/ibisxdb"
        kubectl create secret generic db-secrets -n "$K8S_NAMESPACE" \
            --from-literal=database-url="$db_url"
        log_info "‚úÖ DB secrets cr√©√©s (fonctionnels pour les tests)"
    fi
    
    log_success "‚úÖ Secrets de base cr√©√©s et fonctionnels"
}

# Fonction pour cr√©er les secrets de stockage Azure 
create_storage_secrets_from_azure() {
    log_info "‚òÅÔ∏è R√©cup√©ration des secrets de stockage Azure..."
    
    # Toujours privil√©gier les valeurs Terraform si disponibles
    if [[ -z "$STORAGE_ACCOUNT" ]] || [[ -z "$STORAGE_KEY" ]]; then
        log_info "üìÇ R√©cup√©ration storage depuis Azure CLI..."
        # Filtrer pour r√©cup√©rer sp√©cifiquement le storage account IBIS-X (pattern: ibisxprodstg*)
        STORAGE_ACCOUNT=$(az storage account list --resource-group "$RESOURCE_GROUP" --query "[?starts_with(name, 'ibisxprodstg')].name | [0]" -o tsv 2>/dev/null || echo "")
        if [[ -n "$STORAGE_ACCOUNT" ]]; then
            STORAGE_KEY=$(az storage account keys list --resource-group "$RESOURCE_GROUP" --account-name "$STORAGE_ACCOUNT" --query "[0].value" -o tsv 2>/dev/null || echo "")
        fi
    fi
    
    # S√âCURIT√â : Corriger automatiquement si on d√©tecte l'ancien storage account
    if [[ "$STORAGE_ACCOUNT" == "ibisxprodstg2205" ]]; then
        log_warning "‚ö†Ô∏è Ancien storage account d√©tect√© ($STORAGE_ACCOUNT), correction automatique..."
        STORAGE_ACCOUNT="ibisxprodstg6630"
        STORAGE_KEY=$(cd terraform/azure-infrastructure && terraform output -raw storage_account_primary_key 2>/dev/null || echo "")
        log_info "‚úÖ Correction : STORAGE_ACCOUNT=$STORAGE_ACCOUNT"
    fi
    
    # Cr√©er le secret storage si on a les valeurs
    if [[ -n "$STORAGE_ACCOUNT" ]] && [[ -n "$STORAGE_KEY" ]]; then
        kubectl delete secret storage-secrets -n "$K8S_NAMESPACE" 2>/dev/null || true
        
        log_info "üóÇÔ∏è Cr√©ation storage-secrets avec valeurs Azure: $STORAGE_ACCOUNT"
        
        # Construire la connection string Azure
        local azure_connection_string="DefaultEndpointsProtocol=https;AccountName=${STORAGE_ACCOUNT};AccountKey=${STORAGE_KEY};EndpointSuffix=core.windows.net"
        
        kubectl create secret generic storage-secrets -n "$K8S_NAMESPACE" \
            --from-literal=azure-storage-account-name="$STORAGE_ACCOUNT" \
            --from-literal=azure-storage-account-key="$STORAGE_KEY" \
            --from-literal=azure-connection-string="$azure_connection_string" \
            --from-literal=azure-container-name=ibis-x-datasets
        
        log_success "‚úÖ Storage secrets cr√©√©s: $STORAGE_ACCOUNT"
    else
        log_warning "‚ö†Ô∏è Impossible de r√©cup√©rer les secrets de stockage Azure"
        log_warning "üõ†Ô∏è Vous devrez configurer storage-secrets manuellement si n√©cessaire"
    fi
}

# Fonction pour v√©rifier et red√©marrer les pods en erreur
fix_failed_pods() {
    log_info "V√©rification et correction automatique des pods en erreur..."
    
    # Attendre un peu que les pods d√©marrent
    sleep 30
    
    # V√©rifier les pods qui ne sont pas pr√™ts
    local failed_pods=""
    if kubectl get namespace ibis-x &>/dev/null; then
        failed_pods=$(kubectl get pods -n ibis-x --field-selector=status.phase!=Running -o name 2>/dev/null || echo "")
    fi
    
    if [[ -n "$failed_pods" ]]; then
        log_warning "Pods en erreur d√©tect√©s, red√©marrage automatique..."
        
        # Red√©marrer les pods en erreur
        kubectl delete pod -n ibis-x -l app=service-selection --ignore-not-found=true
        kubectl delete pod -n ibis-x -l app=api-gateway --ignore-not-found=true
        kubectl delete pod -n ibis-x -l app=frontend --ignore-not-found=true
        
        # Attendre que les nouveaux pods d√©marrent
        log_info "Attente du red√©marrage des pods..."
        sleep 60
        
        # V√©rifier √† nouveau
        kubectl get pods -n ibis-x 2>/dev/null || log_info "Pods en cours de cr√©ation..."
    fi
    
    log_success "V√©rification des pods termin√©e"
}

# Fonction pour ex√©cuter les migrations de base de donn√©es avec auto-correction compl√®te
run_migrations() {
    log_info "üöÄ D√©marrage de l'auto-correction compl√®te des migrations..."
    final_auto_check_and_fix
}

# Fonction pour v√©rifier le statut final du job Kaggle
check_final_kaggle_status() {
    log_info "üîç V√©rification finale du statut du job Kaggle..."
    
    local job_exists=false
    local job_status=""
    
    # V√©rifier si le job existe
    if kubectl get job kaggle-dataset-import-job -n "${K8S_NAMESPACE:-ibis-x}" &>/dev/null; then
        job_exists=true
        job_status=$(kubectl get job kaggle-dataset-import-job -n "${K8S_NAMESPACE:-ibis-x}" -o jsonpath='{.status.conditions[0].type}' 2>/dev/null || echo "Active")
        
        local pod_name
        pod_name=$(kubectl get pods -n "${K8S_NAMESPACE:-ibis-x}" -l job-name=kaggle-dataset-import-job -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        
        case "$job_status" in
            "Complete")
                log_success "‚úÖ Job Kaggle termin√© avec succ√®s"
                ;;
            "Failed")
                log_warning "‚ö†Ô∏è Job Kaggle a √©chou√© - donn√©es Kaggle non import√©es"
                log_info "üí° Vous pouvez r√©essayer manuellement avec:"
                log_info "   kubectl exec -n ${K8S_NAMESPACE:-ibis-x} deployment/service-selection -- python kaggle-import/main.py --force-refresh"
                ;;
            "Active"|"")
                if [[ -n "$pod_name" ]]; then
                    local pod_status
                    pod_status=$(kubectl get pod "$pod_name" -n "${K8S_NAMESPACE:-ibis-x}" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
                    
                    if [[ "$pod_status" == "Running" ]]; then
                        log_info "‚è≥ Job Kaggle encore en cours d'ex√©cution en arri√®re-plan"
                        log_info "üìä Vous pouvez suivre l'avancement avec:"
                        log_info "   kubectl logs -f $pod_name -n ${K8S_NAMESPACE:-ibis-x}"
                    else
                        log_warning "‚ö†Ô∏è Job Kaggle dans un √©tat ind√©termin√© (Pod: $pod_status)"
                    fi
                else
                    log_warning "‚ö†Ô∏è Job Kaggle existe mais aucun pod trouv√©"
                fi
                ;;
        esac
    else
        log_info "üìä Aucun job Kaggle en cours (WITH_DATA peut √™tre d√©sactiv√©)"
    fi
}

# Fonction pour afficher les informations de l'application
show_application_info() {
    log_success "üéâ D√©ploiement IBIS-X 100% AUTOMATIQUE termin√© avec succ√®s !"
    echo
    echo "‚úÖ AUTOMATISATION COMPL√àTE R√âUSSIE :"
    echo "===================================="
    echo "‚úÖ Infrastructure Azure cr√©√©e automatiquement"
    echo "‚úÖ Images Docker construites et push√©es automatiquement"
    echo "‚úÖ Secrets Kubernetes cr√©√©s automatiquement (vrais secrets Azure)"
    echo "‚úÖ Applications d√©ploy√©es automatiquement (avec fallback Windows)"
    echo "‚úÖ NGINX Ingress + Cert-Manager install√©s automatiquement"
    echo "‚úÖ Pods d√©faillants red√©marr√©s automatiquement"
    echo "‚úÖ Aucune commande manuelle requise !"
    echo
    echo "üìã Informations de l'application :"
    echo "=================================="
    echo "üåê URL de l'application: https://ibisx.fr (certificat SSL automatique)"
    echo "üåê URL de l'API: https://api.ibisx.fr (certificat SSL automatique)"
    echo "üåê URL HTTP direct: http://$PUBLIC_IP"
    echo "üóÑÔ∏è  Storage Account: $STORAGE_ACCOUNT"
    echo "üê≥ Container Registry: $ACR_NAME.azurecr.io"
    echo "‚ò∏Ô∏è  Cluster AKS: $AKS_NAME"
    echo "üì¶ Resource Group: $RESOURCE_GROUP"
    echo
    echo "üîß Commandes utiles pour monitoring :"
    echo "====================================="
    echo "# Voir l'√©tat des pods:"
    echo "kubectl get pods -n ibis-x"
    echo
    echo "# Voir les services:"
    echo "kubectl get services -n ibis-x"
    echo
    echo "# Voir l'ingress et IP publique:"
    echo "kubectl get ingress -n ibis-x"
    echo
    echo "# Voir les certificats SSL et leur statut:"
    echo "kubectl get certificates -n ibis-x"
    echo ""
    echo "# Voir l'ingress et IP publique:"
    echo "kubectl get ingress -n ibis-x"
    echo ""
    echo "# Tester l'application en ligne:"
    echo "curl -I http://$PUBLIC_IP"
    echo
    echo "# Voir les logs des applications:"
    echo "kubectl logs -f deployment/api-gateway -n ibis-x"
    echo "kubectl logs -f deployment/service-selection -n ibis-x"
    echo "kubectl logs -f deployment/frontend -n ibis-x"
    echo
    echo "üìù SCRIPT ENTI√àREMENT AUTOMATIQUE :"
    echo "==================================="
    echo "üéØ Pour relancer un d√©ploiement complet : ./scripts/deploy-to-azure.sh"
    echo "üîÑ Le script d√©tecte automatiquement l'infrastructure existante"
    echo "üõ†Ô∏è Tous les probl√®mes Windows/Linux g√©r√©s automatiquement"
    echo "üîê Tous les secrets g√©n√©r√©s automatiquement"
    echo "üì¶ Toutes les images avec les bons noms automatiquement"
    echo
    echo "üéØ PROCHAINES √âTAPES (une seule fois) :"
    echo "======================================"
    echo "1. ‚úÖ IP STATIQUE: $PUBLIC_IP (ne changera jamais !)"
    echo "2. üåê Configurez vos DNS D√âFINITIVEMENT vers cette IP"
    echo "3. üîí Les certificats SSL se g√©n√®reront automatiquement"
    echo "4. üöÄ L'application est accessible via https://ibisx.fr"
    echo
    echo "üí° IMPORTANT: Cette IP est STATIQUE et ne changera pas lors des futurs red√©ploiements !"
    echo "üí° Configurez vos DNS une seule fois avec cette IP !"
}

# Fonction pour nettoyer les fichiers de sauvegarde apr√®s un d√©ploiement r√©ussi
cleanup_backup_files() {
    log_info "üßπ Nettoyage des fichiers de sauvegarde..."
    
    # Supprimer les fichiers de sauvegarde
    find "$K8S_DIR" -name "*.yaml.backup" -delete 2>/dev/null || true
    find "$TERRAFORM_DIR" -name "*.backup" -delete 2>/dev/null || true
    find "$K8S_DIR" -name "*.backup-*" -delete 2>/dev/null || true  # Fichiers avec timestamp
    find "$PROJECT_ROOT/frontend/src/environments/" -name "*.backup-*" -delete 2>/dev/null || true  # Frontend backups
    
    log_success "‚úÖ Fichiers de sauvegarde nettoy√©s"
}

# Fonction pour nettoyer en cas d'erreur
cleanup_on_error() {
    log_error "‚ùå Une erreur s'est produite pendant le d√©ploiement."
    
    # üîÑ Restaurer nginx-ingress-values.yaml si modifi√©
    local nginx_values_file="$K8S_DIR/helm-values/nginx-ingress-values.yaml"
    local latest_backup=$(find "$K8S_DIR/helm-values/" -name "nginx-ingress-values.yaml.backup-*" | sort | tail -1 2>/dev/null || echo "")
    if [[ -n "$latest_backup" && -f "$latest_backup" ]]; then
        mv "$latest_backup" "$nginx_values_file"
        log_info "‚úÖ Fichier nginx-ingress-values.yaml restaur√© depuis backup"
    fi
    
    # üîÑ Restaurer environment.prod.ts si modifi√©
    local env_file="$PROJECT_ROOT/frontend/src/environments/environment.prod.ts"
    local latest_env_backup=$(find "$PROJECT_ROOT/frontend/src/environments/" -name "environment.prod.ts.backup-*" | sort | tail -1 2>/dev/null || echo "")
    if [[ -n "$latest_env_backup" && -f "$latest_env_backup" ]]; then
        mv "$latest_env_backup" "$env_file"
        log_info "‚úÖ Fichier environment.prod.ts restaur√© depuis backup"
    fi
    
    # Restaurer les fichiers de sauvegarde s'ils existent
    if [ -f "$K8S_DIR/base/service-selection/storage-secrets.yaml.backup" ]; then
        mv "$K8S_DIR/base/service-selection/storage-secrets.yaml.backup" "$K8S_DIR/base/service-selection/storage-secrets.yaml"
        log_info "‚úÖ Fichier storage-secrets.yaml restaur√©"
    fi
    
    if [ -f "$K8S_DIR/overlays/azure/kustomization.yaml.backup" ]; then
        mv "$K8S_DIR/overlays/azure/kustomization.yaml.backup" "$K8S_DIR/overlays/azure/kustomization.yaml"
        log_info "‚úÖ Fichier kustomization.yaml restaur√©"
    fi
    
    # Restaurer tous les fichiers YAML modifi√©s
    find "$K8S_DIR" -name "*.yaml.backup" -exec bash -c 'mv "$1" "${1%.backup}"' _ {} \; 2>/dev/null || true
    find "$TERRAFORM_DIR" -name "*.backup" -exec bash -c 'mv "$1" "${1%.backup}"' _ {} \; 2>/dev/null || true
    find "$K8S_DIR" -name "*.backup-*" -exec rm -f {} \; 2>/dev/null || true  # Supprimer backups avec timestamp
    log_info "‚úÖ Tous les fichiers restaur√©s"
    
    echo
    log_warning "üí° Pour nettoyer les ressources Azure cr√©√©es, ex√©cutez :"
    echo "./scripts/production/destroy-azure-infrastructure.sh"
}

# Fonction principale
main() {
    log_info "üöÄ Script de D√©ploiement Production IBIS-X"
    log_info "üéØ Mode: $DEPLOYMENT_MODE"
    log_info "‚ÑπÔ∏è  Pour le d√©veloppement local, utilisez: make dev"
    echo
    
    # Configurer le gestionnaire d'erreur
    trap cleanup_on_error ERR
    
    # Workflow principal
    log_info "üìã === D√âPLOIEMENT PRODUCTION ==="
    
    # √âtapes d'infrastructure et d√©ploiement
    check_prerequisites
    check_azure_login
    
    # Gestion de l'infrastructure (cr√©er si n√©cessaire)
    manage_infrastructure
    
    # Configuration et d√©ploiement
    get_terraform_outputs
    configure_kubectl
    check_k8s_connectivity
    
    # Build et d√©ploiement de l'application
    build_and_push_images
    update_k8s_secrets  # Cr√©er le namespace
    create_missing_secrets
    deploy_application
    
    # Migrations et finalisation
    wait_for_migrations
    
    # V√©rification des ressources AKS (cluster configur√© avec 3 n≈ìuds)
    check_aks_resources
    
    initialize_sample_data
    final_auto_check_and_fix
    
    # ‚úÖ AUTOMATIQUE: Forcer rebuild frontend en production si n√©cessaire
    if [[ "$DEPLOYMENT_MODE" == "manual-production" ]] || [[ "$ANGULAR_ENV" == "production" ]]; then
        log_info "üéØ Mode production d√©tect√© - V√©rification et correction automatique du frontend..."
        force_frontend_production_rebuild
    fi
    
    # Nettoyage si mode GitHub Actions
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        cleanup_migration_jobs
    fi
    
    # V√©rification finale du job Kaggle
    check_final_kaggle_status
    
    # Afficher les informations finales
    show_application_info
    
    # Nettoyer les fichiers de sauvegarde
    cleanup_backup_files
    
    log_success "üéâ D√âPLOIEMENT PRODUCTION TERMIN√â AVEC SUCC√àS !"
}

# Fonction pour g√©rer l'infrastructure selon le contexte
manage_infrastructure() {
    log_info "üèóÔ∏è Gestion de l'infrastructure Azure..."
    
    if [[ "$USE_GITHUB_SECRETS" == "true" ]]; then
        # Mode GitHub Actions : infrastructure suppos√©e existante
        log_info "üè≠ Mode GitHub Actions - Infrastructure suppos√©e existante"
        log_info "‚ÑπÔ∏è  Si l'infrastructure n'existe pas, cr√©ez-la manuellement avec ce script"
    else
        # Mode manuel : v√©rifier si infrastructure existe, sinon la cr√©er
        log_info "üõ†Ô∏è Mode manuel - V√©rification de l'infrastructure..."
        
        # V√©rifier si l'infrastructure existe
        if check_infrastructure_exists; then
            log_success "‚úÖ Infrastructure existante d√©tect√©e"
            log_info "üîÑ Continuons avec le d√©ploiement de l'application..."
        else
            log_info "üèóÔ∏è Infrastructure non trouv√©e - Cr√©ation automatique..."
            create_infrastructure
        fi
    fi
}

# Fonction pour v√©rifier si l'infrastructure existe
check_infrastructure_exists() {
    log_info "üîç V√©rification de l'existence de l'infrastructure..."
    
    # V√©rifier si les ressources principales existent
    local acr_exists=$(az acr show --name "$AZURE_CONTAINER_REGISTRY" --resource-group "$AZURE_RESOURCE_GROUP" 2>/dev/null && echo "true" || echo "false")
    local aks_exists=$(az aks show --name "$AKS_CLUSTER_NAME" --resource-group "$AZURE_RESOURCE_GROUP" 2>/dev/null && echo "true" || echo "false")
    
    if [[ "$acr_exists" == "true" ]] && [[ "$aks_exists" == "true" ]]; then
        log_success "‚úÖ Infrastructure existante (ACR + AKS)"
        return 0
    else
        log_info "‚ùå Infrastructure incompl√®te ou manquante"
        log_info "   ACR ($AZURE_CONTAINER_REGISTRY): $acr_exists"
        log_info "   AKS ($AKS_CLUSTER_NAME): $aks_exists"
        return 1
    fi
}

# Fonction pour cr√©er l'infrastructure
create_infrastructure() {
    log_info "üèóÔ∏è Cr√©ation de l'infrastructure Azure via Terraform..."
    
    # Cr√©er l'infrastructure compl√®te
    check_kubernetes_versions
    init_terraform
    deploy_infrastructure
    
    log_success "‚úÖ Infrastructure cr√©√©e avec succ√®s"
}



# V√©rifier si le script est ex√©cut√© directement
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi 
