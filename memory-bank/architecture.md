# Architecture du Projet EXAI (PoC)

**Version :** (Bas√©e sur l'analyse du code au 2024-MM-JJ - *remplacez MM-JJ*)
**Bas√© sur :** `prd_exai_poc_v2.md`, `tech_stack_exai_v2.md`, `implementation_plan_exai_poc_adjusted.md`, analyse du code existant.

## 1. Vue d'ensemble

Le projet EXAI suit une architecture microservices con√ßue pour √™tre d√©ploy√©e sur Kubernetes (Minikube pour la PoC). L'objectif est de cr√©er un pipeline int√©gr√© : S√©lection de Donn√©es -> Pipeline ML Guid√© -> Explication XAI.

```mermaid
graph LR
    A[Utilisateur] --> B(Frontend Angular);
    B --> C{API Gateway FastAPI};
    C --> D[Service S√©lection FastAPI];
    C --> E[Service Pipeline ML FastAPI];
    C --> F[Service XAI FastAPI];
    D --> G[(PostgreSQL)];
    E --> G;
    E --> H[(Redis)];
    E --> I(Celery Worker ML);
    F --> J(Celery Worker XAI);
    F --> H;
    I --> G;
    J --> G;
    subgraph Kubernetes Cluster (Namespace: exai)
        B; C; D; E; F; G; H; I; J;
    end
```

**Composants Principaux :**

*   **`frontend/` :** Interface utilisateur d√©velopp√©e avec Angular et Angular Material.
*   **`api-gateway/` :** Point d'entr√©e unique (FastAPI) g√©rant l'authentification (JWT via `fastapi-users`) et le routage vers les services backend.
*   **`service-selection/` :** Service FastAPI g√©rant les m√©tadonn√©es des datasets (stock√©es dans PostgreSQL), incluant le CRUD, le filtrage/recherche et le scoring.
*   **`ml-pipeline/` :** Service FastAPI orchestrant l'ex√©cution asynchrone (via Celery/Redis) des t√¢ches d'entra√Ænement et d'√©valuation de mod√®les ML (avec Scikit-learn).
*   **`xai-engine/` :** Service FastAPI orchestrant l'ex√©cution asynchrone (via Celery/Redis) des t√¢ches de g√©n√©ration d'explications XAI (avec SHAP/LIME).
*   **`PostgreSQL` :** Base de donn√©es relationnelle pour stocker toutes les m√©tadonn√©es persistantes (datasets, utilisateurs, runs ML, requ√™tes XAI). Sch√©ma g√©r√© via Alembic.
*   **`Redis` :** Broker de messages pour la communication asynchrone via Celery.
*   **`Celery Workers` :** Processus ex√©cutant les t√¢ches longues (ML et XAI) en arri√®re-plan.
*   **`Kubernetes (Minikube)` :** Plateforme d'orchestration pour le d√©ploiement et la gestion des conteneurs Docker.
*   **`Skaffold & Kustomize` :** Outils utilis√©s pour le d√©veloppement local (build/deploy) et la gestion des configurations K8s par environnement.
*   **`docs/` :** Documentation utilisateur et technique au format Antora/Asciidoc (aspect critique du projet).

## 2. √âtat Actuel des Composants (Bas√© sur l'analyse du code)

*   **`api-gateway/` :**
    *   **R√¥le :** Point d'entr√©e, Authentification/Autorisation.
    *   **Technos :** FastAPI, Uvicorn, `fastapi-users[sqlalchemy]`, `asyncpg`, Alembic.
    *   **Statut :**
        *   [‚úÖ] Configuration FastAPI de base.
        *   [‚úÖ] Authentification JWT via `fastapi-users` fonctionnelle (login, register, etc.).
        *   [‚úÖ] Table `user` g√©r√©e par Alembic.
        *   [‚úÖ] Endpoint `/health` pr√©sent.
        *   [‚úÖ] CORS configur√© (permissif).
        *   [‚¨ú] Routage Reverse Proxy vers les autres services **non impl√©ment√©**.
        *   [üöß] D√©ploiement K8s √† finaliser (configuration probes, secrets).

*   **`service-selection/` :**
    *   **R√¥le :** Gestion des m√©tadonn√©es des datasets.
    *   **Technos :** FastAPI, Uvicorn, SQLAlchemy, Pydantic, Alembic, `psycopg2-binary`/`asyncpg`.
    *   **Statut :**
        *   [‚úÖ] Configuration FastAPI de base.
        *   [‚úÖ] Mod√®le SQLAlchemy `Dataset` d√©fini et complet.
        *   [‚úÖ] Sch√©mas Pydantic de base (`DatasetBase`, `Create`, `Update`, `Read`) d√©finis.
        *   [‚úÖ] Endpoints CRUD de base (`POST`, `GET /`, `GET /id`, `PUT`, `DELETE`) impl√©ment√©s.
        *   [‚úÖ] Table `datasets` g√©r√©e par Alembic.
        *   [üöß] Endpoint `GET /datasets` a une pagination simple, mais **pas de filtrage avanc√©**.
        *   [‚¨ú] Sch√©mas Pydantic avanc√©s (`FilterCriteria`, `ScoreRequest`, etc.) **non impl√©ment√©s**.
        *   [‚¨ú] Logique de scoring **non impl√©ment√©e**.
        *   [‚¨ú] Endpoints `/score`, `/preview`, `/stats` **non impl√©ment√©s**.
        *   [üöß] D√©ploiement K8s √† finaliser (configuration probes, secrets).

*   **`ml-pipeline/` :**
    *   **R√¥le :** Orchestration entra√Ænement ML.
    *   **Technos Pr√©vues :** FastAPI, Celery, Scikit-learn.
    *   **Statut :** [‚¨ú] **Non d√©marr√©** (bas√© sur l'absence de code fourni).

*   **`xai-engine/` :**
    *   **R√¥le :** G√©n√©ration explications XAI.
    *   **Technos Pr√©vues :** FastAPI, Celery, SHAP/LIME.
    *   **Statut :** [‚¨ú] **Non d√©marr√©** (bas√© sur l'absence de code fourni).

*   **`frontend/` :**
    *   **R√¥le :** Interface Utilisateur.
    *   **Technos :** Angular, Angular Material, TypeScript.
    *   **Statut :**
        *   [‚úÖ] Projet Angular initialis√©.
        *   [‚úÖ] Angular Material ajout√© comme d√©pendance.
        *   [üöß] Structure de base pr√©sente (`services`, `pages`, `layouts`...).
        *   [üöß] Service `AuthService` et module/pages d'authentification en cours de d√©veloppement.
        *   [‚¨ú] Services API d√©di√©s (`DatasetService`, `PipelineService`...) **non impl√©ment√©s**.
        *   [‚¨ú] Modules/Composants fonctionnels principaux (S√©lection Dataset, Pipeline ML, XAI) **non impl√©ment√©s** ou structur√©s diff√©remment du plan.
        *   [‚¨ú] D√©ploiement K8s non configur√©.

*   **Infrastructure :**
    *   [‚úÖ] PostgreSQL d√©ploy√© sur K8s et accessible.
        *   **Note importante (2024-04-27) :** La gestion de PostgreSQL a √©t√© migr√©e d'un Deployment vers un **StatefulSet** pour une meilleure gestion de l'√©tat, une identit√© stable des pods, et pour r√©soudre les probl√®mes d'attachement de volume ReadWriteOnce (RWO) lors des mises √† jour.
    *   [‚¨ú] Redis non d√©ploy√©.
    *   [‚¨ú] Workers Celery non d√©ploy√©s.
    *   [‚úÖ] Ingress Controller (NGINX via Helm) d√©ploy√© sur AKS.
    *   [‚úÖ] Cert-Manager d√©ploy√© via Helm sur AKS pour gestion TLS Let's Encrypt.
    *   [‚úÖ] Ingress K8s (`exai-ingress`) configur√© pour router `exai-pipeline.fr` vers `frontend` et `api.exai-pipeline.fr` vers `api-gateway`, avec TLS activ√© via cert-manager.
    *   **Note Infrastructure Azure (AKS) :
        *   Le service Nginx Ingress (type LoadBalancer) cr√©e un Load Balancer public Azure.
        *   Des r√®gles NSG sont configur√©es pour autoriser le trafic sur les ports 80 et 443 vers l'IP publique du Load Balancer.
        *   **Point critique (r√©solu le 2025-04-27):** Les sondes de sant√© (Health Probes) HTTP et HTTPS du Load Balancer Azure *doivent* cibler le chemin `/healthz` sur les NodePorts correspondants du service Nginx Ingress (par d√©faut `/` qui provoque des √©checs) pour que le Load Balancer consid√®re les n≈ìuds comme sains et route le trafic correctement.

## 3. Interactions Cl√©s

*   Le **Frontend** communique exclusivement avec l'**API Gateway**.
*   L'**API Gateway** valide l'authentification (JWT) et relaie les requ√™tes aux services backend appropri√©s (fonctionnalit√© de proxy **√† impl√©menter**).
*   **`service-selection`**, **`ml-pipeline`**, **`xai-engine`** interagissent avec la base de donn√©es **PostgreSQL** (via SQLAlchemy et Alembic pour les migrations).
*   **`ml-pipeline`** et **`xai-engine`** utilisent **Redis** comme broker pour envoyer/recevoir des t√¢ches via **Celery Workers**.
*   Les **Celery Workers** interagissent avec **PostgreSQL** pour lire/√©crire les statuts et r√©sultats, et potentiellement avec un stockage de fichiers partag√© (PV K8s / Blob Storage) pour lire/√©crire des datasets/mod√®les/r√©sultats.

## 4. Documentation

*   La documentation utilisateur et technique doit √™tre g√©n√©r√©e dans `docs/` en utilisant **Antora/Asciidoc**. C'est une exigence forte du projet. (Statut actuel : Probablement [‚¨ú])

## 5. D√©ploiement et CI/CD

*   **D√©veloppement Local :** `skaffold dev` est utilis√© pour builder les images Docker localement et d√©ployer sur Minikube en utilisant Kustomize (`k8s/overlays/minikube`).

## D√©veloppement Local

L'environnement de d√©veloppement local utilise Minikube pour simuler le cluster Kubernetes et Skaffold pour automatiser le cycle de build/d√©ploiement.

### Acc√®s aux Services (Profil Local)

Lorsque l'on utilise la commande `skaffold dev --profile=local`, l'acc√®s aux principaux services se fait via des redirections de port g√©r√©es automatiquement par Skaffold :

*   **Frontend:** Accessible sur `http://localhost:8080`
*   **API Gateway:** Accessible sur `http://localhost:9000` (y compris `/docs` et `/redoc`)

Cette m√©thode √©vite d'avoir besoin de `minikube tunnel` ou `minikube service` pour le workflow de d√©veloppement standard.

Il est crucial qu'aucun autre service (comme un serveur XAMPP/Apache local) n'utilise les ports `8080` ou `9000` sur la machine h√¥te.

### Autres Composants

(Description des autres aspects du d√©veloppement local, base de donn√©es, etc.)

## D√©ploiement

*   **D√©ploiement Production (Azure) :**
    *   Un workflow GitHub Actions (`.github/workflows/deploy-production.yml`) est configur√©.
    *   **Trigger :** Push sur la branche `production`.
    *   **√âtapes Principales :
        1.  Checkout du code.
        2.  Login sur Azure Container Registry (ACR).
        3.  Build et Push des images Docker des services (`api-gateway`, `service-selection`, `frontend`, etc.) vers ACR, tagu√©es avec le SHA court du commit et `latest`.
        4.  (Placeholder) Ex√©cution des tests unitaires/int√©gration.
        5.  Login sur Azure (via Service Principal).
        6.  Configuration du contexte `kubectl` pour le cluster AKS cible.
        7.  D√©ploiement sur AKS via `skaffold run --profile=azure --tag=<commit_sha>` qui utilise l'overlay Kustomize `k8s/overlays/azure`.
    *   **Gestion de la Configuration Production :
        *   **Frontend :** Utilisation de `frontend/src/environments/environment.prod.ts` (qui contient l'URL de l'API de production) activ√© par la configuration de build Angular et le Dockerfile.
        *   **Backend :** Les configurations (URL BDD, secrets, etc.) sont inject√©es via des variables d'environnement d√©finies dans les manifestes K8s (via ConfigMaps/Secrets) de l'overlay `k8s/overlays/azure`.
        *   **Kubernetes :** L'overlay `k8s/overlays/azure` contient les manifestes/patches sp√©cifiques √† Azure (ex: nom d'images pr√©fix√© par l'ACR, configurations de ressources, Ingress, r√©f√©rences aux secrets Azure Key Vault si utilis√©).
    *   **Secrets Requis (GitHub Actions) :** `ACR_USERNAME`, `ACR_PASSWORD`, `AZURE_CREDENTIALS`.
    *   **Certificats TLS :** G√©r√©s automatiquement par `cert-manager` via `ClusterIssuer` Let's Encrypt (requiert configuration Ingress correcte et accessibilit√© externe sur port 80 pour challenge HTTP-01).
    *   **Note Infrastructure Azure (AKS) :
        *   Le service Nginx Ingress (type LoadBalancer) cr√©e un Load Balancer public Azure.
        *   Des r√®gles NSG sont configur√©es pour autoriser le trafic sur les ports 80 et 443 vers l'IP publique du Load Balancer.
        *   **Point critique (r√©solu le 2025-04-27):** Les sondes de sant√© (Health Probes) HTTP et HTTPS du Load Balancer Azure *doivent* cibler le chemin `/healthz` sur les NodePorts correspondants du service Nginx Ingress (par d√©faut `/` qui provoque des √©checs) pour que le Load Balancer consid√®re les n≈ìuds comme sains et route le trafic correctement. 