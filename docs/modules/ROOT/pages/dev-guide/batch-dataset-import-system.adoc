= Syst√®me d'Importation de Datasets en Batch - Architecture V2

== Vue d'ensemble

Le syst√®me d'importation de datasets en batch d'IBIS-X V2 permet d'int√©grer facilement de multiples datasets r√©els avec **m√©tadonn√©es enrichies sp√©cifiques**. Il g√®re automatiquement :

* La conversion CSV vers Parquet (gains de performance 10-50x)
* **üÜï M√©tadonn√©es enrichies sp√©cifiques par dataset (39+ champs)**
* L'upload vers le stockage d'objets (MinIO/Azure) avec architecture UUID
* L'insertion en base de donn√©es avec validation compl√®te
* **üÜï CLI de gestion et validation des m√©tadonn√©es**
* Le monitoring et reporting avanc√©

[IMPORTANT]
====
**NOUVEAU SYST√àME V2** : Les m√©tadonn√©es sont maintenant **sp√©cifiques par dataset** au lieu d'utiliser des templates g√©n√©riques. Voir xref:enriched-metadata-system.adoc[Guide des M√©tadonn√©es Enrichies] pour les d√©tails.
====

== Architecture du Syst√®me

=== Structure des Dossiers V2

[source]
----
datasets/
‚îî‚îÄ‚îÄ kaggle-import/                      # Scripts d'importation Kaggle V2
    ‚îú‚îÄ‚îÄ main.py                         # üÜï Script principal unifi√©
    ‚îú‚îÄ‚îÄ kaggle_metadata_mapper_v2.py    # üÜï Nouveau mapper m√©tadonn√©es
    ‚îú‚îÄ‚îÄ metadata_manager.py             # üÜï CLI de gestion m√©tadonn√©es
    ‚îú‚îÄ‚îÄ kaggle_datasets_config.yaml     # Configuration des datasets
    ‚îú‚îÄ‚îÄ requirements.txt                # D√©pendances Python
    ‚îú‚îÄ‚îÄ Makefile                        # Commandes d'automatisation
    ‚îú‚îÄ‚îÄ README.md                       # Guide d'utilisation
    ‚îú‚îÄ‚îÄ enriched_metadata/              # üÜï NOUVEAU SYST√àME M√âTADONN√âES
    ‚îÇ   ‚îú‚îÄ‚îÄ README.md                   # Documentation m√©tadonn√©es
    ‚îÇ   ‚îú‚îÄ‚îÄ schema.json                 # Validation JSON Schema
    ‚îÇ   ‚îú‚îÄ‚îÄ datasets/                   # üìä M√©tadonn√©es sp√©cifiques
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ academic_performance.json    # 39+ champs pr√©cis
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ student_performance.json
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ student_stress.json
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ student_depression.json
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ social_media_addiction.json
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ riiid_answer_prediction.json
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ oulad_dataset.json
    ‚îÇ   ‚îî‚îÄ‚îÄ templates/                  # üìã Templates d'aide
    ‚îÇ       ‚îú‚îÄ‚îÄ education_template.json
    ‚îÇ       ‚îú‚îÄ‚îÄ healthcare_template.json
    ‚îÇ       ‚îî‚îÄ‚îÄ default_template.json
    ‚îú‚îÄ‚îÄ importer_lib/                   # Modules de traitement
    ‚îÇ   ‚îú‚îÄ‚îÄ metadata_loader.py          # üÜï Chargeur m√©tadonn√©es JSON
    ‚îÇ   ‚îú‚îÄ‚îÄ kaggle_api.py               # Interface API Kaggle
    ‚îÇ   ‚îú‚îÄ‚îÄ file_processor.py           # Analyse et conversion fichiers
    ‚îÇ   ‚îú‚îÄ‚îÄ db_manager.py               # Gestion base de donn√©es
    ‚îÇ   ‚îî‚îÄ‚îÄ storage.py                  # Gestion stockage objets
    ‚îú‚îÄ‚îÄ templates/                      # üóëÔ∏è OBSOL√àTE (√† supprimer)
    ‚îÇ   ‚îú‚îÄ‚îÄ ethical_defaults.yaml       # ‚ùå Remplac√© par syst√®me JSON
    ‚îÇ   ‚îî‚îÄ‚îÄ auto_init.py                # ‚ùå Remplac√© par metadata_loader
    ‚îî‚îÄ‚îÄ kaggle_metadata_mapper.py       # üóëÔ∏è OBSOL√àTE (remplac√© par V2)
----

=== Flux de Traitement V2

1. **Configuration** : D√©finition des datasets Kaggle dans `kaggle_datasets_config.yaml`
2. **T√©l√©chargement** : Import automatique via API Kaggle
3. **Analyse** : Extraction m√©tadonn√©es automatique (colonnes, types, statistiques)
4. **üÜï Enrichissement** : Chargement m√©tadonn√©es sp√©cifiques depuis JSON (39+ champs)
5. **Conversion** : CSV ‚Üí Parquet pour optimisation performances  
6. **Upload** : Stockage vers MinIO (local) ou Azure Blob (production) avec UUID
7. **üÜï Validation** : Validation m√©tadonn√©es contre sch√©ma JSON + mod√®le SQLAlchemy
8. **Base de donn√©es** : Insertion des m√©tadonn√©es enrichies compl√®tes
9. **Cache** : Syst√®me de cache intelligent (7 jours)
10. **Nettoyage** : Suppression automatique des fichiers temporaires

[NOTE]
====
**üÜï Diff√©rence cl√© V2** : Les m√©tadonn√©es sont maintenant **charg√©es depuis des fichiers JSON sp√©cifiques** au lieu d'√™tre g√©n√©r√©es par templates g√©n√©riques.
====

== Utilisation Pratique

=== Configuration des Datasets

==== 1. Fichier de Configuration

Le fichier `kaggle_datasets_config.yaml` d√©finit tous les datasets √† importer :

[source,yaml]
----
datasets:
  student_performance:
    kaggle_ref: "spscientist/students-performance-in-exams"
    domain: "education"
    description: "Student performance analysis"
    ml_task: "classification"
    target_column: "math_score"
----

==== 2. Credentials Kaggle

Configuration des credentials API Kaggle :

[source,bash]
----
# Cr√©er le fichier de configuration
mkdir -p ~/.kaggle
echo '{"username":"YOUR_USERNAME","key":"YOUR_API_KEY"}' > ~/.kaggle/kaggle.json
chmod 600 ~/.kaggle/kaggle.json
----

=== üÜï M√©tadonn√©es Enrichies Sp√©cifiques

Le syst√®me V2 utilise des **m√©tadonn√©es sp√©cifiques par dataset** stock√©es en JSON :

[source,json]
----
# Exemple : enriched_metadata/datasets/academic_performance.json
{
  "metadata_version": "1.0",
  "dataset_info": {
    "name": "academic_performance",
    "source": "kaggle",
    "kaggle_ref": "nikhil7280/student-performance-multiple-linear-regression"
  },
  "enriched_metadata": {
    "dataset_name": "Student Academic Performance Dataset", 
    "year": 2025,
    "objective": "To help users explore how various factors affect student performance...",
    "access": "public",
    "availability": "online_download",
    "num_citations": 0,
    "instances_number": 1000,        # Calcul√© automatiquement
    "features_number": 10,           # Calcul√© automatiquement  
    "features_description": "student_id, name, gender, age, grade_level...",
    "domain": ["education"],
    "representativity_level": "moderate",
    "sample_balance_level": "balanced",
    "has_missing_values": false,
    "global_missing_percentage": 0.0,
    "task": ["classification", "regression"],
    
    // üîí Crit√®res √©thiques sp√©cifiques (39+ champs au total)
    "informed_consent": false,
    "transparency": false,
    "anonymization_applied": true,
    "accountability_defined": false
    // ... tous les autres champs √©thiques et techniques
  }
}
----

[TIP]
====
**Avantages V2** : 39+ champs pr√©cis vs 6 champs g√©n√©riques. Voir xref:enriched-metadata-system.adoc[Guide Complet des M√©tadonn√©es Enrichies].
====

=== Scripts d'Importation V2

==== Script Principal (main.py)

**Fonctionnalit√©s V2** :
* Import automatique depuis API Kaggle
* **üÜï Chargement m√©tadonn√©es enrichies sp√©cifiques (JSON)**
* Cache intelligent (7 jours)
* Conversion CSV‚ÜíParquet automatique avec UUID
* Upload parall√®le vers stockage d'objets
* **üÜï Validation m√©tadonn√©es contre sch√©ma JSON**
* Gestion d'erreurs robuste avec fallback
* Reporting d√©taill√©

**Usage** :
[source,bash]
----
# Import de tous les datasets avec m√©tadonn√©es enrichies
python main.py

# Import d'un dataset sp√©cifique 
python main.py --dataset student_performance

# Forcer le re-t√©l√©chargement
python main.py --force-refresh
----

==== üÜï CLI de Gestion M√©tadonn√©es (metadata_manager.py)

**Nouveaux outils de gestion** :

[source,bash]
----
# Lister tous les datasets et leur statut m√©tadonn√©es
python metadata_manager.py list

# Validation compl√®te de toutes les m√©tadonn√©es  
python metadata_manager.py validate

# Validation d'un dataset sp√©cifique avec d√©tails
python metadata_manager.py validate academic_performance

# G√©n√©ration m√©tadonn√©es depuis template
python metadata_manager.py generate nouveau_dataset --domain education

# V√©rification champs manquants  
python metadata_manager.py check-fills student_performance
----

=== Cache et Performance

==== Cache Intelligent

* **Dur√©e** : 7 jours par d√©faut (configurable)
* **√âvite re-t√©l√©chargements** : Datasets d√©j√† import√©s
* **Stockage** : Fichiers JSON dans `cache/`
* **Invalidation** : Automatique ou forc√©e via `--force-refresh`

==== Optimisations

* **Conversion Parquet** : Gains de performance 10-50x
* **Upload parall√®le** : 4 threads simultan√©s
* **Retry logic** : 3 tentatives avec backoff exponentiel
* **Nettoyage automatique** : Suppression fichiers temporaires

=== Commandes Make Disponibles

==== Installation et Configuration

[source,bash]
----
# Installation des d√©pendances
make install

# Test authentification Kaggle
make test-auth

# V√©rification configuration
make check-config
----

==== Op√©rations d'Import

[source,bash]
----
# Import de tous les datasets
make import-all

# Import des petits datasets seulement
make import-small

# Import d'un dataset sp√©cifique
make import-dataset DATASET=student_performance

# Forcer le re-t√©l√©chargement
make force-refresh
----

==== Monitoring et Maintenance V2

[source,bash]
----
# Afficher l'√©tat des imports
make status

# Lister les datasets configur√©s
make list-datasets

# üÜï Validation compl√®te des m√©tadonn√©es enrichies
cd datasets/kaggle-import
python metadata_manager.py validate

# üÜï Afficher le statut d√©taill√© de tous les datasets  
python metadata_manager.py list

# Test de connexion aux services
make test-services

# Nettoyage des fichiers temporaires
make clean

# üÜï Nettoyage des fichiers obsol√®tes (templates YAML)
make clean-legacy
----

==== üÜï Nouvelles Commandes M√©tadonn√©es

[source,bash]
----
# CLI sp√©cialis√© dans la gestion des m√©tadonn√©es enrichies
cd datasets/kaggle-import

# Dashboard complet des datasets
python metadata_manager.py list
# Sortie :
# Dataset                   Configur√©  M√©tadonn√©es  Statut
# ======================================================================
# academic_performance      ‚úÖ          ‚úÖ            ‚úÖ Pr√™t
# oulad_dataset             ‚úÖ          ‚úÖ            ‚úÖ Pr√™t
# ... (7 datasets au total)

# Validation avec d√©tails par dataset
python metadata_manager.py validate --verbose

# G√©n√©ration guid√©e pour nouveau dataset
python metadata_manager.py generate mon_dataset --domain education

# Diagnostic des champs manquants
python metadata_manager.py check-fills --all
----

== Syst√®me de Validation

=== Niveaux de Validation

==== Niveau 1 : M√©tadonn√©es
* Structure YAML valide
* Champs obligatoires pr√©sents
* Types de donn√©es coh√©rents

==== Niveau 2 : Fichiers
* Existence des fichiers CSV
* Int√©grit√© des donn√©es
* Coh√©rence avec m√©tadonn√©es

==== Niveau 3 : Base de Donn√©es
* Insertion r√©ussie
* Contraintes respect√©es
* Relations valides

==== Niveau 4 : Stockage
* Upload r√©ussi vers MinIO/Azure
* Accessibilit√© des fichiers
* Checksums valid√©s

=== Rapports de Validation

Le syst√®me g√©n√®re des rapports d√©taill√©s :

[source,json]
----
{
  "validation_summary": {
    "total_datasets": 7,
    "passed": 7,
    "failed": 0,
    "warnings": 2
  },
  "details": [
    {
      "dataset": "student-performance",
      "status": "passed",
      "checks": {
        "metadata": "‚úì",
        "files": "‚úì", 
        "database": "‚úì",
        "storage": "‚úì"
      }
    }
  ]
}
----

== Exemples Concrets

=== Exemple 1 : Import Simple

[source,bash]
----
# 1. Aller dans le dossier Kaggle
cd datasets/kaggle-import

# 2. Tester la configuration
make test-auth
make check-config

# 3. Importer un dataset sp√©cifique
make import-dataset DATASET=student_performance

# 4. V√©rifier le statut
make status
----

=== Exemple 2 : Import Multiple

[source,bash]
----
# Import de tous les petits datasets
make import-small

# Ou import de tous les datasets
make import-all

# Suivre les logs
tail -f kaggle_import.log
----

=== Exemple 3 : Ajouter un Nouveau Dataset (Proc√©dure V2)

[source,bash]
----
# 1. Ajouter dans kaggle_datasets_config.yaml
# nouveau_dataset:
#   kaggle_ref: "username/dataset-name"
#   domain: "education" 
#   description: "Description du dataset"
#   ml_task: "classification"
#   target_column: "target"

# 2. üÜï G√©n√©rer les m√©tadonn√©es sp√©cifiques depuis template
cd datasets/kaggle-import
python metadata_manager.py generate nouveau_dataset \
  --domain education \
  --kaggle-ref username/dataset-name

# 3. üÜï √âditer le fichier JSON g√©n√©r√© pour personnaliser
# vim enriched_metadata/datasets/nouveau_dataset.json

# 4. üÜï Valider les m√©tadonn√©es
python metadata_manager.py validate nouveau_dataset

# 5. Importer avec m√©tadonn√©es enrichies
make import-dataset DATASET=nouveau_dataset

# 6. üÜï V√©rifier l'int√©gration compl√®te  
python metadata_manager.py list
----

[IMPORTANT]
====
**Proc√©dure V2** : Il faut maintenant **g√©n√©rer et valider les m√©tadonn√©es JSON** avant l'import. Voir xref:enriched-metadata-system.adoc[Guide Complet] pour la proc√©dure d√©taill√©e.
====

== Configuration Avanc√©e

=== Variables d'Environnement

[source,bash]
----
# Configuration MinIO (d√©veloppement)
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=IBIS-X-datasets

# Configuration Azure (production)
AZURE_STORAGE_CONNECTION_STRING=...
AZURE_CONTAINER_NAME=datasets

# Configuration base de donn√©es
DATABASE_URL=postgresql://user:pass@localhost:5432/IBIS-Xdb
----

=== Optimisations Performance

==== Conversion Parquet

* **Compression** : Snappy par d√©faut (bon ratio vitesse/taille)
* **Partitioning** : Par domaine pour acc√®s optimis√©
* **Schema Evolution** : Support des changements de structure

==== Upload Parall√®le

* **Threads** : 4 uploads simultan√©s par d√©faut
* **Retry Logic** : 3 tentatives avec backoff exponentiel
* **Checksum** : Validation MD5 automatique

=== Gestion des Erreurs

==== Types d'Erreurs Communes

1. **Fichiers corrompus** : Validation CSV √©choue
2. **M√©tadonn√©es invalides** : Structure YAML incorrecte
3. **Connexion stockage** : MinIO/Azure inaccessible
4. **Contraintes DB** : Violation des r√®gles m√©tier

==== Strat√©gies de R√©cup√©ration

* **Rollback automatique** : Annulation en cas d'√©chec partiel
* **Mode d√©grad√©** : Import sans upload si stockage indisponible
* **Logs d√©taill√©s** : Tra√ßabilit√© compl√®te des op√©rations

== Bonnes Pratiques

=== Organisation des Datasets

1. **Nommage coh√©rent** : `snake_case` pour fichiers et dossiers
2. **Structure uniforme** : M√™me organisation pour tous les domaines
3. **Documentation** : README.md dans chaque dossier de domaine

=== M√©tadonn√©es de Qualit√©

1. **Descriptions claires** : Explications compr√©hensibles
2. **Tags pertinents** : Facilitent la recherche
3. **Sources cit√©es** : Tra√ßabilit√© des donn√©es

=== Monitoring et Maintenance

1. **Validation r√©guli√®re** : `make validate-all` hebdomadaire
2. **Backup m√©tadonn√©es** : Sauvegarde avant gros changements
3. **Monitoring espace** : Surveillance stockage objets

== Int√©gration avec IBIS-X

=== Backend Integration

Le backend lit exclusivement depuis le stockage d'objets :

[source,python]
----
# common/storage_client.py
async def get_dataset(dataset_id: str):
    # Lecture depuis MinIO/Azure uniquement
    return await storage_client.download_parquet(dataset_id)
----

=== Frontend Integration

Interface utilisateur pour parcourir les datasets import√©s :

* **Catalogue** : Liste tous les datasets disponibles
* **Filtrage** : Par domaine, t√¢che ML, taille
* **Pr√©visualisation** : √âchantillon des donn√©es
* **Statistiques** : M√©triques automatiques

== D√©pannage

=== Probl√®mes Courants

==== Import √âchoue
[source,bash]
----
# V√©rifier les logs
tail -f kaggle_import.log

# Tester l'authentification Kaggle
make test-auth

# Tester la connexion stockage
make test-services
----

==== Dataset Introuvable
[source,bash]
----
# V√©rifier la r√©f√©rence Kaggle
kaggle datasets list -s "nom_dataset"

# Lister les datasets configur√©s
make list-datasets
----

==== Cache Probl√©matique
[source,bash]
----
# Nettoyer le cache
make clean

# Forcer le re-t√©l√©chargement
make force-refresh
----

=== Support et Maintenance

* **Logs centralis√©s** : `kaggle_import.log`
* **Configuration** : `kaggle_datasets_config.yaml`
* **Documentation** : README.md complet dans `datasets/kaggle-import/`

== Migration vers le Syst√®me V2

=== üöÄ Nouveaut√©s R√©volutionnaires  

[cols="2,1,1", options="header"]
|===
|Fonctionnalit√© |Ancien Syst√®me |Syst√®me V2

|M√©tadonn√©es par dataset
|6 champs g√©n√©riques
|**39+ champs sp√©cifiques**

|Pr√©cision des donn√©es
|Templates par domaine
|**Donn√©es r√©elles de la BDD**

|Gestion m√©tadonn√©es
|Complexe (YAML partag√©s)
|**Simple (JSON isol√©s)**

|Validation
|Manuelle
|**CLI automatique complet**

|Ajout nouveau dataset
|30+ min (modification templates)
|**2 min (g√©n√©ration JSON)**

|Tra√ßabilit√©
|Limit√©e
|**Compl√®te avec Git**
|===

=== üìã Checklist Migration

Si vous migrez depuis l'ancien syst√®me :

. ‚úÖ **V√©rifiez** que tous vos datasets ont leurs fichiers JSON dans `enriched_metadata/datasets/`
. ‚úÖ **Testez** la validation : `python metadata_manager.py validate`
. ‚úÖ **Supprimez** les fichiers obsol√®tes : `templates/ethical_defaults.yaml`, `kaggle_metadata_mapper.py`
. ‚úÖ **Mettez √† jour** vos scripts pour utiliser `KaggleMetadataMapperV2`
. ‚úÖ **Documentez** vos nouveaux workflows avec le CLI `metadata_manager.py`

== Conclusion

Le syst√®me d'importation de datasets en batch d'IBIS-X **V2** r√©volutionne la gestion des m√©tadonn√©es avec des **donn√©es sp√©cifiques et r√©elles** pour chaque dataset. Avec ses 39+ champs enrichis, son CLI complet, et sa validation automatique, il garantit une qualit√© de donn√©es industrielle.

**üéØ B√©n√©fices concrets** :
* **‚úÖ M√©tadonn√©es pr√©cises** : Fini les valeurs g√©n√©riques
* **‚úÖ Maintenance simplifi√©e** : Un fichier JSON par dataset  
* **‚úÖ Validation robuste** : CLI complet avec diagnostics
* **‚úÖ √âvolutivit√©** : Ajout de nouveaux datasets en 2 minutes
* **‚úÖ Tra√ßabilit√©** : Historique complet via Git

[IMPORTANT]
====
**Guide Complet** : Pour une proc√©dure d√©taill√©e d'ajout de nouveau dataset, consultez xref:enriched-metadata-system.adoc[Syst√®me de M√©tadonn√©es Enrichies].
====

Pour toute question ou probl√®me, consultez les logs d√©taill√©s et utilisez le nouveau CLI `metadata_manager.py` pour diagnostiquer les issues. 
