= Auto-Initialisation des Vrais Datasets
:description: Guide complet pour l'initialisation automatique des vrais datasets dans EXAI
:keywords: EXAI, datasets, initialisation, automation, production, CSV, Parquet

== Vue d'Ensemble

Le syst√®me EXAI propose maintenant une **auto-initialisation automatis√©e** des vrais datasets, √©liminant le besoin de commandes manuelles pour int√©grer de vraies donn√©es dans l'application.

Cette fonctionnalit√© permet :

* ‚úÖ **D√©veloppement local** : Initialisation automatique avec `make dev-with-data`
* ‚úÖ **Production Azure** : Initialisation automatique au d√©marrage avec la variable `AUTO_INIT_DATA=true`
* ‚úÖ **Vrais datasets** : Utilisation de fichiers CSV r√©els convertis automatiquement en Parquet
* ‚úÖ **Performance optimis√©e** : Gains de 10-50x gr√¢ce au format Parquet
* ‚úÖ **D√©tection intelligente** : √âvite la re-initialisation si les donn√©es existent d√©j√†

== Architecture de l'Auto-Initialisation

=== Composants

[source,mermaid]
----
graph TD
    A[FastAPI Startup] --> B{AUTO_INIT_DATA=true?}
    B -->|Non| C[Skip Auto-Init]
    B -->|Oui| D[V√©rifier Datasets Existants]
    D -->|Existent| C
    D -->|Vides| E[Lance Auto-Init en Background]
    E --> F[Lecture CSV Source]
    F --> G[Conversion Parquet]
    G --> H[Upload Stockage Objets]
    H --> I[MAJ M√©tadonn√©es BDD]
    
    J[make dev-with-data] --> K[kubectl exec init_datasets.py social]
    K --> F
----

=== Fichiers Impliqu√©s

[cols="2,3,3"]
|===
|Fichier |R√¥le |Description

|`service-selection/app/auto_init.py`
|Module d'auto-initialisation
|Logique principale pour d√©tecter et lancer l'initialisation

|`service-selection/app/main.py`
|Event handler FastAPI
|Hook de startup qui d√©clenche l'auto-init

|`service-selection/datasets/Students Social Media Addiction.csv`
|Dataset source
|Fichier CSV avec 705 lignes de vraies donn√©es

|`service-selection/scripts/init_datasets.py`
|Script d'initialisation
|Fonctions `upload_real_dataset_file()` et `init_social_media_addiction_dataset()`

|`k8s/overlays/azure/service-selection-auto-init-patch.yaml`
|Configuration production
|Active `AUTO_INIT_DATA=true` en production Azure

|`Makefile`
|Orchestration locale
|R√®gles `dev-with-data`, `init-data`, et `init-data-job`
|===

== Utilisation

=== D√©veloppement Local

Pour un d√©marrage complet avec vrais datasets automatiques :

[source,bash]
----
make dev-with-data
----

Cette commande :

1. ‚úÖ V√©rifie les pr√©requis (Docker, Minikube, kubectl, Skaffold)
2. ‚úÖ Met √† jour les secrets Kubernetes
3. ‚úÖ D√©marre Minikube avec configuration optimale
4. ‚úÖ Cr√©e le namespace `exai`
5. ‚úÖ Configure l'environnement Docker pour Minikube
6. ‚úÖ D√©ploie l'application avec Skaffold
7. ‚úÖ Lance les migrations de base de donn√©es
8. ‚úÖ **Initialise automatiquement les vrais datasets** (`social`)
9. ‚úÖ Affiche les logs en temps r√©el

=== Production Azure

En production, l'auto-initialisation se d√©clenche automatiquement gr√¢ce au patch Kustomize :

[source,yaml]
----
# k8s/overlays/azure/service-selection-auto-init-patch.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-selection-deployment
spec:
  template:
    spec:
      containers:
      - name: service-selection
        env:
        - name: AUTO_INIT_DATA
          value: "true"
----

L'application d√©tecte automatiquement au d√©marrage :

1. ‚úÖ Variable `AUTO_INIT_DATA=true` activ√©e
2. ‚úÖ Base de donn√©es vide (aucun dataset existant)
3. ‚úÖ Lance l'initialisation en arri√®re-plan sans bloquer l'API
4. ‚úÖ Log du statut et des r√©sultats

=== Commandes Manuelles (Optionnelles)

Si vous souhaitez d√©clencher manuellement :

[source,bash]
----
# Via exec dans un pod existant (d√©veloppement)
make init-data

# Via job Kubernetes (production)
make init-data-job

# Directement avec le script
kubectl exec -n exai deployment/service-selection -- python scripts/init_datasets.py social
----

== Configuration

=== Variables d'Environnement

[cols="2,2,3,1"]
|===
|Variable |Valeurs |Description |Requis

|`AUTO_INIT_DATA`
|`true`, `false`, `1`, `0`, `yes`, `no`, `on`, `off`
|Active l'auto-initialisation au d√©marrage
|‚úÖ

|`DATABASE_URL`
|`postgresql://user:pass@host:port/db`
|Connexion √† PostgreSQL
|‚úÖ

|`MINIO_ENDPOINT`
|URL du serveur MinIO
|Stockage d'objets primaire
|‚úÖ

|`MINIO_ACCESS_KEY`
|Cl√© d'acc√®s MinIO
|Authentification MinIO
|‚úÖ

|`MINIO_SECRET_KEY`
|Cl√© secr√®te MinIO
|Authentification MinIO
|‚úÖ

|`AZURE_STORAGE_ACCOUNT_NAME`
|Nom du compte Azure Storage
|Stockage de fallback
|‚óØ

|`AZURE_STORAGE_ACCOUNT_KEY`
|Cl√© du compte Azure Storage
|Authentification Azure
|‚óØ
|===

=== Logique de D√©tection

L'auto-initialisation ne se d√©clenche que si **toutes** ces conditions sont remplies :

1. ‚úÖ `AUTO_INIT_DATA=true` 
2. ‚úÖ Base de donn√©es accessible
3. ‚úÖ Aucun dataset existant (`SELECT COUNT(*) FROM datasets = 0`)
4. ‚úÖ Fichier CSV source disponible
5. ‚úÖ Configuration stockage d'objets valide

== Datasets Support√©s

=== Students Social Media Addiction

**Source** : `service-selection/datasets/Students Social Media Addiction.csv`

**Caract√©ristiques** :
* üìä **705 lignes** de donn√©es r√©elles
* üìã **13 colonnes** : Student_ID, Age, Gender, Academic_Level, Country, Avg_Daily_Usage_Hours, Most_Used_Platform, Affects_Academic_Performance, Sleep_Hours_Per_Night, Mental_Health_Score, Relationship_Status, Conflicts_Over_Social_Media, Addicted_Score
* üìà **Domaine** : Impact des r√©seaux sociaux sur les √©tudiants
* üóÉÔ∏è **Format final** : Parquet (conversion automatique)
* ‚ö° **Performance** : 10-50x plus rapide que CSV
* üè∑Ô∏è **Type** : VRAI DATASET (marquage automatique)

=== Format de Conversion

[source,python]
----
# Conversion automatique CSV ‚Üí Parquet
def convert_to_parquet(file_content: bytes, filename: str) -> bytes:
    """
    Convertit un fichier CSV en format Parquet.
    Gains de performance : 10-50x en lecture
    Compression : ~60% de r√©duction de taille
    """
    df = pd.read_csv(io.BytesIO(file_content))
    parquet_buffer = io.BytesIO()
    df.to_parquet(parquet_buffer, index=False)
    return parquet_buffer.getvalue()
----

== Surveillance et Logs

=== Logs d'Initialisation

L'auto-initialisation produit des logs d√©taill√©s :

[source,logs]
----
2025-01-XX XX:XX:XX - auto_init - INFO - AUTO_INIT_DATA activ√© - analyse des conditions
2025-01-XX XX:XX:XX - auto_init - INFO - Nombre de datasets existants: 0
2025-01-XX XX:XX:XX - auto_init - INFO - Conditions remplies pour l'auto-initialisation des vrais datasets
2025-01-XX XX:XX:XX - auto_init - INFO - D√©marrage de l'auto-initialisation des vrais datasets...
2025-01-XX XX:XX:XX - auto_init - INFO - Auto-initialisation des vrais datasets termin√©e avec succ√®s
----

=== Monitoring de Production

En production Azure, surveillez :

[source,bash]
----
# Logs du service-selection
kubectl logs -f deployment/service-selection -n exai

# Statut du job d'initialisation (si utilis√©)
kubectl get jobs -n exai
kubectl describe job service-selection-data-init-job -n exai

# V√©rification des datasets cr√©√©s
kubectl exec -n exai deployment/service-selection -- python -c "
import sys; sys.path.append('/app')
from app.database import SessionLocal
from app.models import Dataset
with SessionLocal() as db:
    print(f'Datasets: {db.query(Dataset).count()}')
"
----

== D√©pannage

=== Probl√®mes Courants

[cols="2,3,3"]
|===
|Probl√®me |Sympt√¥me |Solution

|Auto-init d√©sactiv√©e
|Logs : "AUTO_INIT_DATA non activ√©"
|V√©rifier `AUTO_INIT_DATA=true` dans le d√©ploiement

|Donn√©es d√©j√† pr√©sentes
|Logs : "Donn√©es d√©j√† initialis√©es - skip"
|Normal - supprimez les datasets pour re-initialiser

|Erreur de connexion BDD
|Logs : "Impossible de v√©rifier l'√©tat des datasets"
|V√©rifier `DATABASE_URL` et connectivit√© PostgreSQL

|Erreur stockage objets
|Logs : "Erreur lors de l'upload"
|V√©rifier les credentials MinIO/Azure et connectivity

|Script d'init √©choue
|Exit code != 0
|V√©rifier les logs d√©taill√©s et les d√©pendances Python
|===

=== Commandes de Diagnostic

[source,bash]
----
# V√©rifier les variables d'environnement
kubectl exec -n exai deployment/service-selection -- env | grep -E "(AUTO_INIT|DATABASE|MINIO|AZURE)"

# Tester la connectivit√© base de donn√©es
kubectl exec -n exai deployment/service-selection -- python -c "
from app.database import SessionLocal
try:
    with SessionLocal() as db:
        db.execute('SELECT 1')
    print('‚úÖ Connexion BDD OK')
except Exception as e:
    print(f'‚ùå Erreur BDD: {e}')
"

# V√©rifier l'existence du fichier CSV source
kubectl exec -n exai deployment/service-selection -- ls -la datasets/

# Tester l'initialisation manuellement
kubectl exec -n exai deployment/service-selection -- python scripts/init_datasets.py social
----

== Bonnes Pratiques

=== S√©curit√©

* üîí **Secrets** : Utilisez toujours des Kubernetes Secrets pour les credentials
* üîë **Permissions** : Limitez les acc√®s au stockage d'objets aux services n√©cessaires
* üõ°Ô∏è **R√©seau** : S√©curisez les communications inter-services

=== Performance

* ‚ö° **Format Parquet** : Conversion automatique pour des performances optimales
* üìä **Monitoring** : Surveillez les m√©triques de performance post-initialisation
* üíæ **Stockage** : Utilisez des classes de stockage optimis√©es en production

=== Maintenance

* üìù **Logs** : Conservez les logs d'initialisation pour l'audit
* üîÑ **Backup** : Sauvegardez les datasets sources avant d√©ploiement
* üß™ **Test** : Testez l'auto-initialisation sur un environnement de staging

== √âvolution Future

=== Fonctionnalit√©s Pr√©vues

* üîÑ **Multi-datasets** : Support de plusieurs datasets r√©els simultan√©ment
* üìä **Validation** : Validation automatique de la qualit√© des donn√©es
* üè∑Ô∏è **M√©tadonn√©es** : Enrichissement automatique des m√©tadonn√©es
* üîÑ **Synchronisation** : Synchronisation automatique avec sources externes
* üìà **Analytics** : M√©triques d'utilisation des datasets initialis√©s

=== Architecture Extensible

Le syst√®me est con√ßu pour √™tre facilement extensible :

[source,python]
----
# Ajouter un nouveau dataset
def init_new_dataset():
    """
    Template pour ajouter un nouveau dataset r√©el.
    1. Placer le CSV dans datasets/
    2. Cr√©er la fonction d'initialisation
    3. Ajouter au script init_datasets.py
    """
    return upload_real_dataset_file(
        csv_path="datasets/nouveau_dataset.csv",
        dataset_name="Nouveau Dataset",
        description="Description du nouveau dataset VRAI",
        # ... autres m√©tadonn√©es
    )
---- 