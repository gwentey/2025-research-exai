= Guide de Gestion des Datasets EXAI
:description: Documentation compl√®te sur la gestion des datasets dans EXAI - initialisation, ajout de nouveaux datasets, scripts disponibles
:keywords: EXAI, datasets, gestion, initialisation, CSV, Parquet, MinIO, stockage d'objets

== Introduction

Ce guide d√©taille le syst√®me complet de gestion des datasets dans la plateforme EXAI. Il couvre l'architecture du syst√®me, les processus d'initialisation automatique et manuelle, ainsi que les proc√©dures pour ajouter de nouveaux datasets.

[NOTE]
====
La plateforme EXAI utilise un syst√®me hybride de stockage d'objets (MinIO/Azure) avec conversion automatique CSV ‚Üí Parquet pour des performances optimales.
====

== Architecture du Syst√®me de Gestion des Datasets

=== Composants Principaux

[source,mermaid]
----
flowchart TD
    A[Scripts d'Initialisation] --> B[Base de Donn√©es PostgreSQL]
    A --> C[Stockage d'Objets MinIO/Azure]
    D[Auto-Initialisation] --> A
    E[API FastAPI] --> B
    E --> C
    F[Frontend Angular] --> E
    
    subgraph "M√©tadonn√©es"
        B
    end
    
    subgraph "Fichiers de Donn√©es"
        C
    end
    
    subgraph "Initialisation"
        A
        D
    end
----

=== Structure des Donn√©es

La gestion des datasets repose sur trois entit√©s principales :

[cols="1,3,2"]
|===
|Entit√© |Description |Relation

|`Dataset`
|Informations g√©n√©rales sur un dataset (nom, objectif, domaine...)
|Un dataset peut avoir plusieurs fichiers

|`DatasetFile`
|M√©tadonn√©es d'un fichier sp√©cifique (format, taille, nombre de lignes...)
|Un fichier appartient √† un dataset et peut avoir plusieurs colonnes

|`FileColumn`
|Informations sur une colonne d'un fichier (nom, type, description...)
|Une colonne appartient √† un fichier
|===

== Initialisation des Datasets

EXAI propose deux m√©thodes d'initialisation des datasets :

=== 1. Initialisation Automatique (Auto-Init)

L'auto-initialisation se d√©clenche au d√©marrage du service si la variable d'environnement `AUTO_INIT_DATA=true` est d√©finie.

[source,bash]
----
# Activer l'auto-initialisation
export AUTO_INIT_DATA=true
----

Le processus d'auto-initialisation :

1. V√©rifie si `AUTO_INIT_DATA=true`
2. V√©rifie si la base de donn√©es est vide (aucun dataset existant)
3. Ex√©cute le script d'initialisation en arri√®re-plan
4. Convertit les fichiers CSV en Parquet
5. Uploade les fichiers vers le stockage d'objets
6. Met √† jour les m√©tadonn√©es dans la base de donn√©es

[source,python]
----
# Extrait de service-selection/app/auto_init.py
async def auto_init_startup():
    """Fonction de startup pour l'auto-initialisation."""
    if not should_auto_init():
        logger.info("AUTO_INIT_DATA non activ√© - pas d'auto-initialisation")
        return
    
    if check_data_already_initialized():
        logger.info("üõ°Ô∏è  Donn√©es d√©j√† initialis√©es - skip auto-initialisation")
        return
        
    # Lancement de l'initialisation...
----

=== 2. Initialisation Manuelle

Pour initialiser manuellement les datasets :

[source,bash]
----
# Initialiser un dataset sp√©cifique
python scripts/init_datasets.py [ednet|oulad|students|social|academic|depression|stress]

# Initialiser tous les datasets
python scripts/init_datasets.py all
----

== Datasets Disponibles

EXAI inclut actuellement 7 datasets pr√™ts √† l'emploi :

[cols="1,1,2,1,1"]
|===
|Nom |Ann√©e |Description |Lignes |Domaine

|EdNet (Riiid Answer Correctness)
|2020
|Pr√©diction de r√©ussite aux exercices √©ducatifs
|131M
|√âducation

|OULAD
|2014
|Analyse de l'apprentissage en ligne universitaire
|32,593
|√âducation

|Students Performance in Exams
|2018
|Impact des facteurs socio-√©ducatifs sur les scores
|1,000
|√âducation

|Students' Social Media Addiction
|2025
|Usage des r√©seaux sociaux et impact acad√©mique
|705
|√âducation/Social

|Student Academic Performance Dataset
|2025
|Analyse des performances avec facteurs d√©mographiques
|1,000
|√âducation

|Student Depression Dataset
|2024
|Tendances et pr√©dicteurs de d√©pression chez les √©tudiants
|28,000
|Sant√© mentale

|Student Stress Factors
|2023
|Facteurs de stress chez les √©tudiants en ing√©nierie
|520
|Sant√© mentale
|===

== Ajout d'un Nouveau Dataset

Pour ajouter un nouveau dataset √† EXAI, suivez ces √©tapes :

=== 1. Pr√©paration du Dataset

1. Placez votre fichier CSV dans le r√©pertoire `service-selection/datasets/`
2. Assurez-vous que le fichier est correctement format√© :
   * Encodage UTF-8
   * S√©parateur virgule (`,`)
   * Premi√®re ligne = en-t√™tes de colonnes
   * Pas de cellules vides si possible

=== 2. Cr√©ation de la Fonction d'Initialisation

Ajoutez une nouvelle fonction dans `service-selection/scripts/init_datasets.py` :

[source,python]
----
def init_my_new_dataset():
    """
    Initialise le dataset My New Dataset avec son fichier et colonnes.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            # === SUPPRESSION DES DONN√âES EXISTANTES ===
            print("üóëÔ∏è  Suppression des donn√©es existantes pour 'My New Dataset'...")
            
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == "My New Dataset"
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            # === CR√âATION DU DATASET ===
            print("üìä Cr√©ation du dataset My New Dataset...")
            
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer et uploader le fichier
            csv_file_path = "datasets/my_new_dataset.csv"
            storage_path, row_count, file_size = upload_real_dataset_file(
                dataset_id=dataset_id,
                csv_file_path=csv_file_path,
                filename_base="my_new_dataset"
            )
            
            # Cr√©er l'entr√©e Dataset
            dataset = Dataset(
                id=dataset_id,
                dataset_name="My New Dataset",
                year=2025,
                objective="Description de l'objectif du dataset",
                access="public",
                availability="online_download",
                domain=["domaine1", "domaine2"],
                storage_path=storage_path,
                instances_number=row_count,
                features_number=10,  # Nombre de colonnes
                # ... autres attributs ...
            )
            
            session.add(dataset)
            session.flush()
            
            # Cr√©er l'entr√©e DatasetFile
            dataset_file = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage="my_new_dataset.parquet",
                logical_role="main_data",
                format="parquet",
                mime_type="application/parquet",
                size_bytes=file_size,
                row_count=row_count,
                description="Description du fichier"
            )
            
            session.add(dataset_file)
            session.flush()
            
            # Cr√©er les entr√©es FileColumn
            columns_data = [
                {
                    'name': 'colonne1', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Description de la colonne 1', 'pos': 1, 'is_pk': True, 'is_null': False, 
                    'is_pii': False, 'examples': ['exemple1', 'exemple2', 'exemple3']
                },
                # ... autres colonnes ...
            ]
            
            for col_info in columns_data:
                file_column = FileColumn(
                    dataset_file_id=dataset_file.id,
                    column_name=col_info['name'],
                    data_type_original=col_info['type_orig'],
                    data_type_interpreted=col_info['type_interp'],
                    description=col_info['desc'],
                    is_primary_key_component=col_info['is_pk'],
                    is_nullable=col_info['is_null'],
                    is_pii=col_info['is_pii'],
                    example_values=col_info['examples'],
                    position=col_info['pos']
                )
                session.add(file_column)
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset 'My New Dataset' initialis√© avec succ√®s !")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
----

=== 3. Mise √† Jour de la Fonction `main()`

Ajoutez votre dataset √† la fonction `main()` dans le m√™me fichier :

[source,python]
----
def main():
    """Point d'entr√©e principal du script."""
    
    if len(sys.argv) > 1:
        dataset_name = sys.argv[1].lower()
        # ... code existant ...
        elif dataset_name == "mynewdataset":
            print("üìä Initialisation du dataset My New Dataset uniquement")
            try:
                init_my_new_dataset()
                print("\n‚úÖ Dataset My New Dataset initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation My New Dataset: {e}")
                sys.exit(1)
        elif dataset_name == "all":
            # ... code existant ...
            init_my_new_dataset()
            print("\n‚úÖ Dataset My New Dataset initialis√© avec succ√®s !")
            # ... code existant ...
----

=== 4. Test de l'Initialisation

Testez votre nouveau dataset :

[source,bash]
----
# Initialiser uniquement votre dataset
python scripts/init_datasets.py mynewdataset

# V√©rifier qu'il est inclus dans l'initialisation compl√®te
python scripts/init_datasets.py all
----

== Fonctions Utilitaires pour les Datasets

Le script `init_datasets.py` fournit plusieurs fonctions utilitaires pour faciliter la gestion des datasets :

=== `upload_real_dataset_file`

Convertit un fichier CSV en Parquet et l'uploade vers le stockage d'objets.

[source,python]
----
def upload_real_dataset_file(dataset_id: str, csv_file_path: str, filename_base: str = "dataset") -> tuple:
    """
    Lit un vrai fichier CSV, le convertit en Parquet et l'uploade vers le stockage d'objets.
    
    Args:
        dataset_id: UUID du dataset
        csv_file_path: Chemin vers le fichier CSV source
        filename_base: Nom de base pour le fichier (sans extension)
        
    Returns:
        tuple: (storage_path_prefix, row_count, file_size_bytes)
    """
----

=== `upload_sample_dataset`

G√©n√®re et uploade des donn√©es √©chantillons bas√©es sur une description de colonnes.

[source,python]
----
def upload_sample_dataset(dataset_id: str, sample_data_dict: dict, filename_base: str = "sample_data") -> str:
    """
    G√©n√®re et upload des donn√©es √©chantillons vers le stockage d'objets.
    
    Args:
        dataset_id: UUID du dataset
        sample_data_dict: Dictionnaire contenant les donn√©es √©chantillons
        filename_base: Nom de base pour le fichier (sans extension)
        
    Returns:
        storage_path: Pr√©fixe du dossier de stockage
    """
----

=== `upload_multiple_sample_files`

G√©n√®re et uploade plusieurs fichiers √©chantillons pour un m√™me dataset.

[source,python]
----
def upload_multiple_sample_files(dataset_id: str, files_data: list) -> str:
    """
    G√©n√®re et upload plusieurs fichiers √©chantillons vers le stockage d'objets.
    
    Args:
        dataset_id: UUID du dataset
        files_data: Liste des dictionnaires contenant les donn√©es pour chaque fichier
        
    Returns:
        storage_path: Pr√©fixe du dossier de stockage
    """
----

== D√©pannage

=== Probl√®mes Courants

[cols="1,2,2"]
|===
|Probl√®me |Sympt√¥mes |Solution

|Dataset non visible dans l'interface
|Dataset cr√©√© mais non affich√© dans l'UI
|V√©rifier que le storage_path est correctement d√©fini et que les fichiers sont bien upload√©s

|Erreur d'upload
|Message "‚ùå Erreur de stockage"
|V√©rifier les credentials MinIO/Azure et la connectivit√© au stockage d'objets

|Colonnes manquantes
|Dataset visible mais sans colonnes
|V√©rifier la cr√©ation des entr√©es FileColumn dans la fonction d'initialisation

|Doublons de datasets
|Plusieurs entr√©es identiques dans la liste
|Utiliser la v√©rification anti-doublons dans auto_init.py
|===

=== Commandes de Diagnostic

[source,bash]
----
# V√©rifier les datasets en base de donn√©es
kubectl exec -n exai deployment/service-selection -- python -c "
from app.database import SessionLocal
from app.models import Dataset
with SessionLocal() as db:
    datasets = db.query(Dataset).all()
    for ds in datasets:
        print(f'{ds.id}: {ds.dataset_name} ({ds.storage_path})')
"

# V√©rifier les fichiers dans MinIO
kubectl exec -n exai deployment/minio -- mc ls minio/exai-datasets/

# R√©initialiser un dataset sp√©cifique
kubectl exec -n exai deployment/service-selection -- python scripts/init_datasets.py social
----

== Bonnes Pratiques

=== Qualit√© des Donn√©es

* Nettoyez vos donn√©es avant de les ajouter (valeurs manquantes, doublons)
* Documentez chaque colonne avec une description pr√©cise
* Identifiez clairement les colonnes contenant des PII (Personally Identifiable Information)
* Fournissez des exemples de valeurs pour chaque colonne

=== Performance

* Utilisez le format Parquet pour les gros fichiers (conversion automatique)
* Divisez les tr√®s grands datasets en plusieurs fichiers si n√©cessaire
* Utilisez la fonction `upload_multiple_sample_files` pour les datasets multi-fichiers

=== S√©curit√©

* Marquez correctement les colonnes contenant des donn√©es sensibles (`is_pii=True`)
* Utilisez toujours les fonctions d'upload fournies qui g√®rent l'authentification
* Ne stockez pas de credentials dans le code source

== Ressources Additionnelles

* link:auto-dataset-initialization.adoc[Auto-Initialisation des Datasets]
* link:object-storage-implementation.adoc[Impl√©mentation du Stockage d'Objets]
* link:storage-setup-guide.adoc[Guide de Configuration du Stockage]

== Conclusion

Le syst√®me de gestion des datasets d'EXAI offre une solution robuste et performante pour initialiser, stocker et acc√©der aux datasets d'analyse. La combinaison du stockage d'objets et de la base de donn√©es relationnelle permet une gestion efficace des m√©tadonn√©es et des fichiers volumineux, tandis que la conversion automatique CSV ‚Üí Parquet optimise les performances d'acc√®s aux donn√©es. 