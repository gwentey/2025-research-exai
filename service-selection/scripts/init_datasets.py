#!/usr/bin/env python3
"""
Script d'initialisation pour ins√©rer les datasets dans la base de donn√©es normalis√©e.

Datasets support√©s:
- EdNet (Riiid Answer Correctness): Dataset de pr√©diction de r√©ussite aux exercices
- OULAD: Open University Learning Analytics Dataset pour l'analyse d'apprentissage en ligne
- Students Performance in Exams: Dataset sur l'impact des facteurs socio-√©ducatifs sur les scores aux examens
- Students' Social Media Addiction: Dataset sur l'usage des r√©seaux sociaux et impact acad√©mique/relationnel (VRAI DATASET)
- Student Academic Performance Dataset: Dataset d'analyse des performances acad√©miques avec facteurs d√©mographiques et comportementaux
- Student Depression Dataset: Dataset d'analyse des tendances et pr√©dicteurs de d√©pression chez les √©tudiants
- Student Stress Factors: Dataset sur les facteurs de stress chez les √©tudiants en ing√©nierie

Ce script peut √™tre ex√©cut√© pour r√©ins√©rer les donn√©es de test √† chaque fois.

Usage:
    cd service-selection
    python scripts/init_datasets.py [ednet|oulad|students|social|academic|depression|stress|all]
    
    Exemples:
    python scripts/init_datasets.py ednet       # Initialise seulement EdNet
    python scripts/init_datasets.py oulad       # Initialise seulement OULAD
    python scripts/init_datasets.py students    # Initialise seulement Students Performance
    python scripts/init_datasets.py social      # Initialise seulement Social Media Addiction (VRAI DATASET)
    python scripts/init_datasets.py academic    # Initialise seulement Student Academic Performance
    python scripts/init_datasets.py depression  # Initialise seulement Student Depression
    python scripts/init_datasets.py stress      # Initialise seulement Student Stress Factors
    python scripts/init_datasets.py all         # Initialise tous les datasets
    python scripts/init_datasets.py             # Initialise tous les datasets (d√©faut)

Requirements:
    - Base de donn√©es PostgreSQL accessible
    - Variable d'environnement DATABASE_URL d√©finie
    - Migrations Alembic appliqu√©es
"""

import os
import sys
from datetime import datetime, timezone
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
import pandas as pd
import uuid
import io
import json

# Ajouter les r√©pertoires n√©cessaires au path pour importer nos mod√®les
script_dir = os.path.dirname(os.path.abspath(__file__))
service_dir = os.path.dirname(script_dir)

# D√©tection automatique du contexte (local vs Docker)
if os.path.exists(os.path.join(service_dir, 'app')):
    # Contexte local : service-selection/scripts/
    app_dir = os.path.join(service_dir, 'app')
    sys.path.insert(0, app_dir)
    context = "local"
else:
    # Contexte Docker : /app/scripts/
    # Les modules sont directement dans /app/
    sys.path.insert(0, service_dir)
    context = "docker"

try:
    from models import Base, Dataset, DatasetFile, FileColumn
    from database import DATABASE_URL
    # Import du client de stockage commun
    sys.path.append(os.path.join(service_dir, '..'))
    from common.storage_client import get_storage_client, StorageClientError
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    print(f"üîç Contexte d√©tect√©: {context}")
    print(f"üìÅ R√©pertoire script: {script_dir}")
    print(f"üìÅ R√©pertoire service: {service_dir}")
    if context == "local":
        print("üí° Assurez-vous d'ex√©cuter ce script depuis le r√©pertoire service-selection/")
        print("   cd service-selection && python scripts/init_ednet_dataset.py")
    else:
        print("üí° Probl√®me d'import dans le conteneur Docker")
        print("   V√©rifiez que les modules sont bien copi√©s dans l'image")
    sys.exit(1)

def upload_real_dataset_file(dataset_id: str, csv_file_path: str, filename_base: str = "dataset") -> tuple:
    """
    Lit un vrai fichier CSV, le convertit en Parquet et l'uploade vers le stockage d'objets.
    
    Args:
        dataset_id: UUID du dataset
        csv_file_path: Chemin vers le fichier CSV source
        filename_base: Nom de base pour le fichier (sans extension)
        
    Returns:
        tuple: (storage_path_prefix, row_count, file_size_bytes)
    """
    try:
        storage_client = get_storage_client()
        storage_path_prefix = f"exai-datasets/{dataset_id}/"
        
        # V√©rifier que le fichier CSV existe
        if not os.path.exists(csv_file_path):
            raise FileNotFoundError(f"Fichier CSV non trouv√©: {csv_file_path}")
        
        print(f"üìñ Lecture du fichier CSV: {csv_file_path}")
        
        # Lire le fichier CSV
        df = pd.read_csv(csv_file_path)
        
        print(f"‚úÖ Fichier CSV lu avec succ√®s: {len(df)} lignes, {len(df.columns)} colonnes")
        print(f"üìä Colonnes: {', '.join(df.columns.tolist())}")
        
        # Convertir en Parquet
        parquet_buffer = io.BytesIO()
        df.to_parquet(parquet_buffer, index=False)
        parquet_buffer.seek(0)
        parquet_content = parquet_buffer.read()
        parquet_size = len(parquet_content)
        
        print(f"üîß Conversion en Parquet r√©ussie: {parquet_size} bytes")
        
        # Upload vers le stockage
        parquet_filename = f"{filename_base}.parquet"
        object_path = f"{storage_path_prefix}{parquet_filename}"
        storage_client.upload_file(parquet_content, object_path)
        
        print(f"‚òÅÔ∏è  Fichier upload√© vers le stockage: {object_path}")
        
        return storage_path_prefix, len(df), parquet_size
        
    except StorageClientError as e:
        print(f"‚ùå Erreur de stockage pour dataset {dataset_id}: {str(e)}")
        # Ne pas √©chouer le script entier pour des erreurs de stockage
        # Retourner des valeurs par d√©faut bas√©es sur un simple parsing CSV
        try:
            df = pd.read_csv(csv_file_path)
            return f"exai-datasets/{dataset_id}/", len(df), 50000  # Taille estim√©e
        except:
            return f"exai-datasets/{dataset_id}/", 705, 50000  # Valeurs par d√©faut
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors du traitement du fichier {csv_file_path}: {str(e)}")
        return f"exai-datasets/{dataset_id}/", 705, 50000  # Valeurs par d√©faut

def upload_sample_dataset(dataset_id: str, sample_data_dict: dict, filename_base: str = "sample_data") -> str:
    """
    G√©n√®re et upload des donn√©es √©chantillons vers le stockage d'objets.
    
    Args:
        dataset_id: UUID du dataset
        sample_data_dict: Dictionnaire contenant les donn√©es √©chantillons
        filename_base: Nom de base pour le fichier (sans extension)
        
    Returns:
        storage_path: Pr√©fixe du dossier de stockage (ex: 'exai-datasets/uuid/')
    """
    try:
        storage_client = get_storage_client()
        storage_path_prefix = f"exai-datasets/{dataset_id}/"
        
        # Cr√©er un DataFrame √©chantillon bas√© sur les m√©tadonn√©es
        if isinstance(sample_data_dict, dict) and 'columns' in sample_data_dict:
            # Structure avec colonnes d√©finies
            data = {}
            for col_info in sample_data_dict['columns']:
                col_name = col_info['name']
                col_type = col_info.get('type', 'string')
                
                # G√©n√©rer des donn√©es √©chantillons bas√©es sur le type
                if col_type in ['integer', 'int', 'numeric']:
                    data[col_name] = [i + 1 for i in range(100)]
                elif col_type in ['float', 'decimal', 'number']:
                    data[col_name] = [round((i + 1) * 0.85, 2) for i in range(100)]
                elif col_type in ['boolean', 'bool']:
                    data[col_name] = [i % 2 == 0 for i in range(100)]
                else:  # string, text, categorical
                    data[col_name] = [f"sample_value_{i+1}" for i in range(100)]
            
            df = pd.DataFrame(data)
        else:
            # Fallback : cr√©er un DataFrame simple avec quelques colonnes g√©n√©riques
            df = pd.DataFrame({
                'id': range(1, 101),
                'feature_1': [f"value_{i}" for i in range(1, 101)],
                'feature_2': [round(i * 0.75, 2) for i in range(1, 101)],
                'target': [i % 2 for i in range(1, 101)]
            })
        
        # Convertir en Parquet
        parquet_buffer = io.BytesIO()
        df.to_parquet(parquet_buffer, index=False)
        parquet_buffer.seek(0)
        parquet_content = parquet_buffer.read()
        
        # Upload vers le stockage
        parquet_filename = f"{filename_base}.parquet"
        object_path = f"{storage_path_prefix}{parquet_filename}"
        storage_client.upload_file(parquet_content, object_path)
        
        print(f"‚úÖ Fichier √©chantillon upload√©: {object_path}")
        return storage_path_prefix
        
    except StorageClientError as e:
        print(f"‚ùå Erreur de stockage pour dataset {dataset_id}: {str(e)}")
        # Ne pas √©chouer le script entier pour des erreurs de stockage
        return f"exai-datasets/{dataset_id}/"  # Retourner le path attendu
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors de la cr√©ation des donn√©es √©chantillons pour {dataset_id}: {str(e)}")
        return f"exai-datasets/{dataset_id}/"  # Retourner le path attendu


def upload_multiple_sample_files(dataset_id: str, files_data: list) -> str:
    """
    G√©n√®re et upload plusieurs fichiers √©chantillons vers le stockage d'objets.
    
    Args:
        dataset_id: UUID du dataset
        files_data: Liste des dictionnaires contenant les donn√©es pour chaque fichier
                   [{'filename': 'file1.csv', 'columns': [...]}, {'filename': 'file2.csv', 'columns': [...]}]
        
    Returns:
        storage_path: Pr√©fixe du dossier de stockage (ex: 'exai-datasets/uuid/')
    """
    try:
        storage_client = get_storage_client()
        storage_path_prefix = f"exai-datasets/{dataset_id}/"
        
        for file_info in files_data:
            filename = file_info['filename']
            columns_data = file_info['columns']
            
            # Cr√©er un DataFrame √©chantillon bas√© sur les m√©tadonn√©es
            data = {}
            for col_info in columns_data:
                col_name = col_info['name']
                col_type = col_info.get('type', 'string')
                
                # G√©n√©rer des donn√©es √©chantillons bas√©es sur le type
                if col_type in ['integer', 'int', 'numeric']:
                    data[col_name] = [i + 1 for i in range(100)]
                elif col_type in ['float', 'decimal', 'number']:
                    data[col_name] = [round((i + 1) * 0.85, 2) for i in range(100)]
                elif col_type in ['boolean', 'bool']:
                    data[col_name] = [i % 2 == 0 for i in range(100)]
                else:  # string, text, categorical
                    data[col_name] = [f"sample_value_{i+1}" for i in range(100)]
            
            df = pd.DataFrame(data)
            
            # Convertir en Parquet (on garde Parquet pour la performance)
            parquet_buffer = io.BytesIO()
            df.to_parquet(parquet_buffer, index=False)
            parquet_buffer.seek(0)
            parquet_content = parquet_buffer.read()
            
            # Upload vers le stockage avec le nom original (mais en .parquet)
            base_name = filename.rsplit('.', 1)[0]  # Enlever l'extension .csv
            parquet_filename = f"{base_name}.parquet"
            object_path = f"{storage_path_prefix}{parquet_filename}"
            storage_client.upload_file(parquet_content, object_path)
            
            print(f"‚úÖ Fichier √©chantillon upload√©: {object_path}")
        
        return storage_path_prefix
        
    except StorageClientError as e:
        print(f"‚ùå Erreur de stockage pour dataset {dataset_id}: {str(e)}")
        return f"exai-datasets/{dataset_id}/"  # Retourner le path attendu
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur lors de l'upload de multiples fichiers pour {dataset_id}: {str(e)}")
        return f"exai-datasets/{dataset_id}/"  # Retourner le path attendu


def init_ednet_dataset():
    """
    Initialise le dataset EdNet avec tous ses fichiers et colonnes.
    
    Supprime les donn√©es existantes et recr√©e tout.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        print("üí° V√©rifiez que DATABASE_URL est d√©finie dans l'environnement")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            print("üóëÔ∏è  Suppression des donn√©es existantes du dataset EdNet...")
            
            # Supprimer le dataset EdNet s'il existe d√©j√† (cascade supprimera fichiers et colonnes)
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == "EdNet (Riiid Answer Correctness)"
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            print("üìä Cr√©ation du dataset EdNet...")
            
            # === CR√âATION DU DATASET PRINCIPAL ===
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer et uploader des fichiers √©chantillons
            sample_columns_data = {
                'columns': [
                    {'name': 'row_id', 'type': 'integer'},
                    {'name': 'timestamp', 'type': 'integer'},
                    {'name': 'user_id', 'type': 'integer'},
                    {'name': 'content_id', 'type': 'integer'},
                    {'name': 'content_type_id', 'type': 'integer'},
                    {'name': 'task_container_id', 'type': 'integer'},
                    {'name': 'user_answer', 'type': 'integer'},
                    {'name': 'answered_correctly', 'type': 'integer'},
                    {'name': 'prior_question_elapsed_time', 'type': 'float'},
                    {'name': 'prior_question_had_explanation', 'type': 'boolean'}
                ]
            }
            storage_path = upload_sample_dataset(dataset_id, sample_columns_data, "ednet_train")
            
            dataset = Dataset(
                id=dataset_id,
                
                # === IDENTIFICATION & INFORMATIONS G√âN√âRALES ===
                dataset_name="EdNet (Riiid Answer Correctness)",
                year=2020,
                objective="Pr√©diction de la r√©ussite aux exercices",
                access="public",
                availability="online_download",
                num_citations=50,
                citation_link="Riiid Answer Correctness Prediction | Kaggle",
                sources="Application Santa de Riiid",
                storage_uri=None,  # Vide pour le moment
                storage_path=storage_path,
                
                # === CARACT√âRISTIQUES TECHNIQUES ===
                instances_number=131000000,
                features_description="User ID, question ID, timestamp, correct, etc.",
                features_number=10,  # Colonnes principales dans train.csv
                domain=["√©ducation"],
                representativity_description="Large (plus de 700 000 √©l√®ves)",
                representativity_level="√©lev√©e",
                sample_balance_description=None,
                sample_balance_level=None,
                split=True,
                missing_values_description=None,
                has_missing_values=False,
                global_missing_percentage=0.0,
                missing_values_handling_method="none",
                temporal_factors=True,
                metadata_provided_with_dataset=True,
                external_documentation_available=True,
                documentation_link=None,
                task=["classification"],
                
                # === CRIT√àRES √âTHIQUES ===
                informed_consent=True,
                transparency=False,
                user_control=False,
                equity_non_discrimination=False,
                security_measures_in_place=True,
                data_quality_documented=True,
                data_errors_description=None,
                anonymization_applied=True,
                record_keeping_policy_exists=True,
                purpose_limitation_respected=True,
                accountability_defined=True
            )
            
            session.add(dataset)
            session.flush()  # Pour obtenir l'ID du dataset
            
            print(f"‚úÖ Dataset cr√©√© avec ID: {dataset.id}")
            
            # === CR√âATION DES FICHIERS ===
            files_data = [
                {
                    "name": "train.csv",
                    "role": "training_data",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 5000000000,  # ~5GB d'apr√®s description
                    "row_count": 131000000,
                    "description": "Fichier principal d'entra√Ænement contenant les interactions utilisateur-question",
                    "columns": [
                        {"name": "row_id", "type_orig": "int64", "type_interp": "numerical", "desc": "ID code for the row", "pos": 1, "is_pk": True, "is_null": False, "is_pii": False, "examples": ["0", "1", "2"]},
                        {"name": "timestamp", "type_orig": "int64", "type_interp": "temporal", "desc": "Time in milliseconds between user interaction and first event", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1500", "3200"]},
                        {"name": "user_id", "type_orig": "int32", "type_interp": "categorical", "desc": "ID code for the user", "pos": 3, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["115", "501", "1024"]},
                        {"name": "content_id", "type_orig": "int16", "type_interp": "categorical", "desc": "ID code for the user interaction", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["25", "128", "256"]},
                        {"name": "content_type_id", "type_orig": "int8", "type_interp": "categorical", "desc": "0 for question, 1 for lecture", "pos": 5, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1"]},
                        {"name": "task_container_id", "type_orig": "int16", "type_interp": "categorical", "desc": "ID for batch of questions or lectures", "pos": 6, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["1", "15", "32"]},
                        {"name": "user_answer", "type_orig": "int8", "type_interp": "categorical", "desc": "User's answer to question (-1 for null)", "pos": 7, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["0", "1", "2", "3", "-1"]},
                        {"name": "answered_correctly", "type_orig": "int8", "type_interp": "categorical", "desc": "Whether user responded correctly (-1 for null)", "pos": 8, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["0", "1", "-1"]},
                        {"name": "prior_question_elapsed_time", "type_orig": "float32", "type_interp": "numerical", "desc": "Average time in ms for previous question bundle", "pos": 9, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["1500.5", "2300.8", "null"]},
                        {"name": "prior_question_had_explanation", "type_orig": "bool", "type_interp": "categorical", "desc": "Whether user saw explanation after previous bundle", "pos": 10, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["true", "false", "null"]}
                    ]
                },
                {
                    "name": "questions.csv",
                    "role": "metadata",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 10000000,  # Estimation
                    "row_count": 13523,  # Estimation bas√©e sur les IDs de questions
                    "description": "M√©tadonn√©es des questions pos√©es aux utilisateurs",
                    "columns": [
                        {"name": "question_id", "type_orig": "int", "type_interp": "categorical", "desc": "Foreign key for content_id when content_type is question", "pos": 1, "is_pk": True, "is_null": False, "is_pii": False, "examples": ["25", "128", "256"]},
                        {"name": "bundle_id", "type_orig": "int", "type_interp": "categorical", "desc": "Code for which questions are served together", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["1", "15", "32"]},
                        {"name": "correct_answer", "type_orig": "int", "type_interp": "categorical", "desc": "The correct answer to the question", "pos": 3, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1", "2", "3"]},
                        {"name": "part", "type_orig": "int", "type_interp": "categorical", "desc": "The relevant section of the TOEIC test", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["1", "2", "3", "4", "5", "6", "7"]},
                        {"name": "tags", "type_orig": "string", "type_interp": "text", "desc": "Detailed tag codes for the question", "pos": 5, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["1 2 3", "4 5", "6"]}
                    ]
                },
                {
                    "name": "lectures.csv",
                    "role": "metadata",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 1000000,  # Estimation
                    "row_count": 418,  # Estimation bas√©e sur les IDs de lectures
                    "description": "M√©tadonn√©es des cours regard√©s par les utilisateurs",
                    "columns": [
                        {"name": "lecture_id", "type_orig": "int", "type_interp": "categorical", "desc": "Foreign key for content_id when content_type is lecture", "pos": 1, "is_pk": True, "is_null": False, "is_pii": False, "examples": ["1", "15", "32"]},
                        {"name": "part", "type_orig": "int", "type_interp": "categorical", "desc": "Top level category code for the lecture", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["1", "2", "3", "4", "5", "6", "7"]},
                        {"name": "tag", "type_orig": "string", "type_interp": "text", "desc": "Tag code for the lecture", "pos": 3, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["1", "2", "3"]},
                        {"name": "type_of", "type_orig": "string", "type_interp": "text", "desc": "Brief description of the core purpose of the lecture", "pos": 4, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["solving question", "watching", "concept"]}
                    ]
                },
                {
                    "name": "example_test_rows.csv",
                    "role": "test_sample",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 1000000,  # Estimation
                    "row_count": 1000,  # √âchantillon
                    "description": "√âchantillon de donn√©es de test pour l'API time-series",
                    "columns": [
                        {"name": "row_id", "type_orig": "int64", "type_interp": "numerical", "desc": "ID code for the row", "pos": 1, "is_pk": True, "is_null": False, "is_pii": False, "examples": ["0", "1", "2"]},
                        {"name": "timestamp", "type_orig": "int64", "type_interp": "temporal", "desc": "Time in milliseconds between user interaction and first event", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1500", "3200"]},
                        {"name": "user_id", "type_orig": "int32", "type_interp": "categorical", "desc": "ID code for the user", "pos": 3, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["115", "501", "1024"]},
                        {"name": "content_id", "type_orig": "int16", "type_interp": "categorical", "desc": "ID code for the user interaction", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["25", "128", "256"]},
                        {"name": "content_type_id", "type_orig": "int8", "type_interp": "categorical", "desc": "0 for question, 1 for lecture", "pos": 5, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1"]},
                        {"name": "task_container_id", "type_orig": "int16", "type_interp": "categorical", "desc": "ID for batch of questions or lectures", "pos": 6, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["1", "15", "32"]},
                        {"name": "prior_group_responses", "type_orig": "string", "type_interp": "text", "desc": "User answers from previous group as string list", "pos": 7, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["[0, 1, 2]", "[1, 3]", "null"]},
                        {"name": "prior_group_answers_correct", "type_orig": "string", "type_interp": "text", "desc": "Correct answers from previous group as string list", "pos": 8, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["[1, 0, 1]", "[0, 1]", "null"]}
                    ]
                },
                {
                    "name": "example_sample_submission.csv",
                    "role": "submission_template",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 971,  # Taille exacte donn√©e
                    "row_count": 100,  # Estimation
                    "description": "Fichier d'exemple pour les soumissions de la comp√©tition",
                    "columns": [
                        {"name": "row_id", "type_orig": "int64", "type_interp": "numerical", "desc": "ID code for the row", "pos": 1, "is_pk": True, "is_null": False, "is_pii": False, "examples": ["0", "1", "2"]},
                        {"name": "answered_correctly", "type_orig": "float", "type_interp": "numerical", "desc": "Predicted probability of correct answer", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0.5", "0.7", "0.3"]}
                    ]
                }
            ]
            
            # Cr√©er le fichier principal upload√© (Parquet)
            print(f"üìÅ Cr√©ation du fichier principal: ednet_train.parquet")
            
            # Cr√©er le fichier principal (√©chantillon upload√© en Parquet)
            dataset_file = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage="ednet_train.parquet",
                logical_role="training_data",
                format="parquet",
                mime_type="application/octet-stream",
                size_bytes=50000,  # Taille approximative du fichier √©chantillon
                row_count=100,     # 100 lignes √©chantillons
                description="Fichier d'entra√Ænement principal avec donn√©es √©chantillons (format Parquet)"
            )
            
            session.add(dataset_file)
            session.flush()  # Pour obtenir l'ID du fichier
            
            # Cr√©er les colonnes bas√©es sur les m√©tadonn√©es √©chantillons
            columns_info = sample_columns_data['columns']
            for i, col_info in enumerate(columns_info):
                file_column = FileColumn(
                    dataset_file_id=dataset_file.id,
                    column_name=col_info["name"],
                    data_type_original=col_info["type"],
                    data_type_interpreted=col_info["type"],
                    description=f"Colonne {col_info['name']} du dataset EdNet",
                    is_primary_key_component=(col_info["name"] == "row_id"),
                    is_nullable=False,
                    is_pii=(col_info["name"] == "user_id"),
                    example_values=[f"sample_{i+1}", f"sample_{i+2}", f"sample_{i+3}"],
                    position=i+1,
                    stats=None  # Pas de statistiques pour le moment
                )
                session.add(file_column)
            
            print(f"  ‚úÖ {len(columns_info)} colonnes cr√©√©es")
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset EdNet initialis√© avec succ√®s !")
            print(f"üìä Dataset ID: {dataset.id}")
            print(f"üíæ Storage Path: {storage_path}")
            print(f"üìÅ 1 fichier Parquet cr√©√© et upload√©")
            print(f"üìã {len(columns_info)} colonnes cr√©√©es au total")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
        finally:
            session.close()

def init_oulad_dataset():
    """
    Initialise le dataset OULAD avec tous ses fichiers et colonnes.
    
    Supprime les donn√©es existantes et recr√©e tout.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        print("üí° V√©rifiez que DATABASE_URL est d√©finie dans l'environnement")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            print("üóëÔ∏è  Suppression des donn√©es existantes du dataset OULAD...")
            
            # Supprimer le dataset OULAD s'il existe d√©j√† (cascade supprimera fichiers et colonnes)
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == "OULAD"
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            print("üìä Cr√©ation du dataset OULAD...")
            
            # === CR√âATION DU DATASET PRINCIPAL ===
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer et uploader des fichiers √©chantillons
            sample_columns_data = {
                'columns': [
                    {'name': 'code_module', 'type': 'string'},
                    {'name': 'code_presentation', 'type': 'string'},
                    {'name': 'id_student', 'type': 'integer'},
                    {'name': 'gender', 'type': 'string'},
                    {'name': 'region', 'type': 'string'},
                    {'name': 'highest_education', 'type': 'string'},
                    {'name': 'imd_band', 'type': 'string'},
                    {'name': 'age_band', 'type': 'string'},
                    {'name': 'num_of_prev_attempts', 'type': 'integer'},
                    {'name': 'studied_credits', 'type': 'integer'},
                    {'name': 'disability', 'type': 'string'},
                    {'name': 'final_result', 'type': 'string'}
                ]
            }
            storage_path = upload_sample_dataset(dataset_id, sample_columns_data, "oulad_main")
            
            dataset = Dataset(
                id=dataset_id,
                
                # === IDENTIFICATION & INFORMATIONS G√âN√âRALES ===
                dataset_name="OULAD",
                year=2014,
                objective="Analyse de l'apprentissage en ligne dans l'enseignement sup√©rieur",
                access="public",
                availability="online_download",
                num_citations=1800,
                citation_link="OULAD: Open University Learning Analytics Dataset",
                sources="Open University",
                storage_uri=None,  # Vide pour le moment
                storage_path=storage_path,
                
                # === CARACT√âRISTIQUES TECHNIQUES ===
                instances_number=32593,
                features_description="donn√©es d√©mographiques, interactions VLE, scores",
                features_number=25,
                domain=["√©ducation"],
                representativity_description="Universit√© UK √† distance",
                representativity_level="moyenne",
                sample_balance_description=None,
                sample_balance_level=None,
                split=True,
                missing_values_description="Quelques valeurs manquantes sur les scores finaux",
                has_missing_values=True,
                global_missing_percentage=0.5,
                missing_values_handling_method="row_deletion",
                temporal_factors=True,
                metadata_provided_with_dataset=True,
                external_documentation_available=True,
                documentation_link=None,
                task=["classification"],
                
                # === CRIT√àRES √âTHIQUES ===
                informed_consent=True,
                transparency=True,
                user_control=False,
                equity_non_discrimination=True,
                security_measures_in_place=True,
                data_quality_documented=True,
                data_errors_description=None,
                anonymization_applied=True,
                record_keeping_policy_exists=True,
                purpose_limitation_respected=True,
                accountability_defined=True
            )
            
            session.add(dataset)
            session.flush()  # Pour obtenir l'ID du dataset
            
            print(f"‚úÖ Dataset cr√©√© avec ID: {dataset.id}")
            
            # === CR√âATION DES FICHIERS ===
            files_data = [
                {
                    "name": "assessments.csv",
                    "role": "metadata",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 8200,  # 8.2 kB
                    "row_count": 206,  # Estimation bas√©e sur les donn√©es
                    "description": "Informations sur les √©valuations (TMA, CMA, Exam) pour chaque module",
                    "columns": [
                        {"name": "code_module", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification du module", "pos": 1, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["AAA", "BBB", "CCC"]},
                        {"name": "code_presentation", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification de la pr√©sentation", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["2013J", "2014B", "2014J"]},
                        {"name": "id_assessment", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique de l'√©valuation", "pos": 3, "is_pk": True, "is_null": False, "is_pii": False, "examples": ["1752", "1753", "14991"]},
                        {"name": "assessment_type", "type_orig": "string", "type_interp": "categorical", "desc": "Type d'√©valuation", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["TMA", "CMA", "Exam"]},
                        {"name": "date", "type_orig": "int", "type_interp": "temporal", "desc": "Date de l'√©valuation", "pos": 5, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["19", "54", "100"]},
                        {"name": "weight", "type_orig": "float", "type_interp": "numerical", "desc": "Poids de l'√©valuation dans la note finale", "pos": 6, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["10", "20", "100"]}
                    ]
                },
                {
                    "name": "courses.csv",
                    "role": "metadata",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 526,  # 526 B
                    "row_count": 22,  # Estimation bas√©e sur les modules et pr√©sentations
                    "description": "Informations sur les cours/modules et leurs pr√©sentations",
                    "columns": [
                        {"name": "code_module", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification du module", "pos": 1, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["AAA", "BBB", "CCC"]},
                        {"name": "code_presentation", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification de la pr√©sentation", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["2013J", "2014B", "2014J"]},
                        {"name": "module_presentation_length", "type_orig": "int", "type_interp": "numerical", "desc": "Dur√©e de la pr√©sentation du module", "pos": 3, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["268", "269", "237"]}
                    ]
                },
                {
                    "name": "studentAssessment.csv",
                    "role": "assessment_data",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 5690000,  # 5.69 MB
                    "row_count": 173912,  # Estimation bas√©e sur les donn√©es
                    "description": "R√©sultats des √©tudiants aux √©valuations",
                    "columns": [
                        {"name": "id_assessment", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique de l'√©valuation", "pos": 1, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["1752", "1753", "14991"]},
                        {"name": "id_student", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique de l'√©tudiant", "pos": 2, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["11391", "28400", "30268"]},
                        {"name": "date_submitted", "type_orig": "int", "type_interp": "temporal", "desc": "Date de soumission de l'√©valuation", "pos": 3, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["18", "19", "20"]},
                        {"name": "is_banked", "type_orig": "bool", "type_interp": "categorical", "desc": "Indique si l'√©valuation est mise en banque", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1"]},
                        {"name": "score", "type_orig": "float", "type_interp": "numerical", "desc": "Score obtenu √† l'√©valuation", "pos": 5, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["78", "85", "92"]}
                    ]
                },
                {
                    "name": "studentInfo.csv",
                    "role": "student_data",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 3460000,  # 3.46 MB
                    "row_count": 32593,
                    "description": "Informations d√©mographiques et acad√©miques des √©tudiants",
                    "columns": [
                        {"name": "code_module", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification du module", "pos": 1, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["AAA", "BBB", "CCC"]},
                        {"name": "code_presentation", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification de la pr√©sentation", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["2013J", "2014B", "2014J"]},
                        {"name": "id_student", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique de l'√©tudiant", "pos": 3, "is_pk": True, "is_null": False, "is_pii": True, "examples": ["11391", "28400", "30268"]},
                        {"name": "gender", "type_orig": "string", "type_interp": "categorical", "desc": "Genre de l'√©tudiant", "pos": 4, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["M", "F"]},
                        {"name": "region", "type_orig": "string", "type_interp": "categorical", "desc": "R√©gion de r√©sidence de l'√©tudiant", "pos": 5, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["Scotland", "East Anglian Region", "North Western Region"]},
                        {"name": "highest_education", "type_orig": "string", "type_interp": "categorical", "desc": "Plus haut niveau d'√©ducation", "pos": 6, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["HE Qualification", "A Level or Equivalent", "Lower Than A Level"]},
                        {"name": "imd_band", "type_orig": "string", "type_interp": "categorical", "desc": "Bande d'indice de d√©favorisation", "pos": 7, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["90-100%", "20-30%", "30-40%"]},
                        {"name": "age_band", "type_orig": "string", "type_interp": "categorical", "desc": "Tranche d'√¢ge", "pos": 8, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["55<=", "35-55", "0-35"]},
                        {"name": "num_of_prev_attempts", "type_orig": "int", "type_interp": "numerical", "desc": "Nombre de tentatives pr√©c√©dentes", "pos": 9, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1", "2"]},
                        {"name": "studied_credits", "type_orig": "int", "type_interp": "numerical", "desc": "Nombre de cr√©dits √©tudi√©s", "pos": 10, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["240", "60", "120"]},
                        {"name": "disability", "type_orig": "string", "type_interp": "categorical", "desc": "Statut de handicap", "pos": 11, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["N", "Y"]},
                        {"name": "final_result", "type_orig": "string", "type_interp": "categorical", "desc": "R√©sultat final du cours", "pos": 12, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["Pass", "Withdrawn", "Fail", "Distinction"]}
                    ]
                },
                {
                    "name": "studentRegistration.csv",
                    "role": "registration_data",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 1110000,  # 1.11 MB
                    "row_count": 32593,
                    "description": "Informations d'inscription et de d√©sinscription des √©tudiants",
                    "columns": [
                        {"name": "code_module", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification du module", "pos": 1, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["AAA", "BBB", "CCC"]},
                        {"name": "code_presentation", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification de la pr√©sentation", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["2013J", "2014B", "2014J"]},
                        {"name": "id_student", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique de l'√©tudiant", "pos": 3, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["11391", "28400", "30268"]},
                        {"name": "date_registration", "type_orig": "int", "type_interp": "temporal", "desc": "Date d'inscription", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["-150", "-100", "-50"]},
                        {"name": "date_unregistration", "type_orig": "int", "type_interp": "temporal", "desc": "Date de d√©sinscription", "pos": 5, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["50", "100", "null"]}
                    ]
                },
                {
                    "name": "vle.csv",
                    "role": "metadata",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 260130,  # 260.13 kB
                    "row_count": 6364,  # Estimation
                    "description": "Informations sur les activit√©s du VLE (Virtual Learning Environment)",
                    "columns": [
                        {"name": "id_site", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique du site/activit√© VLE", "pos": 1, "is_pk": True, "is_null": False, "is_pii": False, "examples": ["546943", "547217", "547447"]},
                        {"name": "code_module", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification du module", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["AAA", "BBB", "CCC"]},
                        {"name": "code_presentation", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification de la pr√©sentation", "pos": 3, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["2013J", "2014B", "2014J"]},
                        {"name": "activity_type", "type_orig": "string", "type_interp": "categorical", "desc": "Type d'activit√© VLE", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["resource", "subpage", "homepage"]},
                        {"name": "week_from", "type_orig": "int", "type_interp": "temporal", "desc": "Semaine de d√©but d'activit√©", "pos": 5, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["0", "1", "5"]},
                        {"name": "week_to", "type_orig": "int", "type_interp": "temporal", "desc": "Semaine de fin d'activit√©", "pos": 6, "is_pk": False, "is_null": True, "is_pii": False, "examples": ["5", "10", "29"]}
                    ]
                }
            ]
            
            # Ajouter les fichiers studentVle_0.csv √† studentVle_7.csv
            for i in range(8):
                studentVle_file = {
                    "name": f"studentVle_{i}.csv",
                    "role": "interaction_data",
                    "format": "csv",
                    "mime_type": "text/csv",
                    "size_bytes": 57000000,  # ~57 MB chacun (estimation)
                    "row_count": 1500000,  # ~1.5M lignes chacun (estimation)
                    "description": f"Interactions des √©tudiants avec le VLE - partie {i+1}/8",
                    "columns": [
                        {"name": "code_module", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification du module", "pos": 1, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["AAA", "BBB", "CCC"]},
                        {"name": "code_presentation", "type_orig": "string", "type_interp": "categorical", "desc": "Code identification de la pr√©sentation", "pos": 2, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["2013J", "2014B", "2014J"]},
                        {"name": "id_student", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique de l'√©tudiant", "pos": 3, "is_pk": False, "is_null": False, "is_pii": True, "examples": ["11391", "28400", "30268"]},
                        {"name": "id_site", "type_orig": "int", "type_interp": "categorical", "desc": "Identifiant unique du site/activit√© VLE", "pos": 4, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["546943", "547217", "547447"]},
                        {"name": "date", "type_orig": "int", "type_interp": "temporal", "desc": "Date de l'interaction (jour relatif)", "pos": 5, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["-24", "0", "50"]},
                        {"name": "sum_click", "type_orig": "int", "type_interp": "numerical", "desc": "Nombre total de clics sur le site ce jour-l√†", "pos": 6, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["1", "5", "20"]},
                        {"name": "week", "type_orig": "int", "type_interp": "temporal", "desc": "Semaine relative du cours", "pos": 7, "is_pk": False, "is_null": False, "is_pii": False, "examples": ["0", "1", "10"]}
                    ]
                }
                files_data.append(studentVle_file)
            
            # Cr√©er les fichiers et leurs colonnes
            for file_info in files_data:
                print(f"üìÅ Cr√©ation du fichier: {file_info['name']}")
                
                # Cr√©er le fichier
                dataset_file = DatasetFile(
                    dataset_id=dataset.id,
                    file_name_in_storage=file_info["name"],
                    logical_role=file_info["role"],
                    format=file_info["format"],
                    mime_type=file_info["mime_type"],
                    size_bytes=file_info["size_bytes"],
                    row_count=file_info["row_count"],
                    description=file_info["description"]
                )
                
                session.add(dataset_file)
                session.flush()  # Pour obtenir l'ID du fichier
                
                # Cr√©er les colonnes
                for col_info in file_info["columns"]:
                    file_column = FileColumn(
                        dataset_file_id=dataset_file.id,
                        column_name=col_info["name"],
                        data_type_original=col_info["type_orig"],
                        data_type_interpreted=col_info["type_interp"],
                        description=col_info["desc"],
                        is_primary_key_component=col_info["is_pk"],
                        is_nullable=col_info["is_null"],
                        is_pii=col_info["is_pii"],
                        example_values=col_info["examples"],
                        position=col_info["pos"],
                        stats=None  # Pas de statistiques pour le moment
                    )
                    session.add(file_column)
                
                print(f"  ‚úÖ {len(file_info['columns'])} colonnes cr√©√©es")
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset OULAD initialis√© avec succ√®s !")
            print(f"üìä Dataset ID: {dataset.id}")
            print(f"üìÅ {len(files_data)} fichiers cr√©√©s")
            
            total_columns = sum(len(f["columns"]) for f in files_data)
            print(f"üìã {total_columns} colonnes cr√©√©es au total")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
        finally:
            session.close()

def init_students_performance_dataset():
    """
    Initialise le dataset Students Performance in Exams avec son fichier et colonnes.
    
    Supprime les donn√©es existantes et recr√©e tout.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        print("üí° V√©rifiez que DATABASE_URL est d√©finie dans les variables d'environnement")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            # === SUPPRESSION DES DONN√âES EXISTANTES ===
            print("üóëÔ∏è  Suppression des donn√©es existantes pour 'Students Performance in Exams'...")
            
            # Supprimer le dataset Students Performance s'il existe d√©j√† (cascade supprimera fichiers et colonnes)
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == 'Students Performance in Exams'
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            # === CR√âATION DU DATASET ===
            print("üìä Cr√©ation du dataset Students Performance in Exams...")
            
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer et uploader des fichiers √©chantillons
            sample_columns_data = {
                'columns': [
                    {'name': 'gender', 'type': 'string'},
                    {'name': 'race/ethnicity', 'type': 'string'},
                    {'name': 'parental level of education', 'type': 'string'},
                    {'name': 'lunch', 'type': 'string'},
                    {'name': 'test preparation course', 'type': 'string'},
                    {'name': 'math score', 'type': 'integer'},
                    {'name': 'reading score', 'type': 'integer'},
                    {'name': 'writing score', 'type': 'integer'}
                ]
            }
            storage_path = upload_sample_dataset(dataset_id, sample_columns_data, "students_performance")
            
            dataset = Dataset(
                id=dataset_id,
                
                # === IDENTIFICATION & INFORMATIONS G√âN√âRALES ===
                dataset_name='Students Performance in Exams',
                year=2018,
                objective='Understand influence of parents background, test preparation on students performance',
                access='public',
                availability='public',
                num_citations=None,  # Pas de nombre sp√©cifique donn√©
                citation_link='http://roycekimmons.com/tools/generated_data/exams',
                sources='Kaggle, roycekimmons.com',
                storage_uri='https://www.kaggle.com/datasets/jessemostipak/student-performance',
                storage_path=storage_path,
                
                # === CARACT√âRISTIQUES TECHNIQUES ===
                instances_number=1000,
                features_description='gender, race/ethnicity, parental level of education, lunch, test preparation course, math score, reading score, writing score',
                features_number=8,
                domain=['√©ducation'],
                representativity_description='US high school students',
                representativity_level='moyenne',
                sample_balance_description=None,
                sample_balance_level=None,
                split=False,
                missing_values_description='No missing values',
                has_missing_values=False,
                global_missing_percentage=0.0,
                missing_values_handling_method='none',
                temporal_factors=False,
                metadata_provided_with_dataset=False,
                external_documentation_available=False,
                documentation_link=None,
                task=['exploratory_analysis', 'prediction'],
                
                # === CRIT√àRES √âTHIQUES ===
                informed_consent=None,  # Unknown
                transparency=False,     # Medium -> False pour boolean
                user_control=None,      # Unknown
                equity_non_discrimination=None,  # Unknown
                security_measures_in_place=False,  # None -> False
                data_quality_documented=True,
                data_errors_description=None,
                anonymization_applied=True,
                record_keeping_policy_exists=None,  # Unknown
                purpose_limitation_respected=True,
                accountability_defined=None  # Unknown
            )
            
            session.add(dataset)
            session.flush()  # Pour obtenir l'ID du dataset
            
            print(f"‚úÖ Dataset cr√©√© avec ID: {dataset.id}")
            
            # === CR√âATION DU FICHIER ===
            print("üìÅ Cr√©ation du fichier StudentsPerformance.csv...")
            
            dataset_file = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage='StudentsPerformance.csv',
                logical_role='main_data',
                format='csv',
                mime_type='text/csv',
                size_bytes=73788,  # 72.04 kB
                row_count=1000,
                description='Scores obtenus par des √©tudiants du secondaire aux √âtats-Unis'
            )
            
            session.add(dataset_file)
            session.flush()  # Pour obtenir l'ID du fichier
            
            print(f"‚úÖ Fichier cr√©√© avec ID: {dataset_file.id}")
            
            # === CR√âATION DES COLONNES ===
            print("üî¢ Cr√©ation des 8 colonnes...")
            
            columns_data = [
                # Colonnes d√©mographiques et socio-√©conomiques
                {
                    'name': 'gender', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Genre de l\'√©tudiant', 'pos': 1, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['female', 'male']
                },
                {
                    'name': 'race/ethnicity', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Groupe ethnique de l\'√©tudiant', 'pos': 2, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['group A', 'group B', 'group C', 'group D', 'group E']
                },
                {
                    'name': 'parental level of education', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Niveau d\'√©ducation des parents', 'pos': 3, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['some high school', 'high school', 'some college', 'associate\'s degree', 'bachelor\'s degree', 'master\'s degree']
                },
                {
                    'name': 'lunch', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Type de repas (indicateur socio-√©conomique)', 'pos': 4, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['standard', 'free/reduced']
                },
                {
                    'name': 'test preparation course', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Pr√©paration aux examens suivie', 'pos': 5, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['none', 'completed']
                },
                
                # Colonnes de scores (variables cibles)
                {
                    'name': 'math score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Score en math√©matiques (0-100)', 'pos': 6, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['0', '50', '72', '100']
                },
                {
                    'name': 'reading score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Score en lecture (17-100)', 'pos': 7, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['17', '55', '72', '100']
                },
                {
                    'name': 'writing score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Score en √©criture (10-100)', 'pos': 8, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['10', '55', '74', '100']
                }
            ]
            
            # Cr√©er les colonnes
            for col_info in columns_data:
                file_column = FileColumn(
                    dataset_file_id=dataset_file.id,
                    column_name=col_info['name'],
                    data_type_original=col_info['type_orig'],
                    data_type_interpreted=col_info['type_interp'],
                    description=col_info['desc'],
                    is_primary_key_component=col_info['is_pk'],
                    is_nullable=col_info['is_null'],
                    is_pii=col_info['is_pii'],
                    example_values=col_info['examples'],
                    position=col_info['pos'],
                    stats=None  # Pas de statistiques pour le moment
                )
                session.add(file_column)
                print(f"   ‚úì Colonne {col_info['pos']}/8: {col_info['name']}")
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset 'Students Performance in Exams' initialis√© avec succ√®s !")
            print(f"üìä Dataset ID: {dataset.id}")
            print(f"üìÅ 1 fichier cr√©√©")
            print(f"üìã 8 colonnes cr√©√©es")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
        finally:
            session.close()


def init_social_media_addiction_dataset():
    """
    Initialise le dataset Students' Social Media Addiction avec des donn√©es √©chantillons.
    
    Supprime les donn√©es existantes et recr√©e tout avec des donn√©es √©chantillons.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        print("üí° V√©rifiez que DATABASE_URL est d√©finie dans les variables d'environnement")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            # === SUPPRESSION DES DONN√âES EXISTANTES ===
            print("üóëÔ∏è  Suppression des donn√©es existantes pour 'Students' Social Media Addiction'...")
            
            # Supprimer le dataset Social Media Addiction s'il existe d√©j√† (cascade supprimera fichiers et colonnes)
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == "Students' Social Media Addiction"
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            # === CR√âATION DU DATASET ===
            print("üìä Cr√©ation du dataset Students' Social Media Addiction (DONN√âES √âCHANTILLONS)...")
            
            # G√©n√©rer un UUID pour le dataset
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer des donn√©es √©chantillons avec le nouveau syst√®me
            sample_columns_data = {
                'columns': [
                    {'name': 'Student_ID', 'type': 'integer'},
                    {'name': 'Age', 'type': 'integer'},
                    {'name': 'Gender', 'type': 'string'},
                    {'name': 'Academic_Level', 'type': 'string'},
                    {'name': 'Country', 'type': 'string'},
                    {'name': 'Avg_Daily_Usage_Hours', 'type': 'float'},
                    {'name': 'Most_Used_Platform', 'type': 'string'},
                    {'name': 'Affects_Academic_Performance', 'type': 'string'},
                    {'name': 'Sleep_Hours_Per_Night', 'type': 'float'},
                    {'name': 'Mental_Health_Score', 'type': 'integer'},
                    {'name': 'Relationship_Status', 'type': 'string'},
                    {'name': 'Conflicts_Over_Social_Media', 'type': 'integer'},
                    {'name': 'Addicted_Score', 'type': 'integer'}
                ]
            }
            
            # Uploader les donn√©es √©chantillons
            print(f"üìÅ G√©n√©ration et upload des donn√©es √©chantillons du dataset social media addiction...")
            storage_path = upload_sample_dataset(dataset_id, sample_columns_data, "students_social_media_addiction")
            row_count = 705  # Nombre de lignes √©chantillons
            file_size = 50000  # Taille estim√©e
            
            dataset = Dataset(
                id=dataset_id,
                
                # === IDENTIFICATION & INFORMATIONS G√âN√âRALES ===
                dataset_name="Students' Social Media Addiction",
                year=2025,
                objective="Analyze cross-country patterns of social media usage, academic impact, and relationships",
                access="public",
                availability="public",
                num_citations=None,  # Pas de nombre sp√©cifique donn√©
                citation_link="https://www.kaggle.com/datasets/adilshamim8/social-media-addiction-vs-relationships",
                sources="University surveys, online social media recruitment",
                storage_uri=None,  # Vide pour le moment
                storage_path=storage_path,
                
                # === CARACT√âRISTIQUES TECHNIQUES (DONN√âES √âCHANTILLONS) ===
                instances_number=row_count,  # Donn√©es √©chantillons
                features_description="Age, Gender, Academic Level, Usage, Mental health, etc.",
                features_number=13,
                domain=["√©ducation"],
                representativity_description="Multi-country student sample (ages 16‚Äì25, diverse education)",
                representativity_level="moyenne",
                sample_balance_description=None,
                sample_balance_level=None,
                split=False,
                missing_values_description="Aucune valeur manquante dans le dataset r√©el",
                has_missing_values=False,
                global_missing_percentage=0.0,
                missing_values_handling_method="none",
                temporal_factors=True,  # Q1 2025
                metadata_provided_with_dataset=True,
                external_documentation_available=True,
                documentation_link=None,
                task=["classification", "clustering", "regression"],
                
                # === CRIT√àRES √âTHIQUES ===
                informed_consent=True,
                transparency=True,
                user_control=False,
                equity_non_discrimination=False,
                security_measures_in_place=True,
                data_quality_documented=True,
                data_errors_description=None,
                anonymization_applied=True,
                record_keeping_policy_exists=False,
                purpose_limitation_respected=True,
                accountability_defined=True
            )
            
            session.add(dataset)
            session.flush()  # Pour obtenir l'ID du dataset
            
            print(f"‚úÖ Dataset cr√©√© avec ID: {dataset.id}")
            
            # === CR√âATION DU FICHIER PRINCIPAL ===
            print("üìÅ Cr√©ation du fichier principal: students_social_media_addiction.parquet")
            
            dataset_file = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage='students_social_media_addiction.parquet',
                logical_role='main_data',
                format='parquet',
                mime_type='application/octet-stream',
                size_bytes=file_size,
                row_count=row_count,
                description='Sample survey responses from students aged 16‚Äì25 across multiple countries, capturing social media usage patterns and life outcomes (Sample Dataset - Parquet format)'
            )
            
            session.add(dataset_file)
            session.flush()  # Pour obtenir l'ID du fichier
            
            print(f"‚úÖ Fichier cr√©√© avec ID: {dataset_file.id}")
            
            # === CR√âATION DES COLONNES (BAS√âES SUR LE VRAI FICHIER CSV) ===
            print("üî¢ Cr√©ation des 13 colonnes (m√©tadonn√©es du vrai dataset)...")
            
            columns_data = [
                # Identifiant et donn√©es d√©mographiques
                {
                    'name': 'Student_ID', 'type_orig': 'int', 'type_interp': 'categorical', 
                    'desc': 'Unique identifier assigned to each survey respondent', 'pos': 1, 'is_pk': True, 'is_null': False, 
                    'is_pii': False, 'examples': ['1', '2', '3', '705']
                },
                {
                    'name': 'Age', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Student age in completed years at time of survey', 'pos': 2, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['18', '19', '20', '21', '22', '23', '24']
                },
                {
                    'name': 'Gender', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Self-reported gender for demographic analysis', 'pos': 3, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Male', 'Female']
                },
                {
                    'name': 'Academic_Level', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Highest education level currently enrolled in', 'pos': 4, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['High School', 'Undergraduate', 'Graduate']
                },
                {
                    'name': 'Country', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Country of residence where survey was completed', 'pos': 5, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Bangladesh', 'India', 'USA', 'UK', 'Canada', 'Australia', 'Germany']
                },
                
                # Usage et comportement sur les r√©seaux sociaux
                {
                    'name': 'Avg_Daily_Usage_Hours', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'Average hours per day spent on social media platforms', 'pos': 6, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['1.5', '3.0', '4.5', '6.0', '7.2', '8.5']
                },
                {
                    'name': 'Most_Used_Platform', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Social media platform where student spends most time', 'pos': 7, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Instagram', 'TikTok', 'Facebook', 'YouTube', 'Twitter', 'Snapchat', 'LinkedIn']
                },
                {
                    'name': 'Affects_Academic_Performance', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Whether student perceives social media use impacts academics negatively', 'pos': 8, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Yes', 'No']
                },
                
                # Bien-√™tre et sant√©
                {
                    'name': 'Sleep_Hours_Per_Night', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'Average nightly sleep duration in hours', 'pos': 9, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['3.8', '5.0', '6.5', '7.0', '8.0', '9.6']
                },
                {
                    'name': 'Mental_Health_Score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Self-rated mental well-being (1=poor to 10=excellent)', 'pos': 10, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['4', '5', '6', '7', '8', '9']
                },
                
                # Relations et conflits
                {
                    'name': 'Relationship_Status', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Current romantic relationship status', 'pos': 11, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Single', 'In Relationship', 'Complicated']
                },
                {
                    'name': 'Conflicts_Over_Social_Media', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Number of arguments/disagreements due to social media use', 'pos': 12, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['0', '1', '2', '3', '4', '5']
                },
                
                # Score d'addiction (variable cible principale)
                {
                    'name': 'Addicted_Score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Social Media Addiction Score (1=low to 10=high addiction)', 'pos': 13, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['1', '3', '5', '7', '8', '10']
                }
            ]
            
            # Cr√©er les colonnes
            for col_info in columns_data:
                file_column = FileColumn(
                    dataset_file_id=dataset_file.id,
                    column_name=col_info['name'],
                    data_type_original=col_info['type_orig'],
                    data_type_interpreted=col_info['type_interp'],
                    description=col_info['desc'],
                    is_primary_key_component=col_info['is_pk'],
                    is_nullable=col_info['is_null'],
                    is_pii=col_info['is_pii'],
                    example_values=col_info['examples'],
                    position=col_info['pos'],
                    stats=None  # Pas de statistiques pour le moment
                )
                session.add(file_column)
                print(f"   ‚úì Colonne {col_info['pos']}/13: {col_info['name']}")
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset 'Students' Social Media Addiction' initialis√© avec succ√®s !")
            print(f"üìä Dataset ID: {dataset.id}")
            print(f"üíæ Storage Path: {storage_path}")
            print(f"üìÅ 1 fichier Parquet cr√©√© et upload√© (VRAI DATASET)")
            print(f"üìà {row_count} lignes de donn√©es r√©elles")
            print(f"üíæ {file_size} bytes")
            print(f"üìã 13 colonnes cr√©√©es")
            print(f"üîó Format: CSV ‚Üí Parquet (gain de performance 10-50x)")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
        finally:
            session.close()


def init_student_academic_performance_dataset():
    """
    Initialise le dataset Student Academic Performance Dataset avec son fichier et colonnes.
    
    Supprime les donn√©es existantes et recr√©e tout.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        print("üí° V√©rifiez que DATABASE_URL est d√©finie dans les variables d'environnement")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            # === SUPPRESSION DES DONN√âES EXISTANTES ===
            print("üóëÔ∏è  Suppression des donn√©es existantes pour 'Student Academic Performance Dataset'...")
            
            # Supprimer le dataset Student Academic Performance s'il existe d√©j√† (cascade supprimera fichiers et colonnes)
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == "Student Academic Performance Dataset"
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            # === CR√âATION DU DATASET ===
            print("üìä Cr√©ation du dataset Student Academic Performance Dataset...")
            
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer et uploader des fichiers √©chantillons
            sample_columns_data = {
                'columns': [
                    {'name': 'student_id', 'type': 'integer'},
                    {'name': 'name', 'type': 'string'},
                    {'name': 'gender', 'type': 'string'},
                    {'name': 'age', 'type': 'integer'},
                    {'name': 'grade_level', 'type': 'string'},
                    {'name': 'math_score', 'type': 'float'},
                    {'name': 'reading_score', 'type': 'float'},
                    {'name': 'writing_score', 'type': 'float'},
                    {'name': 'attendance_rate', 'type': 'float'},
                    {'name': 'parent_education', 'type': 'string'}
                ]
            }
            storage_path = upload_sample_dataset(dataset_id, sample_columns_data, "student_academic_performance")
            
            dataset = Dataset(
                id=dataset_id,
                
                # === IDENTIFICATION & INFORMATIONS G√âN√âRALES ===
                dataset_name="Student Academic Performance Dataset",
                year=2025,
                objective="To help users explore how various factors (study time, family background, extracurricular activities) affect student academic performance using demographic and behavioral data.",
                access="public",
                availability="online_download",
                num_citations=None,  # Pas de nombre sp√©cifique donn√© (x dans le CSV)
                citation_link=None,  # x dans le CSV
                sources="student_info.csv",
                storage_uri=None,  # Vide dans le CSV
                storage_path=storage_path,
                
                # === CARACT√âRISTIQUES TECHNIQUES ===
                instances_number=1000,
                features_description="student_id, name, gender, age, grade_level, math_score, reading_score, writing_score, attendance_rate, parent_education",
                features_number=10,
                domain=["√©ducation"],
                representativity_description="Sample of 1000 anonymized student records from unspecified population",
                representativity_level="moyenne",  # "Moderate" dans le CSV
                sample_balance_description=None,
                sample_balance_level=None,
                split=False,  # "No predefined split"
                missing_values_description="No missing values mentioned",
                has_missing_values=False,
                global_missing_percentage=0.0,
                missing_values_handling_method="none",  # "N/A"
                temporal_factors=False,  # "Static snapshot"
                metadata_provided_with_dataset=True,
                external_documentation_available=False,
                documentation_link=None,
                task=["exploratory_analysis", "classification", "regression"],
                
                # === CRIT√àRES √âTHIQUES ===
                informed_consent=None,  # "Not specified"
                transparency=False,     # "Limited"
                user_control=None,      # "Not specified"
                equity_non_discrimination=None,  # "Not guaranteed"
                security_measures_in_place=True,  # "Basic anonymization"
                data_quality_documented=False,
                data_errors_description=None,
                anonymization_applied=True,
                record_keeping_policy_exists=None,  # "Not specified"
                purpose_limitation_respected=None,  # "Unknown"
                accountability_defined=None  # "Unknown"
            )
            
            session.add(dataset)
            session.flush()  # Pour obtenir l'ID du dataset
            
            print(f"‚úÖ Dataset cr√©√© avec ID: {dataset.id}")
            
            # === CR√âATION DU FICHIER ===
            print("üìÅ Cr√©ation du fichier student_info.csv...")
            
            dataset_file = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage='student_info.csv',
                logical_role='main_data',
                format='csv',
                mime_type='text/csv',
                size_bytes=107460,  # 107.46 kB
                row_count=1000,
                description='Anonymized student-level data capturing academic, demographic, and behavioral features for performance analysis'
            )
            
            session.add(dataset_file)
            session.flush()  # Pour obtenir l'ID du fichier
            
            print(f"‚úÖ Fichier cr√©√© avec ID: {dataset_file.id}")
            
            # === CR√âATION DES COLONNES ===
            print("üî¢ Cr√©ation des 10 colonnes...")
            
            columns_data = [
                # Identifiant et informations personnelles
                {
                    'name': 'student_id', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Unique identifier for each student', 'pos': 1, 'is_pk': True, 'is_null': False, 
                    'is_pii': False, 'examples': ['S1', 'S2', 'S3', 'S1000']
                },
                {
                    'name': 'name', 'type_orig': 'string', 'type_interp': 'text', 
                    'desc': 'Anonymized full name', 'pos': 2, 'is_pk': False, 'is_null': False, 
                    'is_pii': True, 'examples': ['Student_1', 'Student_2', 'Student_3']
                },
                
                # Donn√©es d√©mographiques
                {
                    'name': 'gender', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Student gender identity', 'pos': 3, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Male', 'Female', 'Other']
                },
                {
                    'name': 'age', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Age in years', 'pos': 4, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['15', '16', '17']
                },
                {
                    'name': 'grade_level', 'type_orig': 'int', 'type_interp': 'categorical', 
                    'desc': 'Current grade in school', 'pos': 5, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['9', '10', '11', '12']
                },
                
                # Scores acad√©miques
                {
                    'name': 'math_score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Marks in mathematics (50-99)', 'pos': 6, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['50', '74', '85', '99']
                },
                {
                    'name': 'reading_score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Marks in reading (50-99)', 'pos': 7, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['50', '61', '77', '99']
                },
                {
                    'name': 'writing_score', 'type_orig': 'int', 'type_interp': 'numerical', 
                    'desc': 'Marks in writing (50-99)', 'pos': 8, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['50', '69', '90', '99']
                },
                
                # Facteurs comportementaux et familiaux
                {
                    'name': 'attendance_rate', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'School attendance percentage (80-100)', 'pos': 9, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['80.0', '89.3', '94.7', '99.9']
                },
                {
                    'name': 'parent_education', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Highest education level of parents', 'pos': 10, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['High School', 'Bachelor\'s', 'Master\'s', 'PhD']
                }
            ]
            
            # Cr√©er les colonnes
            for col_info in columns_data:
                file_column = FileColumn(
                    dataset_file_id=dataset_file.id,
                    column_name=col_info['name'],
                    data_type_original=col_info['type_orig'],
                    data_type_interpreted=col_info['type_interp'],
                    description=col_info['desc'],
                    is_primary_key_component=col_info['is_pk'],
                    is_nullable=col_info['is_null'],
                    is_pii=col_info['is_pii'],
                    example_values=col_info['examples'],
                    position=col_info['pos'],
                    stats=None  # Pas de statistiques pour le moment
                )
                session.add(file_column)
                print(f"   ‚úì Colonne {col_info['pos']}/10: {col_info['name']}")
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset 'Student Academic Performance Dataset' initialis√© avec succ√®s !")
            print(f"üìä Dataset ID: {dataset.id}")
            print(f"üìÅ 1 fichier cr√©√©")
            print(f"üìã 10 colonnes cr√©√©es")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
        finally:
            session.close()


def init_student_depression_dataset():
    """
    Initialise le dataset Student Depression Dataset avec son fichier et colonnes.
    
    Supprime les donn√©es existantes et recr√©e tout.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        print("üí° V√©rifiez que DATABASE_URL est d√©finie dans les variables d'environnement")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            # === SUPPRESSION DES DONN√âES EXISTANTES ===
            print("üóëÔ∏è  Suppression des donn√©es existantes pour 'Student Depression Dataset'...")
            
            # Supprimer le dataset Student Depression s'il existe d√©j√† (cascade supprimera fichiers et colonnes)
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == "Student Depression Dataset"
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            # === CR√âATION DU DATASET ===
            print("üìä Cr√©ation du dataset Student Depression Dataset...")
            
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer et uploader des fichiers √©chantillons
            sample_columns_data = {
                'columns': [
                    {'name': 'id', 'type': 'integer'},
                    {'name': 'Gender', 'type': 'string'},
                    {'name': 'Age', 'type': 'integer'},
                    {'name': 'City', 'type': 'string'},
                    {'name': 'Profession', 'type': 'string'},
                    {'name': 'Academic Pressure', 'type': 'float'},
                    {'name': 'Work Pressure', 'type': 'float'},
                    {'name': 'CGPA', 'type': 'float'},
                    {'name': 'Study Satisfaction', 'type': 'float'},
                    {'name': 'Job Satisfaction', 'type': 'float'},
                    {'name': 'Sleep Duration', 'type': 'string'},
                    {'name': 'Dietary Habits', 'type': 'string'},
                    {'name': 'Degree', 'type': 'string'},
                    {'name': 'Have you ever had suicidal thoughts ?', 'type': 'string'},
                    {'name': 'Work/Study Hours', 'type': 'float'},
                    {'name': 'Financial Stress', 'type': 'float'},
                    {'name': 'Family History of Mental Illness', 'type': 'string'},
                    {'name': 'Depression', 'type': 'float'}
                ]
            }
            storage_path = upload_sample_dataset(dataset_id, sample_columns_data, "student_depression")
            
            dataset = Dataset(
                id=dataset_id,
                
                # === IDENTIFICATION & INFORMATIONS G√âN√âRALES ===
                dataset_name="Student Depression Dataset",
                year=2024,
                objective="Analyzing mental health trends and predictors among students",
                access="public",
                availability="online_download",
                num_citations=None,  # x dans le CSV
                citation_link=None,  # x dans le CSV
                sources="https://www.kaggle.com/datasets",
                storage_uri="student_depression_dataset.csv",
                storage_path=storage_path,
                
                # === CARACT√âRISTIQUES TECHNIQUES ===
                instances_number=28000,
                features_description="Demographics, academic indicators, lifestyle habits, and mental health indicators",
                features_number=18,
                domain=["sant√© mentale", "√©ducation"],
                representativity_description="Wide range of students from various cities and backgrounds",
                representativity_level="moyenne",  # "Medium"
                sample_balance_description=None,
                sample_balance_level=None,
                split=False,
                missing_values_description="Some features may have nulls, especially sensitive ones",
                has_missing_values=True,
                global_missing_percentage=0.5,
                missing_values_handling_method="none",
                temporal_factors=False,
                metadata_provided_with_dataset=True,
                external_documentation_available=False,
                documentation_link=None,
                task=["classification"],  # "Binary classification (Depression: Yes/No)"
                
                # === CRIT√àRES √âTHIQUES ===
                informed_consent=True,
                transparency=True,
                user_control=False,
                equity_non_discrimination=True,
                security_measures_in_place=True,
                data_quality_documented=True,
                data_errors_description=None,
                anonymization_applied=True,
                record_keeping_policy_exists=False,
                purpose_limitation_respected=True,
                accountability_defined=True
            )
            
            session.add(dataset)
            session.flush()  # Pour obtenir l'ID du dataset
            
            print(f"‚úÖ Dataset cr√©√© avec ID: {dataset.id}")
            
            # === CR√âATION DU FICHIER ===
            print("üìÅ Cr√©ation du fichier student_depression_dataset.csv...")
            
            dataset_file = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage='student_depression_dataset.csv',
                logical_role='main_data',
                format='csv',
                mime_type='text/csv',
                size_bytes=2900000,  # 2.9 MB
                row_count=28000,
                description='Comprehensive information about students mental health and related factors for analyzing depression trends and predictors'
            )
            
            session.add(dataset_file)
            session.flush()  # Pour obtenir l'ID du fichier
            
            print(f"‚úÖ Fichier cr√©√© avec ID: {dataset_file.id}")
            
            # === CR√âATION DES COLONNES ===
            print("üî¢ Cr√©ation des 18 colonnes...")
            
            columns_data = [
                # Identifiant
                {
                    'name': 'id', 'type_orig': 'int', 'type_interp': 'categorical', 
                    'desc': 'Unique identifier assigned to each student record', 'pos': 1, 'is_pk': True, 'is_null': False, 
                    'is_pii': False, 'examples': ['2', '8', '26', '28000']
                },
                
                # Donn√©es d√©mographiques
                {
                    'name': 'Gender', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Gender of the student for analyzing gender-specific mental health trends', 'pos': 2, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Male', 'Female', 'Other']
                },
                {
                    'name': 'Age', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'Age of the student in years', 'pos': 3, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['18.0', '24.0', '31.0', '59.0']
                },
                {
                    'name': 'City', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'City or region where student resides, providing geographical context', 'pos': 4, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Kalyan', 'Srinagar', 'Visakhapatnam', 'Bangalore', 'Chennai']
                },
                {
                    'name': 'Profession', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Field of work or study offering insights into occupational stress factors', 'pos': 5, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['Student', 'Architect']
                },
                
                # Facteurs de stress acad√©mique et professionnel
                {
                    'name': 'Academic Pressure', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'Level of pressure faced in academic settings (0-5 scale)', 'pos': 6, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['1.0', '2.0', '3.0', '4.0', '5.0']
                },
                {
                    'name': 'Work Pressure', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'Pressure related to work or job responsibilities (0-5 scale)', 'pos': 7, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['0.0', '1.0', '2.0', '3.0', '4.0', '5.0']
                },
                
                # Performance acad√©mique et satisfaction
                {
                    'name': 'CGPA', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'Cumulative grade point average reflecting overall academic performance (0-10)', 'pos': 8, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['5.59', '7.03', '8.97', '9.79']
                },
                {
                    'name': 'Study Satisfaction', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'How satisfied student is with studies, correlating with mental well-being (0-5)', 'pos': 9, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['1.0', '2.0', '3.0', '4.0', '5.0']
                },
                {
                    'name': 'Job Satisfaction', 'type_orig': 'float', 'type_interp': 'numerical', 
                    'desc': 'Satisfaction with job or work environment if applicable (0-4)', 'pos': 10, 'is_pk': False, 'is_null': False, 
                    'is_pii': False, 'examples': ['0.0', '1.0', '2.0', '3.0', '4.0']
                },
                
                # Facteurs de style de vie
                {
                    'name': 'Sleep Duration', 'type_orig': 'string', 'type_interp': 'text', 
                    'desc': 'Average number of hours student sleeps per day, important mental health factor', 'pos': 11, 'is_pk': False, 'is_null': True, 
                    'is_pii': False, 'examples': ['7-8 hours', '5-6 hours', '6-7 hours', '8+ hours']
                },
                {
                    'name': 'Dietary Habits', 'type_orig': 'string', 'type_interp': 'text', 
                    'desc': 'Assessment of eating patterns and nutritional habits impacting health and mood', 'pos': 12, 'is_pk': False, 'is_null': True, 
                    'is_pii': False, 'examples': ['Healthy', 'Moderate', 'Unhealthy']
                },
                {
                    'name': 'Degree', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Academic degree or program that student is pursuing', 'pos': 13, 'is_pk': False, 'is_null': True, 
                    'is_pii': False, 'examples': ['Bachelor', 'Master', 'PhD', 'High School']
                },
                
                # Indicateurs de sant√© mentale critiques
                {
                    'name': 'Have you ever had suicidal thoughts ?', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Binary indicator reflecting whether student has experienced suicidal ideation', 'pos': 14, 'is_pk': False, 'is_null': True, 
                    'is_pii': True, 'examples': ['Yes', 'No']
                },
                {
                    'name': 'Work/Study Hours', 'type_orig': 'string', 'type_interp': 'text', 
                    'desc': 'Average hours per day dedicated to work or study, influencing stress levels', 'pos': 15, 'is_pk': False, 'is_null': True, 
                    'is_pii': False, 'examples': ['1-3 hours', '4-6 hours', '7-9 hours', '10+ hours']
                },
                {
                    'name': 'Financial Stress', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Stress experienced due to financial concerns affecting mental health', 'pos': 16, 'is_pk': False, 'is_null': True, 
                    'is_pii': False, 'examples': ['Low', 'Moderate', 'High']
                },
                {
                    'name': 'Family History of Mental Illness', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Whether there is family history of mental illness, significant predisposition factor', 'pos': 17, 'is_pk': False, 'is_null': True, 
                    'is_pii': True, 'examples': ['Yes', 'No']
                },
                
                # Variable cible
                {
                    'name': 'Depression', 'type_orig': 'string', 'type_interp': 'categorical', 
                    'desc': 'Target variable indicating whether student is experiencing depression (primary focus)', 'pos': 18, 'is_pk': False, 'is_null': False, 
                    'is_pii': True, 'examples': ['Yes', 'No']
                }
            ]
            
            # Cr√©er les colonnes
            for col_info in columns_data:
                file_column = FileColumn(
                    dataset_file_id=dataset_file.id,
                    column_name=col_info['name'],
                    data_type_original=col_info['type_orig'],
                    data_type_interpreted=col_info['type_interp'],
                    description=col_info['desc'],
                    is_primary_key_component=col_info['is_pk'],
                    is_nullable=col_info['is_null'],
                    is_pii=col_info['is_pii'],
                    example_values=col_info['examples'],
                    position=col_info['pos'],
                    stats=None  # Pas de statistiques pour le moment
                )
                session.add(file_column)
                print(f"   ‚úì Colonne {col_info['pos']}/18: {col_info['name']}")
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset 'Student Depression Dataset' initialis√© avec succ√®s !")
            print(f"üìä Dataset ID: {dataset.id}")
            print(f"üìÅ 1 fichier cr√©√©")
            print(f"üìã 18 colonnes cr√©√©es")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
        finally:
            session.close()


def init_student_stress_factors_dataset():
    """
    Initialise le dataset Student Stress Factors avec tous ses fichiers et colonnes.
    
    Ce dataset analyse les facteurs de stress chez les √©tudiants en ing√©nierie
    bas√© sur la qualit√© du sommeil, maux de t√™te, performance acad√©mique, 
    charge d'√©tude et activit√©s extrascolaires.
    
    Supprime les donn√©es existantes et recr√©e tout.
    """
    
    # Configuration de la base de donn√©es
    try:
        database_url = DATABASE_URL
        print(f"üîå Connexion √† la base de donn√©es...")
    except Exception as e:
        print(f"‚ùå Erreur de configuration base de donn√©es: {e}")
        print("üí° V√©rifiez que DATABASE_URL est d√©finie dans l'environnement")
        sys.exit(1)
    
    # Cr√©er l'engine et la session
    engine = create_engine(database_url)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    with SessionLocal() as session:
        try:
            print("üóëÔ∏è  Suppression des donn√©es existantes du dataset Student Stress Factors...")
            
            # Supprimer le dataset s'il existe d√©j√† (cascade supprimera fichiers et colonnes)
            existing_dataset = session.query(Dataset).filter(
                Dataset.dataset_name == "Student Stress Factors"
            ).first()
            
            if existing_dataset:
                session.delete(existing_dataset)
                session.commit()
                print("‚úÖ Donn√©es existantes supprim√©es")
            
            print("üìä Cr√©ation du dataset Student Stress Factors...")
            
            # === CR√âATION DU DATASET PRINCIPAL ===
            dataset_id = str(uuid.uuid4())
            
            # G√©n√©rer et uploader les 2 fichiers √©chantillons pour Student Stress Factors
            files_data = [
                {
                    'filename': 'Student Stress Factors (2).csv',
                    'columns': [
                        {'name': 'Kindly Rate your Sleep Quality üò¥', 'type': 'integer'},
                        {'name': 'How many times a week do you suffer headaches ü§ï?', 'type': 'integer'},
                        {'name': 'How would you rate you academic performance üë©‚Äçüéì?', 'type': 'integer'},
                        {'name': 'how would you rate your study load?', 'type': 'integer'},
                        {'name': 'How many times a week you practice extracurricular activities üéæ?', 'type': 'integer'},
                        {'name': 'How would you rate your stress levels?', 'type': 'integer'}
                    ]
                },
                {
                    'filename': 'Student Stress Factors.csv',
                    'columns': [
                        {'name': 'Kindly Rate your Sleep Quality üò¥', 'type': 'integer'},
                        {'name': 'How many times a week do you suffer headaches ü§ï?', 'type': 'integer'},
                        {'name': 'How would you rate you academic performance üë©‚Äçüéì?', 'type': 'integer'},
                        {'name': 'how would you rate your study load?', 'type': 'integer'},
                        {'name': 'How many times a week you practice extracurricular activities üéæ?', 'type': 'integer'},
                        {'name': 'How would you rate your stress levels?', 'type': 'integer'}
                    ]
                }
            ]
            storage_path = upload_multiple_sample_files(dataset_id, files_data)
            
            dataset = Dataset(
                id=dataset_id,
                
                # === IDENTIFICATION & INFORMATIONS G√âN√âRALES ===
                dataset_name="Student Stress Factors",
                year=2023,
                objective="Understanding what impacts stress levels among engineering students based on sleep, headaches, academic performance, study load, extracurricular activities.",
                access="public",
                availability="online_download",
                num_citations=None,  # x dans les donn√©es originales
                citation_link="https://www.kaggle.com/datasets/samyakbavadekar/student-stress-factors",
                sources="https://www.kaggle.com/datasets/samyakbavadekar/student-stress-factors",
                storage_uri=None,  # Vide pour le moment
                storage_path=storage_path,
                
                # === CARACT√âRISTIQUES TECHNIQUES ===
                instances_number=520,
                features_description="Sleep quality, headaches frequency, academic performance, study load, extracurricular activity frequency, stress level (all on scales from 1 to 5).",
                features_number=6,
                domain=["√©ducation"],
                representativity_description="Survey mainly among engineering students; voluntary participation via Google Forms.",
                representativity_level="low",
                sample_balance_description=None,
                sample_balance_level=None,
                split=False,  # no split specified
                missing_values_description="No missing values reported.",
                has_missing_values=False,
                global_missing_percentage=0.0,
                missing_values_handling_method="not applicable",
                temporal_factors=False,  # Single time point (2023)
                metadata_provided_with_dataset=False,
                external_documentation_available=False,
                documentation_link=None,
                task=["regression"],
                
                # === CRIT√àRES √âTHIQUES ===
                informed_consent=False,
                transparency=True,
                user_control=None,  # unknown
                equity_non_discrimination=None,  # unknown
                security_measures_in_place=None,  # unknown
                data_quality_documented=False,
                data_errors_description=None,
                anonymization_applied=True,
                record_keeping_policy_exists=None,  # unknown
                purpose_limitation_respected=None,  # not specified
                accountability_defined=None  # not specified
            )
            
            session.add(dataset)
            session.flush()  # Pour obtenir l'ID du dataset
            
            print(f"‚úÖ Dataset cr√©√© avec ID: {dataset.id}")
            
            # === CR√âATION DES 2 FICHIERS ===
            print("üìÅ Cr√©ation des 2 fichiers CSV...")
            
            # Fichier 1: Student Stress Factors (2).csv
            dataset_file_1 = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage="Student Stress Factors (2).parquet",  # Nom r√©el dans MinIO
                logical_role="main_data",
                format="parquet",  # Format r√©el stock√©
                mime_type="application/parquet",
                size_bytes=7050,  # 7.05 kB d'apr√®s description
                row_count=520,
                description="CSV file collected from Google Forms - survey data on engineering student stress factors (version 2)"
            )
            
            session.add(dataset_file_1)
            session.flush()
            print(f"‚úÖ Fichier 1 cr√©√© avec ID: {dataset_file_1.id}")
            
            # Fichier 2: Student Stress Factors.csv
            dataset_file_2 = DatasetFile(
                dataset_id=dataset.id,
                file_name_in_storage="Student Stress Factors.parquet",  # Nom r√©el dans MinIO
                logical_role="main_data",
                format="parquet",  # Format r√©el stock√©
                mime_type="application/parquet",
                size_bytes=9090,  # 9.09 kB d'apr√®s description
                row_count=520,
                description="CSV file collected from Google Forms - survey data on engineering student stress factors (original version)"
            )
            
            session.add(dataset_file_2)
            session.flush()
            print(f"‚úÖ Fichier 2 cr√©√© avec ID: {dataset_file_2.id}")
            
            # === CR√âATION DES COLONNES ===
            print("üìã Cr√©ation des colonnes...")
            
            columns_data = [
                {
                    'name': 'Kindly Rate your Sleep Quality üò¥', 'type_orig': 'integer', 'type_interp': 'numerical',
                    'desc': 'Sleep quality rating from 1 (very poor) to 5 (excellent) - primary lifestyle factor affecting stress', 'pos': 1, 'is_pk': False, 'is_null': False,
                    'is_pii': False, 'examples': ['1', '2', '3', '4', '5']
                },
                {
                    'name': 'How many times a week do you suffer headaches ü§ï?', 'type_orig': 'integer', 'type_interp': 'numerical',
                    'desc': 'Frequency of headaches per week from 1 (rare) to 5 (very frequent) - potential stress indicator', 'pos': 2, 'is_pk': False, 'is_null': False,
                    'is_pii': False, 'examples': ['1', '2', '3', '4', '5']
                },
                {
                    'name': 'How would you rate you academic performance üë©‚Äçüéì?', 'type_orig': 'integer', 'type_interp': 'numerical',
                    'desc': 'Self-assessed academic performance from 1 (poor) to 5 (excellent) - key academic stress factor', 'pos': 3, 'is_pk': False, 'is_null': False,
                    'is_pii': False, 'examples': ['1', '2', '3', '4', '5']
                },
                {
                    'name': 'how would you rate your study load?', 'type_orig': 'integer', 'type_interp': 'numerical',
                    'desc': 'Perceived study workload intensity from 1 (light) to 5 (very heavy) - major stress contributor', 'pos': 4, 'is_pk': False, 'is_null': False,
                    'is_pii': False, 'examples': ['1', '2', '3', '4', '5']
                },
                {
                    'name': 'How many times a week you practice extracurricular activities üéæ?', 'type_orig': 'integer', 'type_interp': 'numerical',
                    'desc': 'Frequency of extracurricular participation per week from 1 (rare) to 5 (very frequent) - stress relief factor', 'pos': 5, 'is_pk': False, 'is_null': False,
                    'is_pii': False, 'examples': ['1', '2', '3', '4', '5']
                },
                {
                    'name': 'How would you rate your stress levels?', 'type_orig': 'integer', 'type_interp': 'numerical',
                    'desc': 'Overall stress level self-assessment from 1 (very low) to 5 (very high) - target variable for regression analysis', 'pos': 6, 'is_pk': False, 'is_null': False,
                    'is_pii': False, 'examples': ['1', '2', '3', '4', '5']
                }
            ]
            
            # Cr√©er les colonnes pour les 2 fichiers (m√™me structure)
            for file_info, file_name in [(dataset_file_1, "Fichier 1"), (dataset_file_2, "Fichier 2")]:
                print(f"üìã Cr√©ation des colonnes pour {file_name}...")
                for col_info in columns_data:
                    file_column = FileColumn(
                        dataset_file_id=file_info.id,
                        column_name=col_info['name'],
                        data_type_original=col_info['type_orig'],
                        data_type_interpreted=col_info['type_interp'],
                        description=col_info['desc'],
                        is_primary_key_component=col_info['is_pk'],
                        is_nullable=col_info['is_null'],
                        is_pii=col_info['is_pii'],
                        example_values=col_info['examples'],
                        position=col_info['pos'],
                        stats=None  # Pas de statistiques pour le moment
                    )
                    session.add(file_column)
                    print(f"   ‚úì Colonne {col_info['pos']}/6: {col_info['name']}")
            
            # Valider toutes les modifications
            session.commit()
            
            print("\nüéâ Dataset 'Student Stress Factors' initialis√© avec succ√®s !")
            print(f"üìä Dataset ID: {dataset.id}")
            print(f"üìÅ 2 fichiers cr√©√©s")
            print(f"üìã 12 colonnes cr√©√©es (6 par fichier)")
            
        except Exception as e:
            session.rollback()
            print(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise
        finally:
            session.close()


def main():
    """Point d'entr√©e principal du script."""
    print("üöÄ Initialisation des datasets")
    print("=" * 60)
    
    if len(sys.argv) > 1:
        dataset_name = sys.argv[1].lower()
        if dataset_name == "ednet":
            print("üìä Initialisation du dataset EdNet uniquement")
            try:
                init_ednet_dataset()
                print("\n‚úÖ Dataset EdNet initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation EdNet: {e}")
                sys.exit(1)
        elif dataset_name == "oulad":
            print("üìä Initialisation du dataset OULAD uniquement")
            try:
                init_oulad_dataset()
                print("\n‚úÖ Dataset OULAD initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation OULAD: {e}")
                sys.exit(1)
        elif dataset_name == "students":
            print("üìä Initialisation du dataset Students Performance uniquement")
            try:
                init_students_performance_dataset()
                print("\n‚úÖ Dataset Students Performance initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation Students Performance: {e}")
                sys.exit(1)
        elif dataset_name == "social":
            print("üìä Initialisation du dataset Social Media Addiction uniquement")
            try:
                init_social_media_addiction_dataset()
                print("\n‚úÖ Dataset Social Media Addiction initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation Social Media Addiction: {e}")
                sys.exit(1)
        elif dataset_name == "academic":
            print("üìä Initialisation du dataset Student Academic Performance uniquement")
            try:
                init_student_academic_performance_dataset()
                print("\n‚úÖ Dataset Student Academic Performance initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation Student Academic Performance: {e}")
                sys.exit(1)
        elif dataset_name == "depression":
            print("üìä Initialisation du dataset Student Depression uniquement")
            try:
                init_student_depression_dataset()
                print("\n‚úÖ Dataset Student Depression initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation Student Depression: {e}")
                sys.exit(1)
        elif dataset_name == "stress":
            print("üìä Initialisation du dataset Student Stress Factors uniquement")
            try:
                init_student_stress_factors_dataset()
                print("\n‚úÖ Dataset Student Stress Factors initialis√© avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation Student Stress Factors: {e}")
                sys.exit(1)
        elif dataset_name == "all":
            print("üìä Initialisation de tous les datasets")
            try:
                init_ednet_dataset()
                print("\n‚úÖ Dataset EdNet initialis√© avec succ√®s !")
                init_oulad_dataset()
                print("\n‚úÖ Dataset OULAD initialis√© avec succ√®s !")
                init_students_performance_dataset()
                print("\n‚úÖ Dataset Students Performance initialis√© avec succ√®s !")
                init_social_media_addiction_dataset()
                print("\n‚úÖ Dataset Social Media Addiction initialis√© avec succ√®s !")
                init_student_academic_performance_dataset()
                print("\n‚úÖ Dataset Student Academic Performance initialis√© avec succ√®s !")
                init_student_depression_dataset()
                print("\n‚úÖ Dataset Student Depression initialis√© avec succ√®s !")
                init_student_stress_factors_dataset()
                print("\n‚úÖ Dataset Student Stress Factors initialis√© avec succ√®s !")
                print("\nüéâ Tous les datasets ont √©t√© initialis√©s avec succ√®s !")
            except Exception as e:
                print(f"\n‚ùå √âchec de l'initialisation: {e}")
                sys.exit(1)
        else:
            print(f"‚ùå Dataset inconnu: {dataset_name}")
            print("üí° Usage: python scripts/init_datasets.py [ednet|oulad|students|social|academic|depression|stress|all]")
            sys.exit(1)
    else:
        # Par d√©faut, initialiser tous les datasets
        print("üìä Initialisation de tous les datasets (par d√©faut)")
        try:
            init_ednet_dataset()
            print("\n‚úÖ Dataset EdNet initialis√© avec succ√®s !")
            init_oulad_dataset()
            print("\n‚úÖ Dataset OULAD initialis√© avec succ√®s !")
            init_students_performance_dataset()
            print("\n‚úÖ Dataset Students Performance initialis√© avec succ√®s !")
            init_social_media_addiction_dataset()
            print("\n‚úÖ Dataset Social Media Addiction initialis√© avec succ√®s !")
            init_student_academic_performance_dataset()
            print("\n‚úÖ Dataset Student Academic Performance initialis√© avec succ√®s !")
            init_student_depression_dataset()
            print("\n‚úÖ Dataset Student Depression initialis√© avec succ√®s !")
            init_student_stress_factors_dataset()
            print("\n‚úÖ Dataset Student Stress Factors initialis√© avec succ√®s !")
            print("\nüéâ Tous les datasets ont √©t√© initialis√©s avec succ√®s !")
        except Exception as e:
            print(f"\n‚ùå √âchec de l'initialisation: {e}")
            sys.exit(1)

if __name__ == "__main__":
    main() 